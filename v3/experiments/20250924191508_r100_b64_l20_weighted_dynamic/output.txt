======================================================================
RUNNING IMPROVED DyNA PPO WITH BETTER SURROGATE LEARNING
======================================================================
======================================================================
IMPROVED DyNA PPO ALGORITHM
======================================================================
Configuration:
  Number of experiment rounds N = 100
  Number of model-based training rounds M = 5
  Minimum model score τ = 0.2
  Batch size B = 64
  Warm-up phase: True
  Surrogate model method: weighted
======================================================================

=== WARM-UP PHASE ===
Generating 50 warm-up samples...
Warm-up statistics:
  Mean reward: 11.197
  Std reward: 3.903
  Min/Max: 0.000 / 17.078

Pre-training surrogate models on warm-up data...

Training on 47 samples (removed 3 outliers)
Reward range: [3.68, 17.08], mean: 11.91
  Created 8 candidate models for data size 47
Current R2 threshold: -0.3
  rf-xs: R2 = -0.350 (std: 0.557)
  rf-s: R2 = -0.540 (std: 0.876)
  knn-xs: R2 = -0.161 (std: 0.139)
  knn-s: R2 = -0.161 (std: 0.139)
  ridge: R2 = -0.070 (std: 0.075)
  gb-xs: R2 = -0.649 (std: 0.828)
  gp: R2 = -24.253 (std: 9.038)
  svr-rbf-s: R2 = -0.062 (std: 0.064)
Initial models trained: 4
Initial R2 scores - Mean: -3.281, Max: -0.062

======================================================================
EXPERIMENT ROUND 1/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.300
Total data collected: 50

--- Round 1 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  TTAACGTTTTAGGCGACGTC
  GTTGGTTTGGGATAAACCTA
  CGGTATGCCGACTCAGTAAT
  CTATGCCGATATATCATGCG
  AACGTATCAGCTACTTCTAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 12.904
  Max reward: 17.796
  With intrinsic bonuses: 12.945

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=-0.0000, value_loss=0.9891, entropy=1.3849, kl_div=0.0000
    Early stopping at epoch 2: KL divergence = 0.0571

=== Surrogate Model Training ===
Total samples: 114

Training on 106 samples (removed 8 outliers)
Reward range: [6.97, 17.88], mean: 12.82
  Created 11 candidate models for data size 106
Current R2 threshold: -0.3
  rf-m: R2 = -0.053 (std: 0.085)
  rf-l: R2 = -0.066 (std: 0.084)
  gb-m: R2 = -0.358 (std: 0.201)
  gb-l: R2 = -0.365 (std: 0.194)
  xgb-m: R2 = -0.439 (std: 0.315)
  knn-m: R2 = -0.186 (std: 0.141)
  knn-tuned: R2 = -0.186 (std: 0.141)
  mlp-m: R2 = -2.008 (std: 1.754)
  svr-rbf: R2 = -0.001 (std: 0.043)
  svr-poly: R2 = -0.001 (std: 0.043)
  ridge: R2 = -0.088 (std: 0.090)

Model-based training with 7 models
Best R2: -0.001, Mean R2: -0.341
Running 2 virtual training rounds
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9860, entropy=1.3818, kl_div=0.0000
  Round 1/2: Mean predicted reward = 12.965
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=1.3787, kl_div=0.0000
  Round 2/2: Mean predicted reward = 13.305

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 1 Results ---
  Mean Oracle Reward: 12.903
  Min Oracle Reward: 6.442
  Max Oracle Reward: 17.759
  Std Oracle Reward: 2.138
  Sequence Diversity: 1.000
  Models Used: 7
  Model R2 - Mean: -0.341, Max: -0.001, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 114
    Oracle Count: 64 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}]

======================================================================
EXPERIMENT ROUND 2/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.300
Total data collected: 114

--- Round 2 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  CGCCAGTGCAGCTACCCCAT
  ACTGCTTTAGAAGGTGAGCG
  TACCAACGACCAAAACCTCG
  GCAGCCGGACCTCGTCCCTA
  ACCTCCGAGCAGCTGAACTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.235
  Max reward: 16.669
  With intrinsic bonuses: 13.214

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=0.0000, value_loss=0.9888, entropy=1.3743, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0940

=== Surrogate Model Training ===
Total samples: 178

Training on 166 samples (removed 12 outliers)
Reward range: [8.34, 17.08], mean: 13.04
  Created 11 candidate models for data size 166
Current R2 threshold: -0.3
  rf-m: R2 = -0.193 (std: 0.186)
  rf-l: R2 = -0.153 (std: 0.152)
  gb-m: R2 = -0.304 (std: 0.328)
  gb-l: R2 = -0.308 (std: 0.333)
  xgb-m: R2 = -0.435 (std: 0.424)
  knn-m: R2 = -0.255 (std: 0.173)
  knn-tuned: R2 = -0.255 (std: 0.173)
  mlp-m: R2 = -2.087 (std: 0.893)
  svr-rbf: R2 = -0.002 (std: 0.096)
  svr-poly: R2 = -0.002 (std: 0.096)
  ridge: R2 = -0.037 (std: 0.066)

Model-based training with 7 models
Best R2: -0.002, Mean R2: -0.366
Running 2 virtual training rounds
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=1.3688, kl_div=0.0000
  Round 1/2: Mean predicted reward = 13.220
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9843, entropy=1.3629, kl_div=0.0000
  Round 2/2: Mean predicted reward = 13.292

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 2 Results ---
  Mean Oracle Reward: 13.204
  Min Oracle Reward: 8.182
  Max Oracle Reward: 16.581
  Std Oracle Reward: 1.675
  Sequence Diversity: 1.000
  Models Used: 7
  Model R2 - Mean: -0.366, Max: -0.002, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 178
    Oracle Count: 128 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}]

======================================================================
EXPERIMENT ROUND 3/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.300
Total data collected: 178

--- Round 3 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  TACCTGAGCGTAAGGCGCAC
  ATGTACACCTTTCCTTGCTC
  ATTCATATGTTTACGTAGCG
  CTTGGGAGGTGCACGAGTAC
  GCGCCACTAAATGTCATCCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.583
  Max reward: 17.030
  With intrinsic bonuses: 13.766

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=-0.0000, value_loss=0.9872, entropy=1.3577, kl_div=0.0000
    Early stopping at epoch 2: KL divergence = 0.0741

=== Surrogate Model Training ===
Total samples: 242

Training on 229 samples (removed 13 outliers)
Reward range: [8.34, 17.88], mean: 13.26
  Created 11 candidate models for data size 229
Current R2 threshold: -0.3
  rf-m: R2 = -0.024 (std: 0.229)
  rf-l: R2 = -0.063 (std: 0.230)
  gb-m: R2 = -0.076 (std: 0.279)
  gb-l: R2 = -0.076 (std: 0.278)
  xgb-m: R2 = -0.185 (std: 0.354)
  knn-m: R2 = -0.159 (std: 0.223)
  knn-tuned: R2 = -0.159 (std: 0.223)
  mlp-m: R2 = -1.957 (std: 1.341)
  svr-rbf: R2 = 0.029 (std: 0.174)
  svr-poly: R2 = 0.029 (std: 0.174)
  ridge: R2 = -0.024 (std: 0.134)

Model-based training with 10 models
Best R2: 0.029, Mean R2: -0.242
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.101 gb-m:0.099 gb-l:0.099 xgb-m:0.089 knn-m:0.091 knn-tuned:0.091 svr-rbf:0.110 svr-poly:0.110 ridge:0.105 
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9899, entropy=1.3490, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0511
  Round 1/3: Mean predicted reward = 13.284
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.101 gb-m:0.099 gb-l:0.099 xgb-m:0.089 knn-m:0.091 knn-tuned:0.091 svr-rbf:0.110 svr-poly:0.110 ridge:0.105 
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9825, entropy=1.3443, kl_div=0.0000
    Epoch 1: policy_loss=-0.0733, value_loss=0.9825, entropy=1.3387, kl_div=0.0413
  Round 2/3: Mean predicted reward = 13.309
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.101 gb-m:0.099 gb-l:0.099 xgb-m:0.089 knn-m:0.091 knn-tuned:0.091 svr-rbf:0.110 svr-poly:0.110 ridge:0.105 
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9885, entropy=1.3326, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0592
  Round 3/3: Mean predicted reward = 13.330

  === Progress Analysis ===
  Status: NORMAL

--- Round 3 Results ---
  Mean Oracle Reward: 13.601
  Min Oracle Reward: 6.631
  Max Oracle Reward: 17.086
  Std Oracle Reward: 1.972
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.242, Max: 0.029, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 242
    Oracle Count: 192 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}]

======================================================================
EXPERIMENT ROUND 4/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.280
Total data collected: 242
  Consistent improvement, increasing LR to 0.000045

--- Round 4 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0100
Exploration rate: 0.280

--- Generated Sequences (Diversity: 1.000) ---
  TCTACGAGCTTGCGAACGCG
  CTAGACGGAGGTTGTACCCC
  CGACTATCCGCAGGGGATTA
  GAGCTGCGTCCCAGTACTAG
  CATGGGCTGAGTCCAAGTCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.928
  Max reward: 18.524
  With intrinsic bonuses: 14.047

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=1.3261, kl_div=0.0000
    Epoch 1: policy_loss=-0.0184, value_loss=0.9880, entropy=1.3231, kl_div=0.0357
    Early stopping at epoch 2: KL divergence = 0.0724

=== Surrogate Model Training ===
Total samples: 306

Training on 290 samples (removed 16 outliers)
Reward range: [8.56, 17.88], mean: 13.43
  Created 11 candidate models for data size 290
Current R2 threshold: -0.3
  rf-m: R2 = -0.003 (std: 0.109)
  rf-l: R2 = 0.006 (std: 0.114)
  gb-m: R2 = -0.006 (std: 0.132)
  gb-l: R2 = -0.010 (std: 0.137)
  xgb-m: R2 = -0.145 (std: 0.207)
  knn-m: R2 = -0.227 (std: 0.187)
  knn-tuned: R2 = -0.227 (std: 0.187)
  mlp-m: R2 = -0.437 (std: 0.211)
  svr-rbf: R2 = 0.022 (std: 0.129)
  svr-poly: R2 = 0.022 (std: 0.129)
  ridge: R2 = -0.016 (std: 0.105)

Model-based training with 10 models
Best R2: 0.022, Mean R2: -0.093
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.106 gb-m:0.105 gb-l:0.104 xgb-m:0.091 knn-m:0.084 knn-tuned:0.084 svr-rbf:0.108 svr-poly:0.108 ridge:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9879, entropy=1.3197, kl_div=0.0000
    Epoch 1: policy_loss=-0.0246, value_loss=0.9879, entropy=1.3166, kl_div=0.0419
  Round 1/3: Mean predicted reward = 13.536
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.106 gb-m:0.105 gb-l:0.104 xgb-m:0.091 knn-m:0.084 knn-tuned:0.084 svr-rbf:0.108 svr-poly:0.108 ridge:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9892, entropy=1.3137, kl_div=0.0000
    Epoch 1: policy_loss=-0.0226, value_loss=0.9892, entropy=1.3105, kl_div=0.0425
  Round 2/3: Mean predicted reward = 13.439
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.105 rf-l:0.106 gb-m:0.105 gb-l:0.104 xgb-m:0.091 knn-m:0.084 knn-tuned:0.084 svr-rbf:0.108 svr-poly:0.108 ridge:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=1.3064, kl_div=0.0000
    Epoch 1: policy_loss=-0.0116, value_loss=0.9854, entropy=1.3032, kl_div=0.0331
  Round 3/3: Mean predicted reward = 13.610

  === Progress Analysis ===
  Status: NORMAL

--- Round 4 Results ---
  Mean Oracle Reward: 13.955
  Min Oracle Reward: 10.151
  Max Oracle Reward: 18.589
  Std Oracle Reward: 1.721
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.093, Max: 0.022, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 306
    Oracle Count: 256 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}]

======================================================================
EXPERIMENT ROUND 5/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.250
Total data collected: 306
  Consistent improvement, increasing LR to 0.000360

--- Round 5 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0100
Exploration rate: 0.250

--- Generated Sequences (Diversity: 1.000) ---
  ATGCTAGGAGACCGCTTCCG
  TGGTCAGTGCCAACAGCTCG
  CCAGGTCTCCTAGCGATGAG
  AGAATGGGCGTTCTACGACC
  GGACGACATTTCGAGCTCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.978
  Max reward: 17.297
  With intrinsic bonuses: 14.123

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9887, entropy=1.2995, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2746

=== Surrogate Model Training ===
Total samples: 370

Training on 353 samples (removed 17 outliers)
Reward range: [9.16, 17.88], mean: 13.55
  Created 14 candidate models for data size 353
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.023 (std: 0.104)
  rf-tuned-xl: R2 = 0.008 (std: 0.128)
  gb-tuned-l: R2 = 0.003 (std: 0.115)
  gb-tuned-xl: R2 = 0.003 (std: 0.115)
  xgb-xl: R2 = -0.106 (std: 0.178)
  xgb-l: R2 = -0.106 (std: 0.178)
  mlp-adaptive-xl: R2 = -0.429 (std: 0.185)
  mlp-l: R2 = -0.459 (std: 0.261)
  svr-rbf-xl: R2 = 0.023 (std: 0.083)
  svr-poly-l: R2 = 0.023 (std: 0.083)
  knn-tuned-sqrt: R2 = -0.155 (std: 0.211)
  knn-tuned-l: R2 = -0.155 (std: 0.211)
  ridge: R2 = -0.010 (std: 0.076)
  gp: R2 = -69.031 (std: 14.771)

Model-based training with 11 models
Best R2: 0.023, Mean R2: -5.026
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.095 gb-tuned-l:0.095 gb-tuned-xl:0.095 xgb-xl:0.085 xgb-l:0.085 svr-rbf-xl:0.097 svr-poly-l:0.097 knn-tuned-sqrt:0.081 knn-tuned-l:0.081 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9884, entropy=1.2758, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2909
  Round 1/3: Mean predicted reward = 13.780
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.095 gb-tuned-l:0.095 gb-tuned-xl:0.095 xgb-xl:0.085 xgb-l:0.085 svr-rbf-xl:0.097 svr-poly-l:0.097 knn-tuned-sqrt:0.081 knn-tuned-l:0.081 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9885, entropy=1.2524, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4320
  Round 2/3: Mean predicted reward = 13.652
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.095 gb-tuned-l:0.095 gb-tuned-xl:0.095 xgb-xl:0.085 xgb-l:0.085 svr-rbf-xl:0.097 svr-poly-l:0.097 knn-tuned-sqrt:0.081 knn-tuned-l:0.081 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9892, entropy=1.2231, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4955
  Round 3/3: Mean predicted reward = 13.683

  === Progress Analysis ===
  Status: NORMAL

--- Round 5 Results ---
  Mean Oracle Reward: 13.965
  Min Oracle Reward: 10.946
  Max Oracle Reward: 17.218
  Std Oracle Reward: 1.408
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -5.026, Max: 0.023, Count: 14
  New best mean reward!
  Total Sequences Evaluated: 370
    Oracle Count: 320 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}]

======================================================================
EXPERIMENT ROUND 6/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.220
Total data collected: 370
  Consistent improvement, increasing LR to 0.000327

--- Round 6 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0100
Exploration rate: 0.220

--- Generated Sequences (Diversity: 1.000) ---
  TTGCGCAACCGTAGACGGGC
  CGATGTGCGTCGGCGACCAA
  GTCCGCGGCAGAAACGCTTT
  CGTGATGTCGCGGCAAACTC
  GGCATAATTTCGCGCAGCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.174
  Max reward: 17.561
  With intrinsic bonuses: 14.275

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9902, entropy=1.1898, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4709

=== Surrogate Model Training ===
Total samples: 434

Training on 417 samples (removed 17 outliers)
Reward range: [9.16, 17.88], mean: 13.64
  Created 14 candidate models for data size 417
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.008 (std: 0.064)
  rf-tuned-xl: R2 = 0.040 (std: 0.075)
  gb-tuned-l: R2 = 0.056 (std: 0.058)
  gb-tuned-xl: R2 = 0.056 (std: 0.058)
  xgb-xl: R2 = -0.154 (std: 0.138)
  xgb-l: R2 = -0.154 (std: 0.138)
  mlp-adaptive-xl: R2 = -0.208 (std: 0.194)
  mlp-l: R2 = -0.346 (std: 0.104)
  svr-rbf-xl: R2 = 0.043 (std: 0.060)
  svr-poly-l: R2 = 0.043 (std: 0.060)
  knn-tuned-sqrt: R2 = -0.122 (std: 0.133)
  knn-tuned-l: R2 = -0.122 (std: 0.133)
  ridge: R2 = 0.017 (std: 0.048)
  gp: R2 = -73.754 (std: 16.143)

Model-based training with 12 models
Best R2: 0.056, Mean R2: -5.328
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.087 rf-tuned-xl:0.090 gb-tuned-l:0.091 gb-tuned-xl:0.091 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.070 svr-rbf-xl:0.090 svr-poly-l:0.090 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.088 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9875, entropy=1.1548, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6051
  Round 1/3: Mean predicted reward = 13.879
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.087 rf-tuned-xl:0.090 gb-tuned-l:0.091 gb-tuned-xl:0.091 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.070 svr-rbf-xl:0.090 svr-poly-l:0.090 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.088 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9874, entropy=1.1172, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6937
  Round 2/3: Mean predicted reward = 13.849
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.087 rf-tuned-xl:0.090 gb-tuned-l:0.091 gb-tuned-xl:0.091 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.070 svr-rbf-xl:0.090 svr-poly-l:0.090 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.088 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9899, entropy=1.0736, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8040
  Round 3/3: Mean predicted reward = 13.803

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 6 Results ---
  Mean Oracle Reward: 14.151
  Min Oracle Reward: 10.720
  Max Oracle Reward: 17.362
  Std Oracle Reward: 1.325
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: -5.328, Max: 0.056, Count: 14
  New best mean reward!
  Total Sequences Evaluated: 434
    Oracle Count: 384 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}]

======================================================================
EXPERIMENT ROUND 7/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.190
Total data collected: 434
  Consistent improvement, increasing LR to 0.000240

--- Round 7 Configuration ---
Learning rate: 0.000240
Entropy coefficient: 0.0100
Exploration rate: 0.190

--- Generated Sequences (Diversity: 1.000) ---
  CGAGCTATCACTGCAGTGAG
  CATCGTGGGCGGAAGCCTCC
  GATGGCGGCCACATTTCAGC
  CCGATTGATCGCCCGAGGCG
  ACTGTGGCAACGATTCGGCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.913
  Max reward: 17.674
  With intrinsic bonuses: 13.990

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9915, entropy=1.0327, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7445

=== Surrogate Model Training ===
Total samples: 498

Training on 479 samples (removed 19 outliers)
Reward range: [9.16, 17.88], mean: 13.71
  Created 14 candidate models for data size 479
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.027 (std: 0.096)
  rf-tuned-xl: R2 = 0.024 (std: 0.090)
  gb-tuned-l: R2 = -0.002 (std: 0.154)
  gb-tuned-xl: R2 = -0.002 (std: 0.154)
  xgb-xl: R2 = -0.208 (std: 0.168)
  xgb-l: R2 = -0.208 (std: 0.168)
  mlp-adaptive-xl: R2 = -0.391 (std: 0.302)
  mlp-l: R2 = -0.339 (std: 0.337)
  svr-rbf-xl: R2 = 0.063 (std: 0.107)
  svr-poly-l: R2 = 0.063 (std: 0.107)
  knn-tuned-sqrt: R2 = -0.068 (std: 0.108)
  knn-tuned-l: R2 = -0.068 (std: 0.108)
  ridge: R2 = 0.006 (std: 0.075)
  gp: R2 = -73.386 (std: 18.386)

Model-based training with 11 models
Best R2: 0.063, Mean R2: -5.321
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.096 gb-tuned-l:0.093 gb-tuned-xl:0.093 xgb-xl:0.076 xgb-l:0.076 svr-rbf-xl:0.100 svr-poly-l:0.100 knn-tuned-sqrt:0.087 knn-tuned-l:0.087 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9870, entropy=0.9933, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7626
  Round 1/3: Mean predicted reward = 14.008
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.096 gb-tuned-l:0.093 gb-tuned-xl:0.093 xgb-xl:0.076 xgb-l:0.076 svr-rbf-xl:0.100 svr-poly-l:0.100 knn-tuned-sqrt:0.087 knn-tuned-l:0.087 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9891, entropy=0.9598, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9338
  Round 2/3: Mean predicted reward = 13.912
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.096 gb-tuned-l:0.093 gb-tuned-xl:0.093 xgb-xl:0.076 xgb-l:0.076 svr-rbf-xl:0.100 svr-poly-l:0.100 knn-tuned-sqrt:0.087 knn-tuned-l:0.087 ridge:0.094 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9922, entropy=0.9248, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9784
  Round 3/3: Mean predicted reward = 13.981

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 7 Results ---
  Mean Oracle Reward: 13.913
  Min Oracle Reward: 4.427
  Max Oracle Reward: 17.790
  Std Oracle Reward: 2.209
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -5.321, Max: 0.063, Count: 14
  Total Sequences Evaluated: 498
    Oracle Count: 448 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}]

======================================================================
EXPERIMENT ROUND 8/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.080
Total data collected: 498

--- Round 8 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.080

--- Generated Sequences (Diversity: 1.000) ---
  GACTCATACGGGCTTACCGG
  GAGCTGACCGTGTCGACGAC
  GTCCGCGGCTTCATCAGAGA
  GGTCCTCAACTATAGGCGCG
  GAGACGCTTAGCCCGCGGTA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.488
  Max reward: 17.418
  With intrinsic bonuses: 14.522

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9871, entropy=0.8874, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4799

=== Surrogate Model Training ===
Total samples: 562

Training on 542 samples (removed 20 outliers)
Reward range: [9.19, 17.88], mean: 13.81
  Created 13 candidate models for data size 542
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.059 (std: 0.077)
  rf-tuned-xl: R2 = 0.073 (std: 0.071)
  gb-tuned-l: R2 = 0.073 (std: 0.065)
  gb-tuned-xl: R2 = 0.073 (std: 0.065)
  xgb-xl: R2 = -0.131 (std: 0.037)
  xgb-l: R2 = -0.131 (std: 0.037)
  mlp-adaptive-xl: R2 = -0.143 (std: 0.107)
  mlp-l: R2 = -0.167 (std: 0.076)
  svr-rbf-xl: R2 = 0.108 (std: 0.061)
  svr-poly-l: R2 = 0.108 (std: 0.061)
  knn-tuned-sqrt: R2 = 0.000 (std: 0.070)
  knn-tuned-l: R2 = 0.000 (std: 0.070)
  ridge: R2 = 0.048 (std: 0.089)

Model-based training with 13 models
Best R2: 0.108, Mean R2: -0.002
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.083 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.067 mlp-l:0.065 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9887, entropy=0.8691, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5218
  Round 1/3: Mean predicted reward = 13.946
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.083 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.067 mlp-l:0.065 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9888, entropy=0.8542, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5306
  Round 2/3: Mean predicted reward = 13.935
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.083 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.067 mlp-l:0.065 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9911, entropy=0.8355, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5478
  Round 3/3: Mean predicted reward = 14.103

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 8 Results ---
  Mean Oracle Reward: 14.460
  Min Oracle Reward: 9.611
  Max Oracle Reward: 17.215
  Std Oracle Reward: 1.580
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.002, Max: 0.108, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 562
    Oracle Count: 512 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}]

======================================================================
EXPERIMENT ROUND 9/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.065
Total data collected: 562

--- Round 9 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.065

--- Generated Sequences (Diversity: 1.000) ---
  CCTCGTGAACCGAAGCTGTG
  GCGACCGCCTGATTGACGCG
  ATCAGGGGGCACTCGGTCCC
  CTGGTAGGCGCATAGCCGCC
  GTCCCGGTAGGGACGCTACC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.214
  Max reward: 19.228
  With intrinsic bonuses: 14.234

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9913, entropy=0.8166, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1982

=== Surrogate Model Training ===
Total samples: 626

Training on 601 samples (removed 25 outliers)
Reward range: [9.16, 18.39], mean: 13.85
  Created 13 candidate models for data size 601
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.056 (std: 0.082)
  rf-tuned-xl: R2 = 0.056 (std: 0.075)
  gb-tuned-l: R2 = 0.112 (std: 0.089)
  gb-tuned-xl: R2 = 0.112 (std: 0.089)
  xgb-xl: R2 = -0.170 (std: 0.076)
  xgb-l: R2 = -0.170 (std: 0.076)
  mlp-adaptive-xl: R2 = -0.100 (std: 0.105)
  mlp-l: R2 = -0.106 (std: 0.168)
  svr-rbf-xl: R2 = 0.112 (std: 0.095)
  svr-poly-l: R2 = 0.112 (std: 0.095)
  knn-tuned-sqrt: R2 = 0.007 (std: 0.060)
  knn-tuned-l: R2 = 0.007 (std: 0.060)
  ridge: R2 = 0.043 (std: 0.071)

Model-based training with 13 models
Best R2: 0.112, Mean R2: 0.005
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.069 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9911, entropy=0.8100, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2038
  Round 1/3: Mean predicted reward = 14.103
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.069 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9913, entropy=0.8081, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2039
  Round 2/3: Mean predicted reward = 14.093
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.069 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9879, entropy=0.7989, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2220
  Round 3/3: Mean predicted reward = 13.995

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 9 Results ---
  Mean Oracle Reward: 14.189
  Min Oracle Reward: 4.487
  Max Oracle Reward: 19.107
  Std Oracle Reward: 2.542
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.005, Max: 0.112, Count: 13
  Total Sequences Evaluated: 626
    Oracle Count: 576 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 10/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 626

--- Round 10 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TTCATGCACCGCGCGAGGGC
  CTGGCAGGAGTGCTCCGCAC
  TGTCCCCCCACGGGGGGAAT
  CCGGTCGACAAGCGTGGTCC
  TCCGCACGAGCCGTTGCGAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.177
  Max reward: 19.238
  With intrinsic bonuses: 14.178

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9909, entropy=0.7980, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.7589

=== Surrogate Model Training ===
Total samples: 690

Training on 661 samples (removed 29 outliers)
Reward range: [9.16, 18.39], mean: 13.90
  Created 13 candidate models for data size 661
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.086 (std: 0.087)
  rf-tuned-xl: R2 = 0.098 (std: 0.076)
  gb-tuned-l: R2 = 0.119 (std: 0.072)
  gb-tuned-xl: R2 = 0.119 (std: 0.072)
  xgb-xl: R2 = -0.158 (std: 0.132)
  xgb-l: R2 = -0.158 (std: 0.132)
  mlp-adaptive-xl: R2 = -0.142 (std: 0.140)
  mlp-l: R2 = -0.119 (std: 0.180)
  svr-rbf-xl: R2 = 0.126 (std: 0.086)
  svr-poly-l: R2 = 0.126 (std: 0.086)
  knn-tuned-sqrt: R2 = 0.026 (std: 0.067)
  knn-tuned-l: R2 = 0.026 (std: 0.067)
  ridge: R2 = 0.074 (std: 0.071)

Model-based training with 13 models
Best R2: 0.126, Mean R2: 0.017
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.065 mlp-l:0.067 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9893, entropy=0.7510, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.9710
  Round 1/3: Mean predicted reward = 14.153
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.065 mlp-l:0.067 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9901, entropy=0.7109, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.1407
  Round 2/3: Mean predicted reward = 14.308
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.065 mlp-l:0.067 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.081 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.6766, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.4756
  Round 3/3: Mean predicted reward = 14.147

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 10 Results ---
  Mean Oracle Reward: 14.195
  Min Oracle Reward: 5.589
  Max Oracle Reward: 18.916
  Std Oracle Reward: 2.291
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.017, Max: 0.126, Count: 13
  Total Sequences Evaluated: 690
    Oracle Count: 640 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 11/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 690

--- Round 11 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GCTTCGGGGCCGACCATGAC
  TGACGCCTCTAGAAGGTCCG
  GCGGGTCCCCGTATCGACAG
  CTCTCTGGGAGAACGCCGGC
  GCCTGTCATGCGAGACGCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.123
  Max reward: 18.239
  With intrinsic bonuses: 14.173

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9859, entropy=0.6439, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.3685

=== Surrogate Model Training ===
Total samples: 754

Training on 725 samples (removed 29 outliers)
Reward range: [9.16, 18.60], mean: 13.94
  Created 13 candidate models for data size 725
Current R2 threshold: -0.29
  rf-tuned-l: R2 = 0.097 (std: 0.083)
  rf-tuned-xl: R2 = 0.091 (std: 0.098)
  gb-tuned-l: R2 = 0.129 (std: 0.094)
  gb-tuned-xl: R2 = 0.129 (std: 0.094)
  xgb-xl: R2 = -0.163 (std: 0.148)
  xgb-l: R2 = -0.163 (std: 0.148)
  mlp-adaptive-xl: R2 = -0.064 (std: 0.186)
  mlp-l: R2 = -0.084 (std: 0.198)
  svr-rbf-xl: R2 = 0.136 (std: 0.103)
  svr-poly-l: R2 = 0.136 (std: 0.103)
  knn-tuned-sqrt: R2 = 0.030 (std: 0.092)
  knn-tuned-l: R2 = 0.030 (std: 0.092)
  ridge: R2 = 0.067 (std: 0.105)

Model-based training with 13 models
Best R2: 0.136, Mean R2: 0.029
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.070 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9903, entropy=0.6163, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.5719
  Round 1/3: Mean predicted reward = 14.233
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.070 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9885, entropy=0.5952, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.7074
  Round 2/3: Mean predicted reward = 14.379
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.070 mlp-l:0.068 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.077 knn-tuned-l:0.077 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9878, entropy=0.5676, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.7169
  Round 3/3: Mean predicted reward = 14.246

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 11 Results ---
  Mean Oracle Reward: 14.126
  Min Oracle Reward: 5.091
  Max Oracle Reward: 18.097
  Std Oracle Reward: 2.284
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.029, Max: 0.136, Count: 13
  Total Sequences Evaluated: 754
    Oracle Count: 704 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 12/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 754
  Performance plateaued, reducing LR to 0.000100

--- Round 12 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ACGCGCCGTCTGGCATGCGA
  GACTGCAAAGGCGCGTTTCC
  TGACTCAGCGGGCTACCGCG
  CACCTGGCTGAGTGCCGACG
  CCCTGCAAGCTGGTCAGCGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.233
  Max reward: 19.280
  With intrinsic bonuses: 14.204

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9899, entropy=0.5400, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1581

=== Surrogate Model Training ===
Total samples: 818

Training on 786 samples (removed 32 outliers)
Reward range: [9.16, 18.63], mean: 13.97
  Created 13 candidate models for data size 786
Current R2 threshold: -0.27999999999999997
  rf-tuned-l: R2 = 0.127 (std: 0.077)
  rf-tuned-xl: R2 = 0.117 (std: 0.088)
  gb-tuned-l: R2 = 0.155 (std: 0.100)
  gb-tuned-xl: R2 = 0.155 (std: 0.100)
  xgb-xl: R2 = -0.073 (std: 0.152)
  xgb-l: R2 = -0.073 (std: 0.152)
  mlp-adaptive-xl: R2 = -0.015 (std: 0.126)
  mlp-l: R2 = -0.012 (std: 0.130)
  svr-rbf-xl: R2 = 0.156 (std: 0.108)
  svr-poly-l: R2 = 0.156 (std: 0.108)
  knn-tuned-sqrt: R2 = 0.017 (std: 0.113)
  knn-tuned-l: R2 = 0.017 (std: 0.113)
  ridge: R2 = 0.064 (std: 0.097)

Model-based training with 13 models
Best R2: 0.156, Mean R2: 0.061
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.071 mlp-l:0.071 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9887, entropy=0.5310, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0983
  Round 1/3: Mean predicted reward = 14.226
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.071 mlp-l:0.071 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9892, entropy=0.5255, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2077
  Round 2/3: Mean predicted reward = 14.209
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.081 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.071 mlp-l:0.071 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9906, entropy=0.5146, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2153
  Round 3/3: Mean predicted reward = 14.304

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 12 Results ---
  Mean Oracle Reward: 14.195
  Min Oracle Reward: 5.770
  Max Oracle Reward: 18.867
  Std Oracle Reward: 2.450
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.061, Max: 0.156, Count: 13
  Total Sequences Evaluated: 818
    Oracle Count: 768 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 13/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 818
  Performance plateaued, reducing LR to 0.000055

--- Round 13 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AATCTCCGGCAGCGGCTCGG
  CGCGCATGGTCCTACGGGAC
  CGGCAGTTCCACCGGTGCGA
  CGTCGTGCGCAGGCGTCCAA
  GGGTCACGTACCGCCGCTGA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.742
  Max reward: 18.205
  With intrinsic bonuses: 13.698

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9895, entropy=0.5045, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6060

=== Surrogate Model Training ===
Total samples: 882

Training on 849 samples (removed 33 outliers)
Reward range: [9.11, 18.63], mean: 13.96
  Created 13 candidate models for data size 849
Current R2 threshold: -0.27
  rf-tuned-l: R2 = 0.114 (std: 0.043)
  rf-tuned-xl: R2 = 0.103 (std: 0.044)
  gb-tuned-l: R2 = 0.170 (std: 0.089)
  gb-tuned-xl: R2 = 0.170 (std: 0.089)
  xgb-xl: R2 = -0.078 (std: 0.111)
  xgb-l: R2 = -0.078 (std: 0.111)
  mlp-adaptive-xl: R2 = 0.003 (std: 0.118)
  mlp-l: R2 = 0.006 (std: 0.132)
  svr-rbf-xl: R2 = 0.157 (std: 0.095)
  svr-poly-l: R2 = 0.157 (std: 0.095)
  knn-tuned-sqrt: R2 = 0.005 (std: 0.114)
  knn-tuned-l: R2 = 0.005 (std: 0.114)
  ridge: R2 = 0.071 (std: 0.093)

Model-based training with 13 models
Best R2: 0.170, Mean R2: 0.062
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.080 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.072 mlp-l:0.072 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9889, entropy=0.4882, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6213
  Round 1/3: Mean predicted reward = 14.138
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.080 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.072 mlp-l:0.072 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9887, entropy=0.4884, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6471
  Round 2/3: Mean predicted reward = 14.117
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.081 rf-tuned-xl:0.080 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.072 mlp-l:0.072 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.4884, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6380
  Round 3/3: Mean predicted reward = 14.297

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 13 Results ---
  Mean Oracle Reward: 13.708
  Min Oracle Reward: 7.644
  Max Oracle Reward: 18.039
  Std Oracle Reward: 2.188
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.062, Max: 0.170, Count: 13
  Total Sequences Evaluated: 882
    Oracle Count: 832 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 14/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 882

--- Round 14 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AGCGTCAGCTGTCCGCCGAG
  GTGCGTCGCGAGCATACCGC
  AGTTTGGCGCAGGCCGACCC
  GGCAGTCCACGACGCTCGGT
  CTGATCGGTACCGAGGCCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.594
  Max reward: 17.918
  With intrinsic bonuses: 13.610

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.4854, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3965

=== Surrogate Model Training ===
Total samples: 946

Training on 911 samples (removed 35 outliers)
Reward range: [9.11, 18.63], mean: 13.95
  Created 13 candidate models for data size 911
Current R2 threshold: -0.26
  rf-tuned-l: R2 = 0.061 (std: 0.039)
  rf-tuned-xl: R2 = 0.053 (std: 0.041)
  gb-tuned-l: R2 = 0.133 (std: 0.075)
  gb-tuned-xl: R2 = 0.133 (std: 0.075)
  xgb-xl: R2 = -0.187 (std: 0.055)
  xgb-l: R2 = -0.187 (std: 0.055)
  mlp-adaptive-xl: R2 = -0.018 (std: 0.097)
  mlp-l: R2 = -0.017 (std: 0.150)
  svr-rbf-xl: R2 = 0.148 (std: 0.097)
  svr-poly-l: R2 = 0.148 (std: 0.097)
  knn-tuned-sqrt: R2 = -0.002 (std: 0.070)
  knn-tuned-l: R2 = -0.002 (std: 0.070)
  ridge: R2 = 0.062 (std: 0.072)

Model-based training with 13 models
Best R2: 0.148, Mean R2: 0.025
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.062 xgb-l:0.062 mlp-adaptive-xl:0.073 mlp-l:0.073 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9886, entropy=0.4813, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4015
  Round 1/3: Mean predicted reward = 14.160
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.062 xgb-l:0.062 mlp-adaptive-xl:0.073 mlp-l:0.073 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9878, entropy=0.4819, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4193
  Round 2/3: Mean predicted reward = 14.092
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.062 xgb-l:0.062 mlp-adaptive-xl:0.073 mlp-l:0.073 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=0.4666, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4351
  Round 3/3: Mean predicted reward = 14.102

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 14 Results ---
  Mean Oracle Reward: 13.582
  Min Oracle Reward: 5.394
  Max Oracle Reward: 17.722
  Std Oracle Reward: 2.423
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.025, Max: 0.148, Count: 13
  Total Sequences Evaluated: 946
    Oracle Count: 896 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 15/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 946

--- Round 15 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CCCAGGGTTACGGTGCGACC
  GGCTTGGCCCAGCGTCAGCA
  TACGGTCTCGCGACGACGGC
  GACAGTGCGGCTTCAGGCCC
  CCGGGCCGACGATGCTAGTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.410
  Max reward: 16.399
  With intrinsic bonuses: 13.401

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9895, entropy=0.4689, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.9833

=== Surrogate Model Training ===
Total samples: 1010

Training on 972 samples (removed 38 outliers)
Reward range: [9.08, 18.63], mean: 13.94
  Created 13 candidate models for data size 972
Current R2 threshold: -0.25
  rf-tuned-l: R2 = 0.036 (std: 0.092)
  rf-tuned-xl: R2 = 0.036 (std: 0.103)
  gb-tuned-l: R2 = 0.097 (std: 0.108)
  gb-tuned-xl: R2 = 0.097 (std: 0.108)
  xgb-xl: R2 = -0.171 (std: 0.098)
  xgb-l: R2 = -0.171 (std: 0.098)
  mlp-adaptive-xl: R2 = -0.025 (std: 0.126)
  mlp-l: R2 = -0.034 (std: 0.144)
  svr-rbf-xl: R2 = 0.111 (std: 0.116)
  svr-poly-l: R2 = 0.111 (std: 0.116)
  knn-tuned-sqrt: R2 = -0.019 (std: 0.091)
  knn-tuned-l: R2 = -0.019 (std: 0.091)
  ridge: R2 = 0.037 (std: 0.069)

Model-based training with 13 models
Best R2: 0.111, Mean R2: 0.007
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.074 mlp-l:0.074 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9864, entropy=0.4659, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.1588
  Round 1/3: Mean predicted reward = 14.045
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.074 mlp-l:0.074 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9862, entropy=0.4465, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.5581
  Round 2/3: Mean predicted reward = 14.292
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.074 mlp-l:0.074 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9867, entropy=0.4323, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.7475
  Round 3/3: Mean predicted reward = 14.202

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 15 Results ---
  Mean Oracle Reward: 13.397
  Min Oracle Reward: 1.544
  Max Oracle Reward: 16.774
  Std Oracle Reward: 2.559
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.007, Max: 0.111, Count: 13
  Total Sequences Evaluated: 1010
    Oracle Count: 960 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 16/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1010

--- Round 16 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGCCCCAGTGCCGGTGATAC
  ACCCTAGGACGTGGATGCCG
  ATCCGAGCGTGGCCGACGTC
  GGGCTCGATGTAGACAGCCC
  CAACGTGCCTGGCGAGTCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.383
  Max reward: 17.260
  With intrinsic bonuses: 13.386

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9889, entropy=0.4198, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 2.8687

=== Surrogate Model Training ===
Total samples: 1074

Training on 1033 samples (removed 41 outliers)
Reward range: [9.06, 18.63], mean: 13.93
  Created 13 candidate models for data size 1033
Current R2 threshold: -0.24
  rf-tuned-l: R2 = 0.018 (std: 0.117)
  rf-tuned-xl: R2 = 0.009 (std: 0.118)
  gb-tuned-l: R2 = 0.078 (std: 0.106)
  gb-tuned-xl: R2 = 0.078 (std: 0.106)
  xgb-xl: R2 = -0.176 (std: 0.117)
  xgb-l: R2 = -0.176 (std: 0.117)
  mlp-adaptive-xl: R2 = -0.026 (std: 0.108)
  mlp-l: R2 = -0.021 (std: 0.105)
  svr-rbf-xl: R2 = 0.093 (std: 0.122)
  svr-poly-l: R2 = 0.093 (std: 0.122)
  knn-tuned-sqrt: R2 = -0.057 (std: 0.099)
  knn-tuned-l: R2 = -0.057 (std: 0.099)
  ridge: R2 = 0.010 (std: 0.080)

Model-based training with 13 models
Best R2: 0.093, Mean R2: -0.010
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.075 mlp-l:0.076 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=0.4118, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.2393
  Round 1/3: Mean predicted reward = 13.855
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.075 mlp-l:0.076 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9883, entropy=0.4027, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.2788
  Round 2/3: Mean predicted reward = 14.084
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.075 mlp-l:0.076 svr-rbf-xl:0.085 svr-poly-l:0.085 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9889, entropy=0.3859, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 3.6158
  Round 3/3: Mean predicted reward = 13.980

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 16 Results ---
  Mean Oracle Reward: 13.329
  Min Oracle Reward: 3.440
  Max Oracle Reward: 16.966
  Std Oracle Reward: 2.671
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.010, Max: 0.093, Count: 13
  Total Sequences Evaluated: 1074
    Oracle Count: 1024 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 17/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1074
  Performance plateaued, reducing LR to 0.000100

--- Round 17 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TCCGTGCCAACGGGGCGCAT
  ATCCCCTGAGGGGGGACCTC
  GCAGGAATGCCCGTGCTCGC
  AAGCCGCATGCCCGGGTGTC
  CCGGGATATTCCCGGGGCCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 12.959
  Max reward: 17.762
  With intrinsic bonuses: 12.959

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=3.5698, entropy=0.3619, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0012

=== Surrogate Model Training ===
Total samples: 1138

Training on 1092 samples (removed 46 outliers)
Reward range: [8.87, 18.63], mean: 13.92
  Created 13 candidate models for data size 1092
Current R2 threshold: -0.22999999999999998
  rf-tuned-l: R2 = 0.010 (std: 0.064)
  rf-tuned-xl: R2 = 0.023 (std: 0.060)
  gb-tuned-l: R2 = 0.075 (std: 0.076)
  gb-tuned-xl: R2 = 0.075 (std: 0.076)
  xgb-xl: R2 = -0.216 (std: 0.092)
  xgb-l: R2 = -0.216 (std: 0.092)
  mlp-adaptive-xl: R2 = 0.022 (std: 0.072)
  mlp-l: R2 = -0.015 (std: 0.121)
  svr-rbf-xl: R2 = 0.103 (std: 0.093)
  svr-poly-l: R2 = 0.103 (std: 0.093)
  knn-tuned-sqrt: R2 = -0.084 (std: 0.046)
  knn-tuned-l: R2 = -0.084 (std: 0.046)
  ridge: R2 = 0.010 (std: 0.078)

Model-based training with 13 models
Best R2: 0.103, Mean R2: -0.015
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.080 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9879, entropy=0.3668, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0284
  Round 1/3: Mean predicted reward = 14.056
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.080 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9858, entropy=0.3735, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0622
  Round 2/3: Mean predicted reward = 14.075
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.080 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.063 xgb-l:0.063 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.086 svr-poly-l:0.086 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9884, entropy=0.3598, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1462
  Round 3/3: Mean predicted reward = 14.027

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 17 Results ---
  Mean Oracle Reward: 12.956
  Min Oracle Reward: 4.860
  Max Oracle Reward: 18.172
  Std Oracle Reward: 3.079
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.015, Max: 0.103, Count: 13
  Total Sequences Evaluated: 1138
    Oracle Count: 1088 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 18/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1138

--- Round 18 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGTCGAGTCCAGCACGTGGC
  GCCCTTGGAGCAAGCTCCGG
  CGGCGTTAAGTCGGGCCCAC
  GGACCCGCGACCTCGTAGTG
  CGGTCGACGCAGCGTAGCTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.045
  Max reward: 17.402
  With intrinsic bonuses: 13.049

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9895, entropy=0.3579, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0025

=== Surrogate Model Training ===
Total samples: 1202

Training on 1148 samples (removed 54 outliers)
Reward range: [8.87, 18.63], mean: 13.91
  Created 13 candidate models for data size 1148
Current R2 threshold: -0.21999999999999997
  rf-tuned-l: R2 = 0.045 (std: 0.053)
  rf-tuned-xl: R2 = 0.043 (std: 0.050)
  gb-tuned-l: R2 = 0.097 (std: 0.065)
  gb-tuned-xl: R2 = 0.097 (std: 0.065)
  xgb-xl: R2 = -0.153 (std: 0.106)
  xgb-l: R2 = -0.153 (std: 0.106)
  mlp-adaptive-xl: R2 = 0.017 (std: 0.076)
  mlp-l: R2 = -0.012 (std: 0.128)
  svr-rbf-xl: R2 = 0.110 (std: 0.070)
  svr-poly-l: R2 = 0.110 (std: 0.070)
  knn-tuned-sqrt: R2 = -0.029 (std: 0.055)
  knn-tuned-l: R2 = -0.029 (std: 0.055)
  ridge: R2 = 0.024 (std: 0.070)

Model-based training with 13 models
Best R2: 0.110, Mean R2: 0.013
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.075 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9883, entropy=0.3710, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9233
  Round 1/3: Mean predicted reward = 13.722
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.075 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9891, entropy=0.3670, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8327
  Round 2/3: Mean predicted reward = 14.191
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.075 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9874, entropy=0.3593, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0532
  Round 3/3: Mean predicted reward = 14.009

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 18 Results ---
  Mean Oracle Reward: 13.051
  Min Oracle Reward: 5.758
  Max Oracle Reward: 17.617
  Std Oracle Reward: 2.672
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.013, Max: 0.110, Count: 13
  Total Sequences Evaluated: 1202
    Oracle Count: 1152 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 19/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1202

--- Round 19 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGGGAGCCCTAGGTATCGCC
  TGACAGGGGCCATCCGCCGT
  CACCCTAGGTTGCAGCGGGC
  AGGCGCGCCTGCTACGACTG
  CGAGGCCGATGTGCCTGCCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.703
  Max reward: 18.577
  With intrinsic bonuses: 13.730

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9891, entropy=0.3568, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2442

=== Surrogate Model Training ===
Total samples: 1266

Training on 1209 samples (removed 57 outliers)
Reward range: [8.87, 18.63], mean: 13.92
  Created 13 candidate models for data size 1209
Current R2 threshold: -0.21
  rf-tuned-l: R2 = 0.039 (std: 0.016)
  rf-tuned-xl: R2 = 0.038 (std: 0.024)
  gb-tuned-l: R2 = 0.088 (std: 0.052)
  gb-tuned-xl: R2 = 0.088 (std: 0.052)
  xgb-xl: R2 = -0.170 (std: 0.061)
  xgb-l: R2 = -0.170 (std: 0.061)
  mlp-adaptive-xl: R2 = 0.037 (std: 0.084)
  mlp-l: R2 = 0.002 (std: 0.064)
  svr-rbf-xl: R2 = 0.110 (std: 0.073)
  svr-poly-l: R2 = 0.110 (std: 0.073)
  knn-tuned-sqrt: R2 = -0.001 (std: 0.062)
  knn-tuned-l: R2 = -0.001 (std: 0.062)
  ridge: R2 = 0.029 (std: 0.054)

Model-based training with 13 models
Best R2: 0.110, Mean R2: 0.015
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9878, entropy=0.3539, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2346
  Round 1/3: Mean predicted reward = 13.945
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9907, entropy=0.3606, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2553
  Round 2/3: Mean predicted reward = 14.129
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9896, entropy=0.3672, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2727
  Round 3/3: Mean predicted reward = 14.055

  === Progress Analysis ===
  Status: NORMAL

--- Round 19 Results ---
  Mean Oracle Reward: 13.712
  Min Oracle Reward: 3.050
  Max Oracle Reward: 18.406
  Std Oracle Reward: 2.621
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.015, Max: 0.110, Count: 13
  Total Sequences Evaluated: 1266
    Oracle Count: 1216 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 20/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1266
  Consistent improvement, increasing LR to 0.000360

--- Round 20 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGCCTTCTGGCCCGGAAGGA
  ACCTCAGGCCGGTGATCGCG
  GCGGCCACCTGCACTAGTGG
  CGTCTGTAGTACAGGAGCCC
  CTCCTGGGCCGAGCTAGCAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.626
  Max reward: 17.337
  With intrinsic bonuses: 13.626

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9887, entropy=0.3595, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.6206

=== Surrogate Model Training ===
Total samples: 1330

Training on 1270 samples (removed 60 outliers)
Reward range: [8.87, 18.63], mean: 13.93
  Created 13 candidate models for data size 1270
Current R2 threshold: -0.19999999999999998
  rf-tuned-l: R2 = 0.046 (std: 0.033)
  rf-tuned-xl: R2 = 0.057 (std: 0.027)
  gb-tuned-l: R2 = 0.086 (std: 0.046)
  gb-tuned-xl: R2 = 0.086 (std: 0.046)
  xgb-xl: R2 = -0.164 (std: 0.031)
  xgb-l: R2 = -0.164 (std: 0.031)
  mlp-adaptive-xl: R2 = 0.025 (std: 0.077)
  mlp-l: R2 = 0.037 (std: 0.065)
  svr-rbf-xl: R2 = 0.099 (std: 0.095)
  svr-poly-l: R2 = 0.099 (std: 0.095)
  knn-tuned-sqrt: R2 = -0.017 (std: 0.061)
  knn-tuned-l: R2 = -0.017 (std: 0.061)
  ridge: R2 = 0.010 (std: 0.039)

Model-based training with 13 models
Best R2: 0.099, Mean R2: 0.014
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9881, entropy=0.3826, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.4966
  Round 1/3: Mean predicted reward = 13.903
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9881, entropy=0.3788, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.4389
  Round 2/3: Mean predicted reward = 14.038
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.064 xgb-l:0.064 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9897, entropy=0.3849, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.3629
  Round 3/3: Mean predicted reward = 14.073

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 20 Results ---
  Mean Oracle Reward: 13.644
  Min Oracle Reward: 1.887
  Max Oracle Reward: 17.445
  Std Oracle Reward: 2.753
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.014, Max: 0.099, Count: 13
  Total Sequences Evaluated: 1330
    Oracle Count: 1280 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 21/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1330

--- Round 21 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GCATCCCGTCTACGGGGCAG
  CCGGCGGCTAACCGTTGAGC
  CGTACGTTGCTGCGAGCACA
  ACAGCTCTGGCGGCTCCGGA
  CTCGAGCGGGTGAGCATCCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.720
  Max reward: 18.353
  With intrinsic bonuses: 13.748

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=0.3765, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0444

=== Surrogate Model Training ===
Total samples: 1394

Training on 1333 samples (removed 61 outliers)
Reward range: [8.87, 18.63], mean: 13.92
  Created 13 candidate models for data size 1333
Current R2 threshold: -0.19
  rf-tuned-l: R2 = 0.030 (std: 0.032)
  rf-tuned-xl: R2 = 0.039 (std: 0.035)
  gb-tuned-l: R2 = 0.093 (std: 0.064)
  gb-tuned-xl: R2 = 0.093 (std: 0.064)
  xgb-xl: R2 = -0.147 (std: 0.063)
  xgb-l: R2 = -0.147 (std: 0.063)
  mlp-adaptive-xl: R2 = 0.022 (std: 0.073)
  mlp-l: R2 = 0.023 (std: 0.094)
  svr-rbf-xl: R2 = 0.110 (std: 0.100)
  svr-poly-l: R2 = 0.110 (std: 0.100)
  knn-tuned-sqrt: R2 = 0.001 (std: 0.080)
  knn-tuned-l: R2 = 0.001 (std: 0.080)
  ridge: R2 = 0.029 (std: 0.055)

Model-based training with 13 models
Best R2: 0.110, Mean R2: 0.020
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9904, entropy=0.3813, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8830
  Round 1/3: Mean predicted reward = 14.144
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9898, entropy=0.3669, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9353
  Round 2/3: Mean predicted reward = 14.180
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.075 knn-tuned-l:0.075 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9887, entropy=0.3558, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8793
  Round 3/3: Mean predicted reward = 14.125

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 21 Results ---
  Mean Oracle Reward: 13.740
  Min Oracle Reward: 7.394
  Max Oracle Reward: 18.386
  Std Oracle Reward: 1.944
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.020, Max: 0.110, Count: 13
  Total Sequences Evaluated: 1394
    Oracle Count: 1344 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}]

======================================================================
EXPERIMENT ROUND 22/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1394
  Performance plateaued, reducing LR to 0.000100

--- Round 22 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GTTAGAGGCCCTCGGACCCG
  GCCCCGGCAAGGCAGTCGTT
  GTGTCGGCACCCGAGGCCAT
  CCCCGGAGCTGGCGGACATT
  TCGCACCGTGACTCGGAGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.133
  Max reward: 20.031
  With intrinsic bonuses: 14.092

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9881, entropy=0.3403, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1829

=== Surrogate Model Training ===
Total samples: 1458

Training on 1396 samples (removed 62 outliers)
Reward range: [8.87, 18.63], mean: 13.93
  Created 13 candidate models for data size 1396
Current R2 threshold: -0.18
  rf-tuned-l: R2 = 0.049 (std: 0.025)
  rf-tuned-xl: R2 = 0.049 (std: 0.021)
  gb-tuned-l: R2 = 0.084 (std: 0.039)
  gb-tuned-xl: R2 = 0.084 (std: 0.039)
  xgb-xl: R2 = -0.143 (std: 0.053)
  xgb-l: R2 = -0.143 (std: 0.053)
  mlp-adaptive-xl: R2 = 0.042 (std: 0.044)
  mlp-l: R2 = 0.030 (std: 0.072)
  svr-rbf-xl: R2 = 0.106 (std: 0.068)
  svr-poly-l: R2 = 0.106 (std: 0.068)
  knn-tuned-sqrt: R2 = -0.021 (std: 0.036)
  knn-tuned-l: R2 = -0.021 (std: 0.036)
  ridge: R2 = 0.026 (std: 0.043)

Model-based training with 13 models
Best R2: 0.106, Mean R2: 0.019
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9891, entropy=0.3308, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2684
  Round 1/3: Mean predicted reward = 14.160
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.3215, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3717
  Round 2/3: Mean predicted reward = 14.052
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.065 xgb-l:0.065 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9877, entropy=0.3193, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4312
  Round 3/3: Mean predicted reward = 14.228

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 22 Results ---
  Mean Oracle Reward: 14.141
  Min Oracle Reward: 9.148
  Max Oracle Reward: 20.196
  Std Oracle Reward: 2.116
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.019, Max: 0.106, Count: 13
  Total Sequences Evaluated: 1458
    Oracle Count: 1408 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 23/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1458
  Consistent improvement, increasing LR to 0.000132

--- Round 23 Configuration ---
Learning rate: 0.000132
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CTTCCGCGCGCATAAGGCGG
  GCAAGATGCGGGCTCCCGCT
  GGGACTAGCAGCCTCCGTGC
  GGGGGGATCCCGCTTCCCAA
  CAGAGGGCGCTAGCTCCTCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.789
  Max reward: 18.476
  With intrinsic bonuses: 13.811

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9896, entropy=0.3235, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3437

=== Surrogate Model Training ===
Total samples: 1522

Training on 1457 samples (removed 65 outliers)
Reward range: [8.92, 18.71], mean: 13.93
  Created 13 candidate models for data size 1457
Current R2 threshold: -0.16999999999999998
  rf-tuned-l: R2 = 0.040 (std: 0.031)
  rf-tuned-xl: R2 = 0.049 (std: 0.038)
  gb-tuned-l: R2 = 0.086 (std: 0.049)
  gb-tuned-xl: R2 = 0.086 (std: 0.049)
  xgb-xl: R2 = -0.148 (std: 0.077)
  xgb-l: R2 = -0.148 (std: 0.077)
  mlp-adaptive-xl: R2 = 0.018 (std: 0.077)
  mlp-l: R2 = 0.014 (std: 0.077)
  svr-rbf-xl: R2 = 0.092 (std: 0.074)
  svr-poly-l: R2 = 0.092 (std: 0.074)
  knn-tuned-sqrt: R2 = -0.049 (std: 0.025)
  knn-tuned-l: R2 = -0.049 (std: 0.025)
  ridge: R2 = 0.014 (std: 0.044)

Model-based training with 13 models
Best R2: 0.092, Mean R2: 0.007
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9888, entropy=0.3110, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3833
  Round 1/3: Mean predicted reward = 13.834
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.2939, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5631
  Round 2/3: Mean predicted reward = 13.997
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9866, entropy=0.2993, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6048
  Round 3/3: Mean predicted reward = 14.154

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 23 Results ---
  Mean Oracle Reward: 13.862
  Min Oracle Reward: 7.097
  Max Oracle Reward: 18.296
  Std Oracle Reward: 1.820
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.007, Max: 0.092, Count: 13
  Total Sequences Evaluated: 1522
    Oracle Count: 1472 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 24/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1522

--- Round 24 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GACCCACGTGTGCGTGACCG
  GCCTTGAAACCCGGGGTCCG
  CCGGACTTGGGTACCCGAGC
  GCTTGACCCTGCGCAAGGCG
  GACCGTGGGCCACCTTACGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.269
  Max reward: 16.973
  With intrinsic bonuses: 13.226

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9886, entropy=0.2795, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1091

=== Surrogate Model Training ===
Total samples: 1586

Training on 1520 samples (removed 66 outliers)
Reward range: [8.87, 18.71], mean: 13.91
  Created 13 candidate models for data size 1520
Current R2 threshold: -0.15999999999999998
  rf-tuned-l: R2 = 0.061 (std: 0.056)
  rf-tuned-xl: R2 = 0.051 (std: 0.058)
  gb-tuned-l: R2 = 0.119 (std: 0.038)
  gb-tuned-xl: R2 = 0.119 (std: 0.038)
  xgb-xl: R2 = -0.118 (std: 0.078)
  xgb-l: R2 = -0.118 (std: 0.078)
  mlp-adaptive-xl: R2 = 0.034 (std: 0.055)
  mlp-l: R2 = 0.025 (std: 0.033)
  svr-rbf-xl: R2 = 0.115 (std: 0.051)
  svr-poly-l: R2 = 0.115 (std: 0.051)
  knn-tuned-sqrt: R2 = -0.025 (std: 0.051)
  knn-tuned-l: R2 = -0.025 (std: 0.051)
  ridge: R2 = 0.034 (std: 0.033)

Model-based training with 13 models
Best R2: 0.119, Mean R2: 0.030
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9912, entropy=0.2799, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0787
  Round 1/3: Mean predicted reward = 13.893
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9890, entropy=0.2804, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1117
  Round 2/3: Mean predicted reward = 14.161
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.084 gb-tuned-xl:0.084 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.077 mlp-l:0.076 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9867, entropy=0.2736, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1571
  Round 3/3: Mean predicted reward = 13.944

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 24 Results ---
  Mean Oracle Reward: 13.243
  Min Oracle Reward: 4.079
  Max Oracle Reward: 17.190
  Std Oracle Reward: 2.443
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.030, Max: 0.119, Count: 13
  Total Sequences Evaluated: 1586
    Oracle Count: 1536 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 25/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1586

--- Round 25 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGGCGTATACGACTCGCGCC
  CGTAACTCGGCCCGGGATGC
  GCGGGGCTATCGGACTCCAC
  TGCCACCACGCTAGGGCGTG
  TGCACGCGGCGCCCAGGATT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.525
  Max reward: 18.739
  With intrinsic bonuses: 13.557

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9876, entropy=0.2769, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7309

=== Surrogate Model Training ===
Total samples: 1650

Training on 1584 samples (removed 66 outliers)
Reward range: [8.87, 18.74], mean: 13.90
  Created 13 candidate models for data size 1584
Current R2 threshold: -0.15
  rf-tuned-l: R2 = 0.084 (std: 0.050)
  rf-tuned-xl: R2 = 0.083 (std: 0.041)
  gb-tuned-l: R2 = 0.128 (std: 0.035)
  gb-tuned-xl: R2 = 0.128 (std: 0.035)
  xgb-xl: R2 = -0.064 (std: 0.072)
  xgb-l: R2 = -0.064 (std: 0.072)
  mlp-adaptive-xl: R2 = 0.041 (std: 0.038)
  mlp-l: R2 = 0.075 (std: 0.054)
  svr-rbf-xl: R2 = 0.125 (std: 0.055)
  svr-poly-l: R2 = 0.125 (std: 0.055)
  knn-tuned-sqrt: R2 = -0.001 (std: 0.058)
  knn-tuned-l: R2 = -0.001 (std: 0.058)
  ridge: R2 = 0.037 (std: 0.036)

Model-based training with 13 models
Best R2: 0.128, Mean R2: 0.053
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.076 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9885, entropy=0.2701, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9626
  Round 1/3: Mean predicted reward = 13.765
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.076 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9880, entropy=0.2699, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2517
  Round 2/3: Mean predicted reward = 14.208
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.083 gb-tuned-xl:0.083 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.076 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9870, entropy=0.2540, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.3665
  Round 3/3: Mean predicted reward = 14.042

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 25 Results ---
  Mean Oracle Reward: 13.535
  Min Oracle Reward: 9.152
  Max Oracle Reward: 18.691
  Std Oracle Reward: 1.934
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.053, Max: 0.128, Count: 13
  Total Sequences Evaluated: 1650
    Oracle Count: 1600 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 26/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1650

--- Round 26 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GACGGCTCAAGCGCGTCCTG
  CCACACGGTGCCTTCGGGGA
  ATACCGCGTGCAGGGCCCGT
  GGCGAGGCTTAGCCCCAGTC
  CGCTGCGGACATCGTCGAGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.934
  Max reward: 19.874
  With intrinsic bonuses: 13.931

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9889, entropy=0.2371, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8172

=== Surrogate Model Training ===
Total samples: 1714

Training on 1646 samples (removed 68 outliers)
Reward range: [8.87, 18.74], mean: 13.90
  Created 13 candidate models for data size 1646
Current R2 threshold: -0.13999999999999999
  rf-tuned-l: R2 = 0.067 (std: 0.049)
  rf-tuned-xl: R2 = 0.074 (std: 0.043)
  gb-tuned-l: R2 = 0.117 (std: 0.038)
  gb-tuned-xl: R2 = 0.117 (std: 0.038)
  xgb-xl: R2 = -0.083 (std: 0.071)
  xgb-l: R2 = -0.083 (std: 0.071)
  mlp-adaptive-xl: R2 = 0.062 (std: 0.030)
  mlp-l: R2 = 0.056 (std: 0.026)
  svr-rbf-xl: R2 = 0.127 (std: 0.054)
  svr-poly-l: R2 = 0.127 (std: 0.054)
  knn-tuned-sqrt: R2 = -0.015 (std: 0.068)
  knn-tuned-l: R2 = -0.015 (std: 0.068)
  ridge: R2 = 0.044 (std: 0.038)

Model-based training with 13 models
Best R2: 0.127, Mean R2: 0.046
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9868, entropy=0.2411, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3686
  Round 1/3: Mean predicted reward = 13.875
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9917, entropy=0.2353, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6378
  Round 2/3: Mean predicted reward = 13.988
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9880, entropy=0.2279, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0986
  Round 3/3: Mean predicted reward = 14.029

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 26 Results ---
  Mean Oracle Reward: 13.933
  Min Oracle Reward: 3.516
  Max Oracle Reward: 19.999
  Std Oracle Reward: 2.433
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.046, Max: 0.127, Count: 13
  Total Sequences Evaluated: 1714
    Oracle Count: 1664 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 27/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1714
  Consistent improvement, increasing LR to 0.000240

--- Round 27 Configuration ---
Learning rate: 0.000240
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGGTACGCTGCACGACCGTG
  CGTGGGCATCCGTGCCAGCA
  TAGCGTGAATCCGCGCCCGG
  CTGGACCCTGACGACGCGTG
  CCAAGCCGTCTGGAGCCGTG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.085
  Max reward: 17.172
  With intrinsic bonuses: 13.099

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9903, entropy=0.2095, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4528

=== Surrogate Model Training ===
Total samples: 1778

Training on 1705 samples (removed 73 outliers)
Reward range: [8.87, 18.74], mean: 13.89
  Created 13 candidate models for data size 1705
Current R2 threshold: -0.12999999999999998
  rf-tuned-l: R2 = 0.055 (std: 0.068)
  rf-tuned-xl: R2 = 0.043 (std: 0.064)
  gb-tuned-l: R2 = 0.101 (std: 0.042)
  gb-tuned-xl: R2 = 0.101 (std: 0.042)
  xgb-xl: R2 = -0.107 (std: 0.074)
  xgb-l: R2 = -0.107 (std: 0.074)
  mlp-adaptive-xl: R2 = 0.053 (std: 0.030)
  mlp-l: R2 = 0.039 (std: 0.023)
  svr-rbf-xl: R2 = 0.116 (std: 0.055)
  svr-poly-l: R2 = 0.116 (std: 0.055)
  knn-tuned-sqrt: R2 = -0.014 (std: 0.074)
  knn-tuned-l: R2 = -0.014 (std: 0.074)
  ridge: R2 = 0.042 (std: 0.022)

Model-based training with 13 models
Best R2: 0.116, Mean R2: 0.033
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9901, entropy=0.2242, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1104
  Round 1/3: Mean predicted reward = 13.724
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9905, entropy=0.2137, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4563
  Round 2/3: Mean predicted reward = 14.050
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9884, entropy=0.2323, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6943
  Round 3/3: Mean predicted reward = 14.090

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 27 Results ---
  Mean Oracle Reward: 13.086
  Min Oracle Reward: 6.433
  Max Oracle Reward: 17.032
  Std Oracle Reward: 2.321
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.033, Max: 0.116, Count: 13
  Total Sequences Evaluated: 1778
    Oracle Count: 1728 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 28/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1778

--- Round 28 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGCGTTGACCTGAGCCCGAG
  CTGCCGGCCGGCGATGATAC
  GCGCCCGTCGAATGCCGGTA
  GAGGGCCTCTACAGGCTGCC
  CTACGGACAGGTGCCGTCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.970
  Max reward: 18.964
  With intrinsic bonuses: 14.006

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9899, entropy=0.2266, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1519

=== Surrogate Model Training ===
Total samples: 1842

Training on 1767 samples (removed 75 outliers)
Reward range: [8.87, 18.74], mean: 13.89
  Created 13 candidate models for data size 1767
Current R2 threshold: -0.12
  rf-tuned-l: R2 = 0.039 (std: 0.063)
  rf-tuned-xl: R2 = 0.034 (std: 0.065)
  gb-tuned-l: R2 = 0.088 (std: 0.031)
  gb-tuned-xl: R2 = 0.088 (std: 0.031)
  xgb-xl: R2 = -0.097 (std: 0.084)
  xgb-l: R2 = -0.097 (std: 0.084)
  mlp-adaptive-xl: R2 = 0.062 (std: 0.033)
  mlp-l: R2 = 0.043 (std: 0.042)
  svr-rbf-xl: R2 = 0.110 (std: 0.048)
  svr-poly-l: R2 = 0.110 (std: 0.048)
  knn-tuned-sqrt: R2 = -0.056 (std: 0.115)
  knn-tuned-l: R2 = -0.056 (std: 0.115)
  ridge: R2 = 0.037 (std: 0.021)

Model-based training with 13 models
Best R2: 0.110, Mean R2: 0.023
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.2185, kl_div=0.0000
    Epoch 1: policy_loss=-0.0178, value_loss=0.9880, entropy=0.2187, kl_div=0.0301
  Round 1/3: Mean predicted reward = 13.879
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9893, entropy=0.2257, kl_div=0.0000
    Epoch 1: policy_loss=-0.0228, value_loss=0.9892, entropy=0.2261, kl_div=0.0156
  Round 2/3: Mean predicted reward = 14.260
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.078 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9895, entropy=0.2210, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2724
  Round 3/3: Mean predicted reward = 14.260

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 28 Results ---
  Mean Oracle Reward: 13.994
  Min Oracle Reward: 7.917
  Max Oracle Reward: 19.276
  Std Oracle Reward: 2.281
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.023, Max: 0.110, Count: 13
  Total Sequences Evaluated: 1842
    Oracle Count: 1792 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 29/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1842

--- Round 29 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ACCCGAGATGCGTGTCGCCG
  TTCAGCCAGCAGGGCCCGTG
  TGCCGGAGCGTCGTAACGCC
  CTGGTCAGACGCAGGGCTCC
  GAGGGCGTACCGCATCGCTA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.205
  Max reward: 18.705
  With intrinsic bonuses: 14.211

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9885, entropy=0.2156, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1284

=== Surrogate Model Training ===
Total samples: 1906

Training on 1829 samples (removed 77 outliers)
Reward range: [8.87, 18.74], mean: 13.91
  Created 13 candidate models for data size 1829
Current R2 threshold: -0.10999999999999999
  rf-tuned-l: R2 = 0.067 (std: 0.050)
  rf-tuned-xl: R2 = 0.070 (std: 0.055)
  gb-tuned-l: R2 = 0.109 (std: 0.037)
  gb-tuned-xl: R2 = 0.109 (std: 0.037)
  xgb-xl: R2 = -0.092 (std: 0.057)
  xgb-l: R2 = -0.092 (std: 0.057)
  mlp-adaptive-xl: R2 = 0.055 (std: 0.040)
  mlp-l: R2 = 0.046 (std: 0.041)
  svr-rbf-xl: R2 = 0.131 (std: 0.032)
  svr-poly-l: R2 = 0.131 (std: 0.032)
  knn-tuned-sqrt: R2 = -0.037 (std: 0.063)
  knn-tuned-l: R2 = -0.037 (std: 0.063)
  ridge: R2 = 0.046 (std: 0.027)

Model-based training with 13 models
Best R2: 0.131, Mean R2: 0.039
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9876, entropy=0.2150, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0655
  Round 1/3: Mean predicted reward = 13.847
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9886, entropy=0.2155, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1142
  Round 2/3: Mean predicted reward = 14.084
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.067 xgb-l:0.067 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.077 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9901, entropy=0.2282, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1480
  Round 3/3: Mean predicted reward = 14.137

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 29 Results ---
  Mean Oracle Reward: 14.211
  Min Oracle Reward: 8.436
  Max Oracle Reward: 18.962
  Std Oracle Reward: 2.023
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.039, Max: 0.131, Count: 13
  Total Sequences Evaluated: 1906
    Oracle Count: 1856 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 30/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1906
  Consistent improvement, increasing LR to 0.000360

--- Round 30 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGAAGGTCCTTGGCCCGCAG
  ATGCGGCGCGCGAGCTTCCA
  ACGTGACGCGATTCGCCCGG
  CCCCGCGGCTGAGAACTGGT
  CGGTACCCTCCCTGGGGAAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.022
  Max reward: 18.470
  With intrinsic bonuses: 14.003

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9857, entropy=0.2210, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.3380

=== Surrogate Model Training ===
Total samples: 1970

Training on 1891 samples (removed 79 outliers)
Reward range: [8.87, 18.74], mean: 13.92
  Created 13 candidate models for data size 1891
Current R2 threshold: -0.09999999999999998
  rf-tuned-l: R2 = 0.090 (std: 0.027)
  rf-tuned-xl: R2 = 0.090 (std: 0.038)
  gb-tuned-l: R2 = 0.107 (std: 0.033)
  gb-tuned-xl: R2 = 0.107 (std: 0.033)
  xgb-xl: R2 = -0.037 (std: 0.040)
  xgb-l: R2 = -0.037 (std: 0.040)
  mlp-adaptive-xl: R2 = 0.051 (std: 0.022)
  mlp-l: R2 = 0.080 (std: 0.037)
  svr-rbf-xl: R2 = 0.131 (std: 0.037)
  svr-poly-l: R2 = 0.131 (std: 0.037)
  knn-tuned-sqrt: R2 = -0.014 (std: 0.050)
  knn-tuned-l: R2 = -0.014 (std: 0.050)
  ridge: R2 = 0.045 (std: 0.032)

Model-based training with 13 models
Best R2: 0.131, Mean R2: 0.056
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.076 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9871, entropy=0.2159, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6897
  Round 1/3: Mean predicted reward = 13.854
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.076 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9884, entropy=0.1982, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9941
  Round 2/3: Mean predicted reward = 14.123
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.076 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9905, entropy=0.2055, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2802
  Round 3/3: Mean predicted reward = 13.991

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 30 Results ---
  Mean Oracle Reward: 14.028
  Min Oracle Reward: 6.160
  Max Oracle Reward: 18.415
  Std Oracle Reward: 1.944
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.056, Max: 0.131, Count: 13
  Total Sequences Evaluated: 1970
    Oracle Count: 1920 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 31/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1970

--- Round 31 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGTAACATGCTCCGGGCCCG
  TGGACGCCGGGACTGCTCCA
  CGAGTCCACACTGCGGTGCG
  ATACGGGCGACGCCCGCTGT
  CGTGCATTGACGCGACGGCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.277
  Max reward: 19.035
  With intrinsic bonuses: 14.280

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9899, entropy=0.1912, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.3271

=== Surrogate Model Training ===
Total samples: 2034

Training on 1953 samples (removed 81 outliers)
Reward range: [8.87, 18.74], mean: 13.93
  Created 13 candidate models for data size 1953
Current R2 threshold: -0.09
  rf-tuned-l: R2 = 0.089 (std: 0.035)
  rf-tuned-xl: R2 = 0.078 (std: 0.053)
  gb-tuned-l: R2 = 0.112 (std: 0.052)
  gb-tuned-xl: R2 = 0.112 (std: 0.052)
  xgb-xl: R2 = -0.031 (std: 0.037)
  xgb-l: R2 = -0.031 (std: 0.037)
  mlp-adaptive-xl: R2 = 0.074 (std: 0.034)
  mlp-l: R2 = 0.086 (std: 0.035)
  svr-rbf-xl: R2 = 0.134 (std: 0.053)
  svr-poly-l: R2 = 0.134 (std: 0.053)
  knn-tuned-sqrt: R2 = -0.004 (std: 0.026)
  knn-tuned-l: R2 = -0.004 (std: 0.026)
  ridge: R2 = 0.048 (std: 0.046)

Model-based training with 13 models
Best R2: 0.134, Mean R2: 0.061
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9879, entropy=0.1742, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9119
  Round 1/3: Mean predicted reward = 13.747
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9911, entropy=0.1723, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0820
  Round 2/3: Mean predicted reward = 14.380
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9883, entropy=0.1811, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2412
  Round 3/3: Mean predicted reward = 14.256

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 31 Results ---
  Mean Oracle Reward: 14.247
  Min Oracle Reward: 7.849
  Max Oracle Reward: 18.942
  Std Oracle Reward: 2.079
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.061, Max: 0.134, Count: 13
  Total Sequences Evaluated: 2034
    Oracle Count: 1984 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 32/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2034

--- Round 32 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.984) ---
  GCGGGGAAATCCCCCCGTGT
  GCACGGACCTTAGGCTCGCG
  GTGGAGGGCCCTCCTAGCAC
  GTTCAGACTCCCCGGCGGAG
  GGCTGCGGCCCACGTTAGAC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.184
  Max reward: 19.042
  With intrinsic bonuses: 14.188

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9894, entropy=0.1682, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8410

=== Surrogate Model Training ===
Total samples: 2098

Training on 2014 samples (removed 84 outliers)
Reward range: [8.87, 18.89], mean: 13.95
  Created 13 candidate models for data size 2014
Current R2 threshold: -0.07999999999999999
  rf-tuned-l: R2 = 0.089 (std: 0.039)
  rf-tuned-xl: R2 = 0.079 (std: 0.044)
  gb-tuned-l: R2 = 0.116 (std: 0.050)
  gb-tuned-xl: R2 = 0.116 (std: 0.050)
  xgb-xl: R2 = -0.061 (std: 0.070)
  xgb-l: R2 = -0.061 (std: 0.070)
  mlp-adaptive-xl: R2 = 0.070 (std: 0.022)
  mlp-l: R2 = 0.081 (std: 0.032)
  svr-rbf-xl: R2 = 0.139 (std: 0.048)
  svr-poly-l: R2 = 0.139 (std: 0.048)
  knn-tuned-sqrt: R2 = -0.024 (std: 0.037)
  knn-tuned-l: R2 = -0.024 (std: 0.037)
  ridge: R2 = 0.048 (std: 0.045)

Model-based training with 13 models
Best R2: 0.139, Mean R2: 0.055
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9869, entropy=0.1592, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3886
  Round 1/3: Mean predicted reward = 13.497
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.1543, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6977
  Round 2/3: Mean predicted reward = 14.240
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.068 xgb-l:0.068 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.1547, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8778
  Round 3/3: Mean predicted reward = 14.225

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 32 Results ---
  Mean Oracle Reward: 14.200
  Min Oracle Reward: 6.245
  Max Oracle Reward: 19.181
  Std Oracle Reward: 2.394
  Sequence Diversity: 0.984
  Models Used: 13
  Model R2 - Mean: 0.055, Max: 0.139, Count: 13
  Total Sequences Evaluated: 2098
    Oracle Count: 2048 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 33/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2098

--- Round 33 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.984) ---
  CGCGCAGCCGTTGGGCAACT
  ACATCCCTCGGAGCCGGGTG
  TCGGATCGCGCCGATGGCCA
  ATGTGCCGCGCCGTAGCCGA
  CGGATGGTCTCACACGGGCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.354
  Max reward: 18.954
  With intrinsic bonuses: 14.306

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9892, entropy=0.1472, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2695

=== Surrogate Model Training ===
Total samples: 2162

Training on 2077 samples (removed 85 outliers)
Reward range: [8.87, 18.89], mean: 13.96
  Created 13 candidate models for data size 2077
Current R2 threshold: -0.06999999999999998
  rf-tuned-l: R2 = 0.064 (std: 0.056)
  rf-tuned-xl: R2 = 0.066 (std: 0.062)
  gb-tuned-l: R2 = 0.115 (std: 0.056)
  gb-tuned-xl: R2 = 0.115 (std: 0.056)
  xgb-xl: R2 = -0.037 (std: 0.062)
  xgb-l: R2 = -0.037 (std: 0.062)
  mlp-adaptive-xl: R2 = 0.077 (std: 0.043)
  mlp-l: R2 = 0.073 (std: 0.037)
  svr-rbf-xl: R2 = 0.144 (std: 0.045)
  svr-poly-l: R2 = 0.144 (std: 0.045)
  knn-tuned-sqrt: R2 = -0.024 (std: 0.036)
  knn-tuned-l: R2 = -0.024 (std: 0.036)
  ridge: R2 = 0.048 (std: 0.053)

Model-based training with 13 models
Best R2: 0.144, Mean R2: 0.056
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9867, entropy=0.1511, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1135
  Round 1/3: Mean predicted reward = 12.931
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9890, entropy=0.1468, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2166
  Round 2/3: Mean predicted reward = 14.059
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9908, entropy=0.1520, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3426
  Round 3/3: Mean predicted reward = 14.326

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 33 Results ---
  Mean Oracle Reward: 14.368
  Min Oracle Reward: 9.766
  Max Oracle Reward: 19.001
  Std Oracle Reward: 2.078
  Sequence Diversity: 0.984
  Models Used: 13
  Model R2 - Mean: 0.056, Max: 0.144, Count: 13
  Total Sequences Evaluated: 2162
    Oracle Count: 2112 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 34/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2162
  Performance plateaued, reducing LR to 0.000019

--- Round 34 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  CCCGTATGGACTCAGGCGCG
  GCGTCACGCTTGCACACGGG
  GGGAGCGCTGTCCACATGCC
  TCCCACGAGTCCGGGAGCGT
  TCGGACGCTGAGAGCGCTCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.526
  Max reward: 19.401
  With intrinsic bonuses: 14.509

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9912, entropy=0.1445, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0709

=== Surrogate Model Training ===
Total samples: 2226

Training on 2135 samples (removed 91 outliers)
Reward range: [8.92, 18.89], mean: 13.98
  Created 13 candidate models for data size 2135
Current R2 threshold: -0.06
  rf-tuned-l: R2 = 0.080 (std: 0.061)
  rf-tuned-xl: R2 = 0.085 (std: 0.068)
  gb-tuned-l: R2 = 0.131 (std: 0.066)
  gb-tuned-xl: R2 = 0.131 (std: 0.066)
  xgb-xl: R2 = -0.037 (std: 0.081)
  xgb-l: R2 = -0.037 (std: 0.081)
  mlp-adaptive-xl: R2 = 0.087 (std: 0.059)
  mlp-l: R2 = 0.092 (std: 0.068)
  svr-rbf-xl: R2 = 0.151 (std: 0.081)
  svr-poly-l: R2 = 0.151 (std: 0.081)
  knn-tuned-sqrt: R2 = -0.033 (std: 0.060)
  knn-tuned-l: R2 = -0.033 (std: 0.060)
  ridge: R2 = 0.051 (std: 0.062)

Model-based training with 13 models
Best R2: 0.151, Mean R2: 0.063
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.069 xgb-l:0.069 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.070 knn-tuned-l:0.070 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9858, entropy=0.1436, kl_div=0.0000
    Epoch 1: policy_loss=0.0152, value_loss=0.9858, entropy=0.1434, kl_div=0.0368
  Round 1/3: Mean predicted reward = 13.171
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.069 xgb-l:0.069 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.070 knn-tuned-l:0.070 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9885, entropy=0.1441, kl_div=0.0000
    Epoch 1: policy_loss=-0.0110, value_loss=0.9885, entropy=0.1439, kl_div=0.0221
  Round 2/3: Mean predicted reward = 14.320
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.082 gb-tuned-xl:0.082 xgb-xl:0.069 xgb-l:0.069 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.070 knn-tuned-l:0.070 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9879, entropy=0.1485, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0653
  Round 3/3: Mean predicted reward = 14.424

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 34 Results ---
  Mean Oracle Reward: 14.493
  Min Oracle Reward: 8.732
  Max Oracle Reward: 19.231
  Std Oracle Reward: 2.238
  Sequence Diversity: 0.969
  Models Used: 13
  Model R2 - Mean: 0.063, Max: 0.151, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2226
    Oracle Count: 2176 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 35/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2226
  Consistent improvement, increasing LR to 0.000360

--- Round 35 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  AGTACCGTCCGCGCGCAGTG
  TCCCTCAAGGGCGCGGCATG
  AGTCCGGAGGCCCTAGCGCT
  CTACAGGGGCGGCCTCTAGC
  CGTCCGCACGCACGGGTTAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.635
  Max reward: 20.126
  With intrinsic bonuses: 14.603

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9909, entropy=0.1529, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7144

=== Surrogate Model Training ===
Total samples: 2290

Training on 2196 samples (removed 94 outliers)
Reward range: [8.92, 18.89], mean: 13.99
  Created 13 candidate models for data size 2196
Current R2 threshold: -0.04999999999999999
  rf-tuned-l: R2 = 0.088 (std: 0.063)
  rf-tuned-xl: R2 = 0.093 (std: 0.058)
  gb-tuned-l: R2 = 0.127 (std: 0.065)
  gb-tuned-xl: R2 = 0.127 (std: 0.065)
  xgb-xl: R2 = -0.016 (std: 0.065)
  xgb-l: R2 = -0.016 (std: 0.065)
  mlp-adaptive-xl: R2 = 0.107 (std: 0.062)
  mlp-l: R2 = 0.091 (std: 0.060)
  svr-rbf-xl: R2 = 0.154 (std: 0.076)
  svr-poly-l: R2 = 0.154 (std: 0.076)
  knn-tuned-sqrt: R2 = -0.035 (std: 0.072)
  knn-tuned-l: R2 = -0.035 (std: 0.072)
  ridge: R2 = 0.053 (std: 0.059)

Model-based training with 13 models
Best R2: 0.154, Mean R2: 0.069
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9861, entropy=0.1311, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2716
  Round 1/3: Mean predicted reward = 12.868
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.1404, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4907
  Round 2/3: Mean predicted reward = 14.216
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.079 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.084 svr-poly-l:0.084 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.076 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9898, entropy=0.1350, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9268
  Round 3/3: Mean predicted reward = 14.317

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 35 Results ---
  Mean Oracle Reward: 14.578
  Min Oracle Reward: 8.743
  Max Oracle Reward: 19.706
  Std Oracle Reward: 2.230
  Sequence Diversity: 0.938
  Models Used: 13
  Model R2 - Mean: 0.069, Max: 0.154, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2290
    Oracle Count: 2240 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 36/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2290
  Consistent improvement, increasing LR to 0.000327

--- Round 36 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  CCTGACGCGACATGTGGCCG
  GCGCGGCGCATGGATCTCAC
  GTCGCTGGTGGCCCAGACCA
  CCGTCGACGGTTCCAGGGAC
  GTCAGTAGCATGCACCGCGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.639
  Max reward: 18.975
  With intrinsic bonuses: 14.662

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9898, entropy=0.1193, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1866

=== Surrogate Model Training ===
Total samples: 2354

Training on 2260 samples (removed 94 outliers)
Reward range: [8.87, 18.90], mean: 14.01
  Created 13 candidate models for data size 2260
Current R2 threshold: -0.03999999999999998
  rf-tuned-l: R2 = 0.107 (std: 0.050)
  rf-tuned-xl: R2 = 0.107 (std: 0.050)
  gb-tuned-l: R2 = 0.140 (std: 0.058)
  gb-tuned-xl: R2 = 0.140 (std: 0.058)
  xgb-xl: R2 = 0.017 (std: 0.069)
  xgb-l: R2 = 0.017 (std: 0.069)
  mlp-adaptive-xl: R2 = 0.105 (std: 0.053)
  mlp-l: R2 = 0.100 (std: 0.059)
  svr-rbf-xl: R2 = 0.163 (std: 0.070)
  svr-poly-l: R2 = 0.163 (std: 0.070)
  knn-tuned-sqrt: R2 = 0.009 (std: 0.062)
  knn-tuned-l: R2 = 0.009 (std: 0.062)
  ridge: R2 = 0.064 (std: 0.048)

Model-based training with 13 models
Best R2: 0.163, Mean R2: 0.088
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=36.3080, entropy=0.1084, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6806
  Round 1/3: Mean predicted reward = 12.223
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9898, entropy=0.1108, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9103
  Round 2/3: Mean predicted reward = 14.479
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.078 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9894, entropy=0.1020, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1870
  Round 3/3: Mean predicted reward = 14.468

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 36 Results ---
  Mean Oracle Reward: 14.655
  Min Oracle Reward: 6.994
  Max Oracle Reward: 19.014
  Std Oracle Reward: 2.208
  Sequence Diversity: 0.938
  Models Used: 13
  Model R2 - Mean: 0.088, Max: 0.163, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2354
    Oracle Count: 2304 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 37/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2354
  Performance plateaued, reducing LR to 0.000100

--- Round 37 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  GCGCCCGTAGCCAGTCTAGG
  CGCGCACTGCACGGCGATTG
  CAGAGCCCGCGGCTCGTGAT
  CCGCCGTACTGGGAGTCCAG
  GTTAGTAGCCGCCCGACGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.936
  Max reward: 19.294
  With intrinsic bonuses: 14.947

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9904, entropy=0.0850, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2461

=== Surrogate Model Training ===
Total samples: 2418

Training on 2322 samples (removed 96 outliers)
Reward range: [8.92, 18.99], mean: 14.04
  Created 13 candidate models for data size 2322
Current R2 threshold: -0.02999999999999997
  rf-tuned-l: R2 = 0.097 (std: 0.072)
  rf-tuned-xl: R2 = 0.092 (std: 0.077)
  gb-tuned-l: R2 = 0.137 (std: 0.064)
  gb-tuned-xl: R2 = 0.137 (std: 0.064)
  xgb-xl: R2 = 0.005 (std: 0.099)
  xgb-l: R2 = 0.005 (std: 0.099)
  mlp-adaptive-xl: R2 = 0.122 (std: 0.077)
  mlp-l: R2 = 0.099 (std: 0.079)
  svr-rbf-xl: R2 = 0.164 (std: 0.084)
  svr-poly-l: R2 = 0.164 (std: 0.084)
  knn-tuned-sqrt: R2 = -0.002 (std: 0.067)
  knn-tuned-l: R2 = -0.002 (std: 0.067)
  ridge: R2 = 0.060 (std: 0.055)

Model-based training with 13 models
Best R2: 0.164, Mean R2: 0.083
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.077 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=42.4139, entropy=0.0976, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0785
  Round 1/3: Mean predicted reward = 11.643
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.077 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9915, entropy=0.1011, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1752
  Round 2/3: Mean predicted reward = 14.419
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.078 rf-tuned-xl:0.077 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.080 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.075 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.0848, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3242
  Round 3/3: Mean predicted reward = 14.510

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 37 Results ---
  Mean Oracle Reward: 14.944
  Min Oracle Reward: 8.778
  Max Oracle Reward: 19.176
  Std Oracle Reward: 2.006
  Sequence Diversity: 0.875
  Models Used: 13
  Model R2 - Mean: 0.083, Max: 0.164, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2418
    Oracle Count: 2368 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 38/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2418
  Consistent improvement, increasing LR to 0.000132

--- Round 38 Configuration ---
Learning rate: 0.000132
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  GCCATGCCGGCAGGCTTAGC
  GACCACCCTGGGTCATGGGC
  AGGGGCCAACTTCTCCGGGC
  CTCGTGAGCAGCGTCACGCG
  CCCTAGTGTCGGGCCCAGAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.515
  Max reward: 19.131
  With intrinsic bonuses: 14.546

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9910, entropy=0.0926, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5125

=== Surrogate Model Training ===
Total samples: 2482

Training on 2387 samples (removed 95 outliers)
Reward range: [8.92, 19.05], mean: 14.06
  Created 13 candidate models for data size 2387
Current R2 threshold: -0.019999999999999962
  rf-tuned-l: R2 = 0.131 (std: 0.085)
  rf-tuned-xl: R2 = 0.123 (std: 0.087)
  gb-tuned-l: R2 = 0.146 (std: 0.070)
  gb-tuned-xl: R2 = 0.146 (std: 0.070)
  xgb-xl: R2 = 0.032 (std: 0.108)
  xgb-l: R2 = 0.032 (std: 0.108)
  mlp-adaptive-xl: R2 = 0.136 (std: 0.085)
  mlp-l: R2 = 0.122 (std: 0.083)
  svr-rbf-xl: R2 = 0.176 (std: 0.092)
  svr-poly-l: R2 = 0.176 (std: 0.092)
  knn-tuned-sqrt: R2 = 0.038 (std: 0.088)
  knn-tuned-l: R2 = 0.038 (std: 0.088)
  ridge: R2 = 0.065 (std: 0.063)

Model-based training with 13 models
Best R2: 0.176, Mean R2: 0.105
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.074 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=51.0079, entropy=0.0985, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2918
  Round 1/3: Mean predicted reward = 10.834
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.074 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9902, entropy=0.0911, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4177
  Round 2/3: Mean predicted reward = 14.404
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.083 svr-poly-l:0.083 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.074 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9893, entropy=0.0961, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4356
  Round 3/3: Mean predicted reward = 14.469

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 38 Results ---
  Mean Oracle Reward: 14.513
  Min Oracle Reward: 9.624
  Max Oracle Reward: 19.210
  Std Oracle Reward: 2.242
  Sequence Diversity: 0.906
  Models Used: 13
  Model R2 - Mean: 0.105, Max: 0.176, Count: 13
  Total Sequences Evaluated: 2482
    Oracle Count: 2432 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 39/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2482

--- Round 39 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.859) ---
  ATCCGCCCGGCGTACGTGGA
  AAGGGGCTTCACCCCGGGTC
  AGAGACCGGTCGTCCTCGGC
  GGTACCGGCGTGCGAACCCT
  TGTAACGGGCCGTGCGCCCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.731
  Max reward: 19.198
  With intrinsic bonuses: 14.719

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9895, entropy=0.0899, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1738

=== Surrogate Model Training ===
Total samples: 2546

Training on 2451 samples (removed 95 outliers)
Reward range: [8.92, 19.08], mean: 14.07
  Created 13 candidate models for data size 2451
Current R2 threshold: -0.010000000000000009
  rf-tuned-l: R2 = 0.155 (std: 0.097)
  rf-tuned-xl: R2 = 0.157 (std: 0.097)
  gb-tuned-l: R2 = 0.152 (std: 0.080)
  gb-tuned-xl: R2 = 0.152 (std: 0.080)
  xgb-xl: R2 = 0.042 (std: 0.124)
  xgb-l: R2 = 0.042 (std: 0.124)
  mlp-adaptive-xl: R2 = 0.148 (std: 0.099)
  mlp-l: R2 = 0.132 (std: 0.093)
  svr-rbf-xl: R2 = 0.184 (std: 0.106)
  svr-poly-l: R2 = 0.184 (std: 0.106)
  knn-tuned-sqrt: R2 = 0.050 (std: 0.088)
  knn-tuned-l: R2 = 0.050 (std: 0.088)
  ridge: R2 = 0.066 (std: 0.070)

Model-based training with 13 models
Best R2: 0.184, Mean R2: 0.117
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.073 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=66.9096, entropy=0.0886, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0939
  Round 1/3: Mean predicted reward = 9.826
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9874, entropy=0.0818, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1633
  Round 2/3: Mean predicted reward = 14.474
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9906, entropy=0.0801, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1883
  Round 3/3: Mean predicted reward = 14.556

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 39 Results ---
  Mean Oracle Reward: 14.716
  Min Oracle Reward: 9.144
  Max Oracle Reward: 19.421
  Std Oracle Reward: 2.255
  Sequence Diversity: 0.859
  Models Used: 13
  Model R2 - Mean: 0.117, Max: 0.184, Count: 13
  Total Sequences Evaluated: 2546
    Oracle Count: 2496 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 40/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2546

--- Round 40 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.891) ---
  GCCCGACAGGCGGTTACCTG
  GCACCGCGCATTCACGGGGT
  ACCGGACGGGGGCCTCTACT
  GACTGGAGGACGTCCTCCGC
  CCTAGGTGCCTCCGCGAGAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.565
  Max reward: 19.150
  With intrinsic bonuses: 14.594

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9908, entropy=0.0795, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2888

=== Surrogate Model Training ===
Total samples: 2610

Training on 2513 samples (removed 97 outliers)
Reward range: [8.93, 19.09], mean: 14.09
  Created 13 candidate models for data size 2513
Current R2 threshold: 0.0
  rf-tuned-l: R2 = 0.171 (std: 0.097)
  rf-tuned-xl: R2 = 0.174 (std: 0.100)
  gb-tuned-l: R2 = 0.166 (std: 0.087)
  gb-tuned-xl: R2 = 0.166 (std: 0.087)
  xgb-xl: R2 = 0.071 (std: 0.119)
  xgb-l: R2 = 0.071 (std: 0.119)
  mlp-adaptive-xl: R2 = 0.150 (std: 0.097)
  mlp-l: R2 = 0.141 (std: 0.111)
  svr-rbf-xl: R2 = 0.193 (std: 0.110)
  svr-poly-l: R2 = 0.193 (std: 0.110)
  knn-tuned-sqrt: R2 = 0.090 (std: 0.118)
  knn-tuned-l: R2 = 0.090 (std: 0.118)
  ridge: R2 = 0.073 (std: 0.073)

Model-based training with 13 models
Best R2: 0.193, Mean R2: 0.135
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=91.1056, entropy=0.0841, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8942
  Round 1/3: Mean predicted reward = 9.932
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9895, entropy=0.0817, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1457
  Round 2/3: Mean predicted reward = 14.449
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9894, entropy=0.0890, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2758
  Round 3/3: Mean predicted reward = 14.464

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 40 Results ---
  Mean Oracle Reward: 14.564
  Min Oracle Reward: 10.378
  Max Oracle Reward: 18.960
  Std Oracle Reward: 1.943
  Sequence Diversity: 0.891
  Models Used: 13
  Model R2 - Mean: 0.135, Max: 0.193, Count: 13
  Total Sequences Evaluated: 2610
    Oracle Count: 2560 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 41/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2610
  Performance plateaued, reducing LR to 0.000136

--- Round 41 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  CGGAGAGGCTTTACCGCCGC
  TATCCCGGGCGCCTGAAGGC
  ATCTGGCGTCGGACCAGCCG
  CGGCGCAGGTGCGACATTCC
  AAGGTCGGGCTCCCACGTCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.117
  Max reward: 18.749
  With intrinsic bonuses: 14.137

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9880, entropy=0.0967, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3278

=== Surrogate Model Training ===
Total samples: 2674

Training on 2574 samples (removed 100 outliers)
Reward range: [9.00, 19.06], mean: 14.09
  Created 13 candidate models for data size 2574
Current R2 threshold: 0.010000000000000009
  rf-tuned-l: R2 = 0.170 (std: 0.100)
  rf-tuned-xl: R2 = 0.174 (std: 0.108)
  gb-tuned-l: R2 = 0.175 (std: 0.094)
  gb-tuned-xl: R2 = 0.175 (std: 0.094)
  xgb-xl: R2 = 0.076 (std: 0.115)
  xgb-l: R2 = 0.076 (std: 0.115)
  mlp-adaptive-xl: R2 = 0.150 (std: 0.111)
  mlp-l: R2 = 0.165 (std: 0.102)
  svr-rbf-xl: R2 = 0.196 (std: 0.105)
  svr-poly-l: R2 = 0.196 (std: 0.105)
  knn-tuned-sqrt: R2 = 0.082 (std: 0.123)
  knn-tuned-l: R2 = 0.082 (std: 0.123)
  ridge: R2 = 0.079 (std: 0.076)

Model-based training with 13 models
Best R2: 0.196, Mean R2: 0.138
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=43.4907, entropy=0.0980, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1352
  Round 1/3: Mean predicted reward = 11.050
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9879, entropy=0.0977, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2306
  Round 2/3: Mean predicted reward = 14.448
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9865, entropy=0.0926, kl_div=0.0000
    Epoch 1: policy_loss=-0.0335, value_loss=0.9865, entropy=0.0901, kl_div=0.0334
  Round 3/3: Mean predicted reward = 14.319

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 41 Results ---
  Mean Oracle Reward: 14.138
  Min Oracle Reward: 9.205
  Max Oracle Reward: 18.823
  Std Oracle Reward: 1.645
  Sequence Diversity: 0.938
  Models Used: 13
  Model R2 - Mean: 0.138, Max: 0.196, Count: 13
  Total Sequences Evaluated: 2674
    Oracle Count: 2624 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 42/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2674

--- Round 42 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.953) ---
  TCGAAGCCCACGGGCGTTCG
  GCGAAGCTTCGGCTACCGCG
  TGTCCCCAGGATCCGAGGGC
  CCGCTACGGTCAGCCTAGGG
  GCCATCGTGGGGCAATCCCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.135
  Max reward: 17.198
  With intrinsic bonuses: 14.130

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.0854, kl_div=0.0000
    Epoch 1: policy_loss=-0.0496, value_loss=0.9877, entropy=0.0776, kl_div=-0.1970

=== Surrogate Model Training ===
Total samples: 2738

Training on 2637 samples (removed 101 outliers)
Reward range: [9.00, 19.06], mean: 14.10
  Created 13 candidate models for data size 2637
Current R2 threshold: 0.020000000000000018
  rf-tuned-l: R2 = 0.173 (std: 0.100)
  rf-tuned-xl: R2 = 0.169 (std: 0.100)
  gb-tuned-l: R2 = 0.182 (std: 0.101)
  gb-tuned-xl: R2 = 0.182 (std: 0.101)
  xgb-xl: R2 = 0.068 (std: 0.112)
  xgb-l: R2 = 0.068 (std: 0.112)
  mlp-adaptive-xl: R2 = 0.165 (std: 0.116)
  mlp-l: R2 = 0.170 (std: 0.104)
  svr-rbf-xl: R2 = 0.199 (std: 0.110)
  svr-poly-l: R2 = 0.199 (std: 0.110)
  knn-tuned-sqrt: R2 = 0.103 (std: 0.112)
  knn-tuned-l: R2 = 0.103 (std: 0.112)
  ridge: R2 = 0.084 (std: 0.085)

Model-based training with 13 models
Best R2: 0.199, Mean R2: 0.144
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=89.8970, entropy=0.0729, kl_div=0.0000
    Epoch 1: policy_loss=0.0455, value_loss=89.8975, entropy=0.0695, kl_div=-0.2194
  Round 1/3: Mean predicted reward = 9.552
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9875, entropy=0.0585, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1796
  Round 2/3: Mean predicted reward = 14.626
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.071 xgb-l:0.071 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.0590, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2935
  Round 3/3: Mean predicted reward = 14.480

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 42 Results ---
  Mean Oracle Reward: 14.069
  Min Oracle Reward: 3.629
  Max Oracle Reward: 17.323
  Std Oracle Reward: 2.022
  Sequence Diversity: 0.953
  Models Used: 13
  Model R2 - Mean: 0.144, Max: 0.199, Count: 13
  Total Sequences Evaluated: 2738
    Oracle Count: 2688 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 43/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2738

--- Round 43 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.859) ---
  GACCGGCTATTCCCGGGACG
  GTAGCGGGTACCACCCTGGC
  CATCCGCGTGTGGGCGAACC
  GGCCGGCAGCGTCCGTTAAC
  ACCCGCGCGTCTGGAAGGTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.697
  Max reward: 19.108
  With intrinsic bonuses: 14.666

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.0639, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0523

=== Surrogate Model Training ===
Total samples: 2802

Training on 2699 samples (removed 103 outliers)
Reward range: [9.05, 19.05], mean: 14.11
  Created 13 candidate models for data size 2699
Current R2 threshold: 0.030000000000000027
  rf-tuned-l: R2 = 0.190 (std: 0.101)
  rf-tuned-xl: R2 = 0.192 (std: 0.099)
  gb-tuned-l: R2 = 0.188 (std: 0.100)
  gb-tuned-xl: R2 = 0.188 (std: 0.100)
  xgb-xl: R2 = 0.104 (std: 0.112)
  xgb-l: R2 = 0.104 (std: 0.112)
  mlp-adaptive-xl: R2 = 0.184 (std: 0.117)
  mlp-l: R2 = 0.166 (std: 0.114)
  svr-rbf-xl: R2 = 0.215 (std: 0.108)
  svr-poly-l: R2 = 0.215 (std: 0.108)
  knn-tuned-sqrt: R2 = 0.119 (std: 0.106)
  knn-tuned-l: R2 = 0.119 (std: 0.106)
  ridge: R2 = 0.089 (std: 0.084)

Model-based training with 13 models
Best R2: 0.215, Mean R2: 0.159
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=126.9368, entropy=0.0616, kl_div=0.0000
    Epoch 1: policy_loss=-0.0198, value_loss=126.9371, entropy=0.0607, kl_div=-0.0699
  Round 1/3: Mean predicted reward = 8.655
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9890, entropy=0.0604, kl_div=0.0000
    Epoch 1: policy_loss=0.0057, value_loss=0.9890, entropy=0.0601, kl_div=-0.0368
  Round 2/3: Mean predicted reward = 14.550
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.077 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9890, entropy=0.0565, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1737
  Round 3/3: Mean predicted reward = 14.645

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 43 Results ---
  Mean Oracle Reward: 14.648
  Min Oracle Reward: 10.970
  Max Oracle Reward: 19.110
  Std Oracle Reward: 1.827
  Sequence Diversity: 0.859
  Models Used: 13
  Model R2 - Mean: 0.159, Max: 0.215, Count: 13
  Total Sequences Evaluated: 2802
    Oracle Count: 2752 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 44/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2802
  Consistent improvement, increasing LR to 0.000045

--- Round 44 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  TGGCGACAGCGTGCCTCGCA
  TGGCAGGCGAGCGTCCTACC
  ACGCCGGCGCGTAGGACTTC
  GGCCCCCGACCTAGTTGGGA
  TCGCCAGTCGTCACGACGGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.364
  Max reward: 17.320
  With intrinsic bonuses: 14.377

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.0665, kl_div=0.0000
    Epoch 1: policy_loss=-0.0054, value_loss=0.9880, entropy=0.0662, kl_div=0.0216

=== Surrogate Model Training ===
Total samples: 2866

Training on 2769 samples (removed 97 outliers)
Reward range: [9.00, 19.09], mean: 14.13
  Created 13 candidate models for data size 2769
Current R2 threshold: 0.040000000000000036
  rf-tuned-l: R2 = 0.196 (std: 0.104)
  rf-tuned-xl: R2 = 0.197 (std: 0.099)
  gb-tuned-l: R2 = 0.189 (std: 0.110)
  gb-tuned-xl: R2 = 0.189 (std: 0.110)
  xgb-xl: R2 = 0.117 (std: 0.106)
  xgb-l: R2 = 0.117 (std: 0.106)
  mlp-adaptive-xl: R2 = 0.179 (std: 0.109)
  mlp-l: R2 = 0.189 (std: 0.116)
  svr-rbf-xl: R2 = 0.215 (std: 0.108)
  svr-poly-l: R2 = 0.215 (std: 0.108)
  knn-tuned-sqrt: R2 = 0.084 (std: 0.064)
  knn-tuned-l: R2 = 0.084 (std: 0.064)
  ridge: R2 = 0.091 (std: 0.087)

Model-based training with 13 models
Best R2: 0.215, Mean R2: 0.159
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=118.8496, entropy=0.0548, kl_div=0.0000
    Epoch 1: policy_loss=-0.0189, value_loss=118.8497, entropy=0.0544, kl_div=-0.0596
  Round 1/3: Mean predicted reward = 6.369
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9871, entropy=0.0536, kl_div=0.0000
    Epoch 1: policy_loss=0.0155, value_loss=0.9871, entropy=0.0534, kl_div=-0.0385
  Round 2/3: Mean predicted reward = 14.610
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9891, entropy=0.0469, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0631
  Round 3/3: Mean predicted reward = 14.679

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 44 Results ---
  Mean Oracle Reward: 14.358
  Min Oracle Reward: 9.117
  Max Oracle Reward: 17.117
  Std Oracle Reward: 1.785
  Sequence Diversity: 0.766
  Models Used: 13
  Model R2 - Mean: 0.159, Max: 0.215, Count: 13
  Total Sequences Evaluated: 2866
    Oracle Count: 2816 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 45/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2866

--- Round 45 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.828) ---
  TTACGGGGCGCTGACCACCG
  GAAGTTGGCTCGGCCAGCCC
  CCGTACGAGTGGCCGACCGT
  GGGTCGCCAACGTGCGCCAT
  CACGTCTGAGCGCGGATCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.645
  Max reward: 18.971
  With intrinsic bonuses: 14.666

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9893, entropy=0.0545, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0797

=== Surrogate Model Training ===
Total samples: 2930

Training on 2832 samples (removed 98 outliers)
Reward range: [9.05, 19.09], mean: 14.14
  Created 13 candidate models for data size 2832
Current R2 threshold: 0.050000000000000044
  rf-tuned-l: R2 = 0.209 (std: 0.126)
  rf-tuned-xl: R2 = 0.207 (std: 0.123)
  gb-tuned-l: R2 = 0.196 (std: 0.123)
  gb-tuned-xl: R2 = 0.196 (std: 0.123)
  xgb-xl: R2 = 0.127 (std: 0.116)
  xgb-l: R2 = 0.127 (std: 0.116)
  mlp-adaptive-xl: R2 = 0.193 (std: 0.141)
  mlp-l: R2 = 0.194 (std: 0.139)
  svr-rbf-xl: R2 = 0.230 (std: 0.125)
  svr-poly-l: R2 = 0.230 (std: 0.125)
  knn-tuned-sqrt: R2 = 0.097 (std: 0.069)
  knn-tuned-l: R2 = 0.097 (std: 0.069)
  ridge: R2 = 0.092 (std: 0.096)

Model-based training with 13 models
Best R2: 0.230, Mean R2: 0.169
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=183.0161, entropy=0.0544, kl_div=0.0000
    Epoch 1: policy_loss=0.0058, value_loss=183.0172, entropy=0.0535, kl_div=-0.2529
  Round 1/3: Mean predicted reward = 5.933
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9891, entropy=0.0489, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3418
  Round 2/3: Mean predicted reward = 14.621
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9884, entropy=0.0496, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6374
  Round 3/3: Mean predicted reward = 14.734

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 45 Results ---
  Mean Oracle Reward: 14.695
  Min Oracle Reward: 10.629
  Max Oracle Reward: 19.162
  Std Oracle Reward: 1.876
  Sequence Diversity: 0.828
  Models Used: 13
  Model R2 - Mean: 0.169, Max: 0.230, Count: 13
  Total Sequences Evaluated: 2930
    Oracle Count: 2880 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 46/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2930

--- Round 46 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  ATTCCGGGGACGAGTGCCCC
  AGGTATCGGGCCTCCAGGCC
  AGTCCTGGCCGACAGCTGGC
  GCGGATCGTCGCGCCTCAGA
  CACGTTCGCGGCGCTGCGAA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.554
  Max reward: 18.489
  With intrinsic bonuses: 14.557

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9890, entropy=0.0497, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1961

=== Surrogate Model Training ===
Total samples: 2994

Training on 2896 samples (removed 98 outliers)
Reward range: [9.05, 19.10], mean: 14.15
  Created 13 candidate models for data size 2896
Current R2 threshold: 0.06
  rf-tuned-l: R2 = 0.215 (std: 0.126)
  rf-tuned-xl: R2 = 0.215 (std: 0.129)
  gb-tuned-l: R2 = 0.212 (std: 0.138)
  gb-tuned-xl: R2 = 0.212 (std: 0.138)
  xgb-xl: R2 = 0.141 (std: 0.128)
  xgb-l: R2 = 0.141 (std: 0.128)
  mlp-adaptive-xl: R2 = 0.209 (std: 0.142)
  mlp-l: R2 = 0.214 (std: 0.136)
  svr-rbf-xl: R2 = 0.241 (std: 0.126)
  svr-poly-l: R2 = 0.241 (std: 0.126)
  knn-tuned-sqrt: R2 = 0.102 (std: 0.075)
  knn-tuned-l: R2 = 0.102 (std: 0.075)
  ridge: R2 = 0.099 (std: 0.110)

Model-based training with 13 models
Best R2: 0.241, Mean R2: 0.180
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=201.2126, entropy=0.0532, kl_div=0.0000
    Epoch 1: policy_loss=-0.0240, value_loss=201.2137, entropy=0.0529, kl_div=-0.0781
  Round 1/3: Mean predicted reward = 4.937
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9884, entropy=0.0519, kl_div=0.0000
    Epoch 1: policy_loss=-0.0092, value_loss=0.9884, entropy=0.0523, kl_div=-0.0111
  Round 2/3: Mean predicted reward = 14.675
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9880, entropy=0.0554, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3837
  Round 3/3: Mean predicted reward = 14.648

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 46 Results ---
  Mean Oracle Reward: 14.566
  Min Oracle Reward: 8.812
  Max Oracle Reward: 18.692
  Std Oracle Reward: 2.310
  Sequence Diversity: 0.812
  Models Used: 13
  Model R2 - Mean: 0.180, Max: 0.241, Count: 13
  Total Sequences Evaluated: 2994
    Oracle Count: 2944 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 47/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2994

--- Round 47 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.797) ---
  GCTTGACACCAGCGGTGGCC
  CGTAGCTCCCGGCTGACGGA
  CTCGCCAGGGATTGCGCCAG
  TCCCGTGCGGGACGCGACTA
  TTGCACCCGGTGAGCCCAGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.686
  Max reward: 17.490
  With intrinsic bonuses: 14.680

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9897, entropy=0.0539, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1841

=== Surrogate Model Training ===
Total samples: 3058

Training on 2959 samples (removed 99 outliers)
Reward range: [9.06, 19.10], mean: 14.17
  Created 13 candidate models for data size 2959
Current R2 threshold: 0.07
  rf-tuned-l: R2 = 0.222 (std: 0.113)
  rf-tuned-xl: R2 = 0.222 (std: 0.113)
  gb-tuned-l: R2 = 0.217 (std: 0.127)
  gb-tuned-xl: R2 = 0.217 (std: 0.127)
  xgb-xl: R2 = 0.139 (std: 0.127)
  xgb-l: R2 = 0.139 (std: 0.127)
  mlp-adaptive-xl: R2 = 0.216 (std: 0.127)
  mlp-l: R2 = 0.222 (std: 0.121)
  svr-rbf-xl: R2 = 0.240 (std: 0.114)
  svr-poly-l: R2 = 0.240 (std: 0.114)
  knn-tuned-sqrt: R2 = 0.103 (std: 0.057)
  knn-tuned-l: R2 = 0.103 (std: 0.057)
  ridge: R2 = 0.102 (std: 0.108)

Model-based training with 13 models
Best R2: 0.240, Mean R2: 0.183
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=213.5428, entropy=0.0600, kl_div=0.0000
    Epoch 1: policy_loss=-0.0182, value_loss=213.5436, entropy=0.0600, kl_div=-0.0399
  Round 1/3: Mean predicted reward = 4.986
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9896, entropy=0.0585, kl_div=0.0000
    Epoch 1: policy_loss=-0.0137, value_loss=0.9895, entropy=0.0592, kl_div=0.0083
  Round 2/3: Mean predicted reward = 14.482
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9874, entropy=0.0570, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3455
  Round 3/3: Mean predicted reward = 14.658

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 47 Results ---
  Mean Oracle Reward: 14.705
  Min Oracle Reward: 9.286
  Max Oracle Reward: 17.601
  Std Oracle Reward: 1.805
  Sequence Diversity: 0.797
  Models Used: 13
  Model R2 - Mean: 0.183, Max: 0.240, Count: 13
  Total Sequences Evaluated: 3058
    Oracle Count: 3008 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 48/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 3058
  Performance plateaued, reducing LR to 0.000055

--- Round 48 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GCGCTAGTCGCGAGTACCCG
  CGCGAGCACCTGCATCGTGG
  GAGTCCTACGCCGGAGCGCT
  TCCCCGGAGGTGCCCTGAAG
  ACCCGGCGTACGGGCGACTT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.331
  Max reward: 17.532
  With intrinsic bonuses: 14.326

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9886, entropy=0.0665, kl_div=0.0000
    Epoch 1: policy_loss=0.0169, value_loss=0.9886, entropy=0.0666, kl_div=0.0343

=== Surrogate Model Training ===
Total samples: 3122

Training on 3024 samples (removed 98 outliers)
Reward range: [9.06, 19.13], mean: 14.17
  Created 13 candidate models for data size 3024
Current R2 threshold: 0.08000000000000002
  rf-tuned-l: R2 = 0.207 (std: 0.102)
  rf-tuned-xl: R2 = 0.213 (std: 0.107)
  gb-tuned-l: R2 = 0.215 (std: 0.119)
  gb-tuned-xl: R2 = 0.215 (std: 0.119)
  xgb-xl: R2 = 0.149 (std: 0.111)
  xgb-l: R2 = 0.149 (std: 0.111)
  mlp-adaptive-xl: R2 = 0.201 (std: 0.101)
  mlp-l: R2 = 0.209 (std: 0.122)
  svr-rbf-xl: R2 = 0.234 (std: 0.111)
  svr-poly-l: R2 = 0.234 (std: 0.111)
  knn-tuned-sqrt: R2 = 0.105 (std: 0.082)
  knn-tuned-l: R2 = 0.105 (std: 0.082)
  ridge: R2 = 0.106 (std: 0.109)

Model-based training with 13 models
Best R2: 0.234, Mean R2: 0.180
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=352.5690, entropy=0.0580, kl_div=0.0000
    Epoch 1: policy_loss=-0.0124, value_loss=352.5692, entropy=0.0575, kl_div=-0.0760
  Round 1/3: Mean predicted reward = 5.144
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9884, entropy=0.0584, kl_div=0.0000
    Epoch 1: policy_loss=-0.0100, value_loss=0.9884, entropy=0.0587, kl_div=0.0449
  Round 2/3: Mean predicted reward = 14.571
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.079 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.074 xgb-l:0.074 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.071 knn-tuned-l:0.071 ridge:0.071 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9884, entropy=0.0628, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1357
  Round 3/3: Mean predicted reward = 14.659

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 48 Results ---
  Mean Oracle Reward: 14.332
  Min Oracle Reward: 6.881
  Max Oracle Reward: 17.474
  Std Oracle Reward: 1.899
  Sequence Diversity: 0.844
  Models Used: 13
  Model R2 - Mean: 0.180, Max: 0.234, Count: 13
  Total Sequences Evaluated: 3122
    Oracle Count: 3072 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 49/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 3122

--- Round 49 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.859) ---
  CTGCCGAAAGTGCGCCCGGT
  CGCCGCTGAGGCACTCAGGT
  CCCCGGGCGTAGTAACCTGG
  TCCGGCGGAACGTAGCCCGT
  TTCAGCCGTAGCGGCGCGCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.206
  Max reward: 17.384
  With intrinsic bonuses: 14.223

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9901, entropy=0.0658, kl_div=0.0000
    Epoch 1: policy_loss=0.0164, value_loss=0.9901, entropy=0.0660, kl_div=0.0298

=== Surrogate Model Training ===
Total samples: 3186

Training on 3088 samples (removed 98 outliers)
Reward range: [9.06, 19.13], mean: 14.18
  Created 13 candidate models for data size 3088
Current R2 threshold: 0.09000000000000002
  rf-tuned-l: R2 = 0.208 (std: 0.096)
  rf-tuned-xl: R2 = 0.208 (std: 0.098)
  gb-tuned-l: R2 = 0.213 (std: 0.107)
  gb-tuned-xl: R2 = 0.213 (std: 0.107)
  xgb-xl: R2 = 0.111 (std: 0.108)
  xgb-l: R2 = 0.111 (std: 0.108)
  mlp-adaptive-xl: R2 = 0.194 (std: 0.115)
  mlp-l: R2 = 0.193 (std: 0.112)
  svr-rbf-xl: R2 = 0.231 (std: 0.103)
  svr-poly-l: R2 = 0.231 (std: 0.103)
  knn-tuned-sqrt: R2 = 0.100 (std: 0.078)
  knn-tuned-l: R2 = 0.100 (std: 0.078)
  ridge: R2 = 0.111 (std: 0.107)

Model-based training with 13 models
Best R2: 0.231, Mean R2: 0.171
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=313.1841, entropy=0.0663, kl_div=0.0000
    Epoch 1: policy_loss=-0.0128, value_loss=313.1842, entropy=0.0661, kl_div=-0.0298
  Round 1/3: Mean predicted reward = 6.167
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9885, entropy=0.0633, kl_div=0.0000
    Epoch 1: policy_loss=-0.0008, value_loss=0.9885, entropy=0.0632, kl_div=-0.0286
  Round 2/3: Mean predicted reward = 14.415
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.080 rf-tuned-xl:0.080 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.072 xgb-l:0.072 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.072 knn-tuned-l:0.072 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9881, entropy=0.0628, kl_div=0.0000
    Epoch 1: policy_loss=-0.0085, value_loss=0.9881, entropy=0.0629, kl_div=0.0374
  Round 3/3: Mean predicted reward = 14.678

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 49 Results ---
  Mean Oracle Reward: 14.231
  Min Oracle Reward: 9.304
  Max Oracle Reward: 17.512
  Std Oracle Reward: 1.830
  Sequence Diversity: 0.859
  Models Used: 13
  Model R2 - Mean: 0.171, Max: 0.231, Count: 13
  Total Sequences Evaluated: 3186
    Oracle Count: 3136 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 50/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 3186

--- Round 50 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.797) ---
  AGCGATCCGGCGTCCGTGCA
  CCTCGGGGGAGCATCACGCT
  AAGAGGCGTCCCGGCTGTCC
  CCCTAAGGGCATGGTCGCCG
  CCGCCTGGGAGCTTACGACG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.541
  Max reward: 17.406
  With intrinsic bonuses: 14.515

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9884, entropy=0.0641, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2130

=== Surrogate Model Training ===
Total samples: 3250

Training on 3150 samples (removed 100 outliers)
Reward range: [9.08, 19.13], mean: 14.19
  Created 13 candidate models for data size 3150
Current R2 threshold: 0.10000000000000003
  rf-tuned-l: R2 = 0.187 (std: 0.091)
  rf-tuned-xl: R2 = 0.182 (std: 0.092)
  gb-tuned-l: R2 = 0.212 (std: 0.102)
  gb-tuned-xl: R2 = 0.212 (std: 0.102)
  xgb-xl: R2 = 0.118 (std: 0.100)
  xgb-l: R2 = 0.118 (std: 0.100)
  mlp-adaptive-xl: R2 = 0.189 (std: 0.100)
  mlp-l: R2 = 0.183 (std: 0.097)
  svr-rbf-xl: R2 = 0.227 (std: 0.098)
  svr-poly-l: R2 = 0.227 (std: 0.098)
  knn-tuned-sqrt: R2 = 0.091 (std: 0.083)
  knn-tuned-l: R2 = 0.091 (std: 0.083)
  ridge: R2 = 0.113 (std: 0.102)

Model-based training with 11 models
Best R2: 0.227, Mean R2: 0.165
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.091 gb-tuned-l:0.094 gb-tuned-xl:0.094 xgb-xl:0.085 xgb-l:0.085 mlp-adaptive-xl:0.092 mlp-l:0.091 svr-rbf-xl:0.095 svr-poly-l:0.095 ridge:0.085 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=199.8614, entropy=0.0691, kl_div=0.0000
    Epoch 1: policy_loss=0.0138, value_loss=199.8626, entropy=0.0672, kl_div=-0.0595
  Round 1/3: Mean predicted reward = 3.992
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.091 gb-tuned-l:0.094 gb-tuned-xl:0.094 xgb-xl:0.085 xgb-l:0.085 mlp-adaptive-xl:0.092 mlp-l:0.091 svr-rbf-xl:0.095 svr-poly-l:0.095 ridge:0.085 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9883, entropy=0.0626, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3967
  Round 2/3: Mean predicted reward = 14.667
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.091 gb-tuned-l:0.094 gb-tuned-xl:0.094 xgb-xl:0.085 xgb-l:0.085 mlp-adaptive-xl:0.092 mlp-l:0.091 svr-rbf-xl:0.095 svr-poly-l:0.095 ridge:0.085 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9880, entropy=0.0720, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1344
  Round 3/3: Mean predicted reward = 14.487

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 50 Results ---
  Mean Oracle Reward: 14.537
  Min Oracle Reward: 10.242
  Max Oracle Reward: 17.406
  Std Oracle Reward: 1.585
  Sequence Diversity: 0.797
  Models Used: 11
  Model R2 - Mean: 0.165, Max: 0.227, Count: 13
  Total Sequences Evaluated: 3250
    Oracle Count: 3200 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 51/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 3250

--- Round 51 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.859) ---
  ACTCCGAGACGGTGGCTGCC
  CAGGCTGTGGCGGCCCTAAC
  CCTGGTCGAACCGAGTCCGG
  GGGCCTAGACGCATCCTGGC
  CACCTCGACGCGTGGACGGT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.342
  Max reward: 17.396
  With intrinsic bonuses: 14.358

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9870, entropy=0.0674, kl_div=0.0000
    Epoch 1: policy_loss=-0.0559, value_loss=0.9870, entropy=0.0659, kl_div=-0.1369

=== Surrogate Model Training ===
Total samples: 3314

Training on 3212 samples (removed 102 outliers)
Reward range: [9.11, 19.12], mean: 14.19
  Created 13 candidate models for data size 3212
Current R2 threshold: 0.11000000000000004
  rf-tuned-l: R2 = 0.194 (std: 0.082)
  rf-tuned-xl: R2 = 0.191 (std: 0.086)
  gb-tuned-l: R2 = 0.214 (std: 0.093)
  gb-tuned-xl: R2 = 0.214 (std: 0.093)
  xgb-xl: R2 = 0.118 (std: 0.093)
  xgb-l: R2 = 0.118 (std: 0.093)
  mlp-adaptive-xl: R2 = 0.184 (std: 0.085)
  mlp-l: R2 = 0.189 (std: 0.082)
  svr-rbf-xl: R2 = 0.226 (std: 0.089)
  svr-poly-l: R2 = 0.226 (std: 0.089)
  knn-tuned-sqrt: R2 = 0.124 (std: 0.100)
  knn-tuned-l: R2 = 0.124 (std: 0.100)
  ridge: R2 = 0.118 (std: 0.101)

Model-based training with 13 models
Best R2: 0.226, Mean R2: 0.172
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=320.4369, entropy=0.0622, kl_div=0.0000
    Epoch 1: policy_loss=0.0075, value_loss=320.4386, entropy=0.0608, kl_div=-0.1518
  Round 1/3: Mean predicted reward = 2.476
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9876, entropy=0.0665, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3805
  Round 2/3: Mean predicted reward = 14.665
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.079 rf-tuned-xl:0.078 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.073 xgb-l:0.073 mlp-adaptive-xl:0.078 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9885, entropy=0.0707, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4702
  Round 3/3: Mean predicted reward = 14.464

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 51 Results ---
  Mean Oracle Reward: 14.388
  Min Oracle Reward: 10.506
  Max Oracle Reward: 17.531
  Std Oracle Reward: 1.505
  Sequence Diversity: 0.859
  Models Used: 13
  Model R2 - Mean: 0.172, Max: 0.226, Count: 13
  Total Sequences Evaluated: 3314
    Oracle Count: 3264 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 52/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 3314

--- Round 52 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  GTCCCGGCGACGAGACGTTC
  CCGGGGTCCCGGACAGATCT
  GCCACGTCTCCGGGGAATGC
  GTCGGAACGCGCCCTCTGAG
  CAGCGGCCGTACTAGCGCGT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.857
  Max reward: 18.427
  With intrinsic bonuses: 13.890

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9868, entropy=0.0788, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1419

=== Surrogate Model Training ===
Total samples: 3378

Training on 3270 samples (removed 108 outliers)
Reward range: [9.16, 19.10], mean: 14.19
  Created 13 candidate models for data size 3270
Current R2 threshold: 0.12
  rf-tuned-l: R2 = 0.189 (std: 0.080)
  rf-tuned-xl: R2 = 0.186 (std: 0.081)
  gb-tuned-l: R2 = 0.207 (std: 0.082)
  gb-tuned-xl: R2 = 0.207 (std: 0.082)
  xgb-xl: R2 = 0.113 (std: 0.100)
  xgb-l: R2 = 0.113 (std: 0.100)
  mlp-adaptive-xl: R2 = 0.182 (std: 0.093)
  mlp-l: R2 = 0.197 (std: 0.093)
  svr-rbf-xl: R2 = 0.214 (std: 0.083)
  svr-poly-l: R2 = 0.214 (std: 0.083)
  knn-tuned-sqrt: R2 = 0.114 (std: 0.085)
  knn-tuned-l: R2 = 0.114 (std: 0.085)
  ridge: R2 = 0.115 (std: 0.091)

Model-based training with 8 models
Best R2: 0.214, Mean R2: 0.167
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.124 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=183.8918, entropy=0.0723, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1392
  Round 1/3: Mean predicted reward = 5.535
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.124 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0793, kl_div=0.0000
    Epoch 1: policy_loss=-0.0203, value_loss=0.9854, entropy=0.0787, kl_div=-0.0365
  Round 2/3: Mean predicted reward = 14.237
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.124 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9873, entropy=0.0723, kl_div=0.0000
    Epoch 1: policy_loss=-0.0300, value_loss=0.9873, entropy=0.0707, kl_div=0.0145
  Round 3/3: Mean predicted reward = 14.452

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 52 Results ---
  Mean Oracle Reward: 13.920
  Min Oracle Reward: 8.628
  Max Oracle Reward: 18.949
  Std Oracle Reward: 1.921
  Sequence Diversity: 0.938
  Models Used: 8
  Model R2 - Mean: 0.167, Max: 0.214, Count: 13
  Total Sequences Evaluated: 3378
    Oracle Count: 3328 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 53/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 3378

--- Round 53 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.922) ---
  CGCGCGGCCCAAGTGGCATT
  CATGGCGGTTACCGCAGCGC
  GCGGCGACGCAGGTCTCCTA
  CTGGGCGACCTCGGAACGCT
  GCGGCGCGAATCTAGTCCCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.803
  Max reward: 16.940
  With intrinsic bonuses: 13.776

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9871, entropy=0.0705, kl_div=0.0000
    Epoch 1: policy_loss=-0.0477, value_loss=0.9871, entropy=0.0698, kl_div=-0.0057

=== Surrogate Model Training ===
Total samples: 3442

Training on 3329 samples (removed 113 outliers)
Reward range: [9.16, 19.08], mean: 14.17
  Created 13 candidate models for data size 3329
Current R2 threshold: 0.13
  rf-tuned-l: R2 = 0.180 (std: 0.090)
  rf-tuned-xl: R2 = 0.184 (std: 0.092)
  gb-tuned-l: R2 = 0.202 (std: 0.082)
  gb-tuned-xl: R2 = 0.202 (std: 0.082)
  xgb-xl: R2 = 0.117 (std: 0.099)
  xgb-l: R2 = 0.117 (std: 0.099)
  mlp-adaptive-xl: R2 = 0.176 (std: 0.088)
  mlp-l: R2 = 0.190 (std: 0.085)
  svr-rbf-xl: R2 = 0.207 (std: 0.088)
  svr-poly-l: R2 = 0.207 (std: 0.088)
  knn-tuned-sqrt: R2 = 0.111 (std: 0.088)
  knn-tuned-l: R2 = 0.111 (std: 0.088)
  ridge: R2 = 0.113 (std: 0.084)

Model-based training with 8 models
Best R2: 0.207, Mean R2: 0.163
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.124 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=455.2183, entropy=0.0626, kl_div=0.0000
    Epoch 1: policy_loss=0.0347, value_loss=455.2192, entropy=0.0615, kl_div=-0.1205
  Round 1/3: Mean predicted reward = 4.532
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.124 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9872, entropy=0.0639, kl_div=0.0000
    Epoch 1: policy_loss=-0.0182, value_loss=0.9872, entropy=0.0636, kl_div=0.0334
  Round 2/3: Mean predicted reward = 14.423
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.124 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.127 svr-poly-l:0.127 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9863, entropy=0.0590, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0682
  Round 3/3: Mean predicted reward = 14.588

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 53 Results ---
  Mean Oracle Reward: 13.776
  Min Oracle Reward: 9.014
  Max Oracle Reward: 17.069
  Std Oracle Reward: 1.712
  Sequence Diversity: 0.922
  Models Used: 8
  Model R2 - Mean: 0.163, Max: 0.207, Count: 13
  Total Sequences Evaluated: 3442
    Oracle Count: 3392 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 54/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 3442

--- Round 54 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.797) ---
  CCGCTGGGTCAGTGCGACCA
  GTAGTCGGCGGCCTGCAACC
  GCCGCGCTCAGCGTAGAGCT
  GGAGTCCACGGAGTCTGCCC
  TGTACGTAACCCCGCGGCGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.968
  Max reward: 17.264
  With intrinsic bonuses: 14.009

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9895, entropy=0.0648, kl_div=0.0000
    Epoch 1: policy_loss=-0.0136, value_loss=0.9895, entropy=0.0646, kl_div=-0.0084

=== Surrogate Model Training ===
Total samples: 3506

Training on 3390 samples (removed 116 outliers)
Reward range: [9.16, 19.06], mean: 14.17
  Created 13 candidate models for data size 3390
Current R2 threshold: 0.14
  rf-tuned-l: R2 = 0.170 (std: 0.093)
  rf-tuned-xl: R2 = 0.170 (std: 0.088)
  gb-tuned-l: R2 = 0.197 (std: 0.075)
  gb-tuned-xl: R2 = 0.197 (std: 0.075)
  xgb-xl: R2 = 0.105 (std: 0.095)
  xgb-l: R2 = 0.105 (std: 0.095)
  mlp-adaptive-xl: R2 = 0.181 (std: 0.077)
  mlp-l: R2 = 0.179 (std: 0.078)
  svr-rbf-xl: R2 = 0.198 (std: 0.084)
  svr-poly-l: R2 = 0.198 (std: 0.084)
  knn-tuned-sqrt: R2 = 0.115 (std: 0.086)
  knn-tuned-l: R2 = 0.115 (std: 0.086)
  ridge: R2 = 0.117 (std: 0.081)

Model-based training with 8 models
Best R2: 0.198, Mean R2: 0.157
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.126 svr-poly-l:0.126 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=331.6736, entropy=0.0617, kl_div=0.0000
    Epoch 1: policy_loss=-0.0020, value_loss=331.6739, entropy=0.0615, kl_div=-0.0709
  Round 1/3: Mean predicted reward = 3.222
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.126 svr-poly-l:0.126 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9872, entropy=0.0649, kl_div=0.0000
    Epoch 1: policy_loss=-0.0007, value_loss=0.9872, entropy=0.0649, kl_div=0.0044
  Round 2/3: Mean predicted reward = 14.444
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.123 rf-tuned-xl:0.123 gb-tuned-l:0.126 gb-tuned-xl:0.126 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.126 svr-poly-l:0.126 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9870, entropy=0.0628, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0509
  Round 3/3: Mean predicted reward = 14.454

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 54 Results ---
  Mean Oracle Reward: 13.945
  Min Oracle Reward: 9.084
  Max Oracle Reward: 17.347
  Std Oracle Reward: 2.012
  Sequence Diversity: 0.797
  Models Used: 8
  Model R2 - Mean: 0.157, Max: 0.198, Count: 13
  Total Sequences Evaluated: 3506
    Oracle Count: 3456 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 55/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 3506
  Performance plateaued, reducing LR to 0.000150

--- Round 55 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.891) ---
  CTACACGGGGACCCGGTTGC
  GCGGACCCCTAATTCCGGGG
  CAAGCCTAGCTGGGCCGTGC
  GGATCGTCGCGCAATGCCCG
  CCGTCGGTGTGCGAACGCAC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.983
  Max reward: 17.675
  With intrinsic bonuses: 13.983

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.0604, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0741

=== Surrogate Model Training ===
Total samples: 3570

Training on 3454 samples (removed 116 outliers)
Reward range: [9.16, 19.06], mean: 14.17
  Created 13 candidate models for data size 3454
Current R2 threshold: 0.15000000000000002
  rf-tuned-l: R2 = 0.145 (std: 0.098)
  rf-tuned-xl: R2 = 0.144 (std: 0.098)
  gb-tuned-l: R2 = 0.186 (std: 0.073)
  gb-tuned-xl: R2 = 0.186 (std: 0.073)
  xgb-xl: R2 = 0.083 (std: 0.092)
  xgb-l: R2 = 0.083 (std: 0.092)
  mlp-adaptive-xl: R2 = 0.167 (std: 0.086)
  mlp-l: R2 = 0.161 (std: 0.089)
  svr-rbf-xl: R2 = 0.187 (std: 0.089)
  svr-poly-l: R2 = 0.187 (std: 0.089)
  knn-tuned-sqrt: R2 = 0.093 (std: 0.093)
  knn-tuned-l: R2 = 0.093 (std: 0.093)
  ridge: R2 = 0.111 (std: 0.075)

Model-based training with 6 models
Best R2: 0.187, Mean R2: 0.141
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.168 gb-tuned-xl:0.168 mlp-adaptive-xl:0.165 mlp-l:0.164 svr-rbf-xl:0.168 svr-poly-l:0.168 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=228.9960, entropy=0.0598, kl_div=0.0000
    Epoch 1: policy_loss=-0.0122, value_loss=228.9968, entropy=0.0598, kl_div=-0.0481
  Round 1/3: Mean predicted reward = 4.868
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.168 gb-tuned-xl:0.168 mlp-adaptive-xl:0.165 mlp-l:0.164 svr-rbf-xl:0.168 svr-poly-l:0.168 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9870, entropy=0.0636, kl_div=0.0000
    Epoch 1: policy_loss=-0.0033, value_loss=0.9870, entropy=0.0638, kl_div=-0.1466
  Round 2/3: Mean predicted reward = 14.510
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.168 gb-tuned-xl:0.168 mlp-adaptive-xl:0.165 mlp-l:0.164 svr-rbf-xl:0.168 svr-poly-l:0.168 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9875, entropy=0.0588, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1079
  Round 3/3: Mean predicted reward = 14.403

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 55 Results ---
  Mean Oracle Reward: 13.969
  Min Oracle Reward: 9.612
  Max Oracle Reward: 17.398
  Std Oracle Reward: 1.889
  Sequence Diversity: 0.891
  Models Used: 6
  Model R2 - Mean: 0.141, Max: 0.187, Count: 13
  Total Sequences Evaluated: 3570
    Oracle Count: 3520 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 56/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 3570
  Performance plateaued, reducing LR to 0.000136

--- Round 56 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.891) ---
  GGTCGGCCCGGGCCACTTAA
  AGGTTCCGGTCCACGGCGCA
  CCGCCTAGGGCCAAGTTGCG
  GAGGGGTCCTCGCGCAACTC
  AGTGCATAGCTCGCGCCGCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.053
  Max reward: 17.487
  With intrinsic bonuses: 14.018

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9874, entropy=0.0635, kl_div=0.0000
    Epoch 1: policy_loss=-0.0298, value_loss=0.9874, entropy=0.0634, kl_div=-0.0278

=== Surrogate Model Training ===
Total samples: 3634

Training on 3516 samples (removed 118 outliers)
Reward range: [9.19, 19.06], mean: 14.17
  Created 13 candidate models for data size 3516
Current R2 threshold: 0.16000000000000003
  rf-tuned-l: R2 = 0.136 (std: 0.113)
  rf-tuned-xl: R2 = 0.135 (std: 0.112)
  gb-tuned-l: R2 = 0.180 (std: 0.086)
  gb-tuned-xl: R2 = 0.180 (std: 0.086)
  xgb-xl: R2 = 0.086 (std: 0.107)
  xgb-l: R2 = 0.086 (std: 0.107)
  mlp-adaptive-xl: R2 = 0.165 (std: 0.094)
  mlp-l: R2 = 0.152 (std: 0.106)
  svr-rbf-xl: R2 = 0.179 (std: 0.118)
  svr-poly-l: R2 = 0.179 (std: 0.118)
  knn-tuned-sqrt: R2 = 0.085 (std: 0.100)
  knn-tuned-l: R2 = 0.085 (std: 0.100)
  ridge: R2 = 0.111 (std: 0.075)

Model-based training with 5 models
Best R2: 0.180, Mean R2: 0.135
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.201 gb-tuned-xl:0.201 mlp-adaptive-xl:0.198 svr-rbf-xl:0.201 svr-poly-l:0.201 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=800.7545, entropy=0.0636, kl_div=0.0000
    Epoch 1: policy_loss=0.0374, value_loss=800.7559, entropy=0.0635, kl_div=-0.1991
  Round 1/3: Mean predicted reward = 0.792
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.201 gb-tuned-xl:0.201 mlp-adaptive-xl:0.198 svr-rbf-xl:0.201 svr-poly-l:0.201 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9876, entropy=0.0649, kl_div=0.0000
    Epoch 1: policy_loss=-0.0145, value_loss=0.9876, entropy=0.0654, kl_div=0.0111
  Round 2/3: Mean predicted reward = 14.418
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.201 gb-tuned-xl:0.201 mlp-adaptive-xl:0.198 svr-rbf-xl:0.201 svr-poly-l:0.201 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9864, entropy=0.0591, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2476
  Round 3/3: Mean predicted reward = 14.502

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 56 Results ---
  Mean Oracle Reward: 14.054
  Min Oracle Reward: 9.722
  Max Oracle Reward: 17.413
  Std Oracle Reward: 1.701
  Sequence Diversity: 0.891
  Models Used: 5
  Model R2 - Mean: 0.135, Max: 0.180, Count: 13
  Total Sequences Evaluated: 3634
    Oracle Count: 3584 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 57/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 3634
  Performance plateaued, reducing LR to 0.000100

--- Round 57 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.891) ---
  GGGTTGCGAGCGCTCACACC
  CTCGGGCCCGACTCGGAAGT
  GTAGCTACCGGGGACCCGCT
  CGTCGGTTAGCGCGACGCCA
  TGACCCTCGCGGGTAACGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.584
  Max reward: 17.926
  With intrinsic bonuses: 13.551

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9875, entropy=0.0651, kl_div=0.0000
    Epoch 1: policy_loss=0.0460, value_loss=0.9875, entropy=0.0650, kl_div=0.0352

=== Surrogate Model Training ===
Total samples: 3698

Training on 3575 samples (removed 123 outliers)
Reward range: [9.19, 19.04], mean: 14.16
  Created 13 candidate models for data size 3575
Current R2 threshold: 0.17000000000000004
  rf-tuned-l: R2 = 0.115 (std: 0.118)
  rf-tuned-xl: R2 = 0.119 (std: 0.117)
  gb-tuned-l: R2 = 0.160 (std: 0.100)
  gb-tuned-xl: R2 = 0.160 (std: 0.100)
  xgb-xl: R2 = 0.067 (std: 0.128)
  xgb-l: R2 = 0.067 (std: 0.128)
  mlp-adaptive-xl: R2 = 0.133 (std: 0.102)
  mlp-l: R2 = 0.153 (std: 0.104)
  svr-rbf-xl: R2 = 0.169 (std: 0.129)
  svr-poly-l: R2 = 0.169 (std: 0.129)
  knn-tuned-sqrt: R2 = 0.073 (std: 0.106)
  knn-tuned-l: R2 = 0.073 (std: 0.106)
  ridge: R2 = 0.099 (std: 0.070)
  Fallback: Using svr-rbf-xl with R2 = 0.169

Model-based training with 1 models
Best R2: 0.169, Mean R2: 0.120
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=224.4970, entropy=0.0671, kl_div=0.0000
    Epoch 1: policy_loss=-0.0209, value_loss=224.4975, entropy=0.0671, kl_div=-0.1190
  Round 1/3: Mean predicted reward = 3.542
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9883, entropy=0.0622, kl_div=0.0000
    Epoch 1: policy_loss=-0.0113, value_loss=0.9882, entropy=0.0622, kl_div=-0.0797
  Round 2/3: Mean predicted reward = 14.268
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9874, entropy=0.0647, kl_div=0.0000
    Epoch 1: policy_loss=-0.0124, value_loss=0.9874, entropy=0.0647, kl_div=-0.0732
  Round 3/3: Mean predicted reward = 14.411

  === Progress Analysis ===
  Status: NORMAL

--- Round 57 Results ---
  Mean Oracle Reward: 13.571
  Min Oracle Reward: 8.139
  Max Oracle Reward: 17.970
  Std Oracle Reward: 1.908
  Sequence Diversity: 0.891
  Models Used: 1
  Model R2 - Mean: 0.120, Max: 0.169, Count: 13
  Total Sequences Evaluated: 3698
    Oracle Count: 3648 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 58/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 3698

--- Round 58 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.859) ---
  TGCGCGGCGTACACCCTGAG
  CAGCTTGGCCCGAGGACCGT
  CCGGGAGGATAGGCCTCCCT
  CGTGCTGTAAGACGCCCGCG
  TCCACAGCGACTGTGGCCGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 13.994
  Max reward: 17.464
  With intrinsic bonuses: 13.976

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9874, entropy=0.0652, kl_div=0.0000
    Epoch 1: policy_loss=-0.0788, value_loss=0.9873, entropy=0.0647, kl_div=-0.1096

=== Surrogate Model Training ===
Total samples: 3762

Training on 3638 samples (removed 124 outliers)
Reward range: [9.19, 18.99], mean: 14.15
  Created 13 candidate models for data size 3638
Current R2 threshold: 0.18
  rf-tuned-l: R2 = 0.099 (std: 0.140)
  rf-tuned-xl: R2 = 0.097 (std: 0.148)
  gb-tuned-l: R2 = 0.147 (std: 0.121)
  gb-tuned-xl: R2 = 0.147 (std: 0.121)
  xgb-xl: R2 = 0.047 (std: 0.147)
  xgb-l: R2 = 0.047 (std: 0.147)
  mlp-adaptive-xl: R2 = 0.117 (std: 0.131)
  mlp-l: R2 = 0.113 (std: 0.141)
  svr-rbf-xl: R2 = 0.153 (std: 0.157)
  svr-poly-l: R2 = 0.153 (std: 0.157)
  knn-tuned-sqrt: R2 = 0.060 (std: 0.120)
  knn-tuned-l: R2 = 0.060 (std: 0.120)
  ridge: R2 = 0.092 (std: 0.072)
  Fallback: Using svr-rbf-xl with R2 = 0.153

Model-based training with 1 models
Best R2: 0.153, Mean R2: 0.103
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=225.2939, entropy=0.0582, kl_div=0.0000
    Epoch 1: policy_loss=-0.0054, value_loss=225.2947, entropy=0.0569, kl_div=-0.1930
  Round 1/3: Mean predicted reward = 1.613
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9887, entropy=0.0555, kl_div=0.0000
    Epoch 1: policy_loss=0.0360, value_loss=0.9887, entropy=0.0544, kl_div=-0.1997
  Round 2/3: Mean predicted reward = 14.636
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9890, entropy=0.0518, kl_div=0.0000
    Epoch 1: policy_loss=-0.0042, value_loss=0.9890, entropy=0.0501, kl_div=-0.1491
  Round 3/3: Mean predicted reward = 14.733

  === Progress Analysis ===
  Status: NORMAL

--- Round 58 Results ---
  Mean Oracle Reward: 13.974
  Min Oracle Reward: 9.856
  Max Oracle Reward: 17.308
  Std Oracle Reward: 1.510
  Sequence Diversity: 0.859
  Models Used: 1
  Model R2 - Mean: 0.103, Max: 0.153, Count: 13
  Total Sequences Evaluated: 3762
    Oracle Count: 3712 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 59/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 3762

--- Round 59 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.797) ---
  CGAATGCGCGGCTCCTACGG
  CGGCGCCTCAGCGAAGGTCT
  CTAACCCATGGGGGGTGCCC
  GGCATACCACTGGTCGGGCC
  CCCTCAGGAGAGCTTGCGCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.691
  Max reward: 18.139
  With intrinsic bonuses: 14.743

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9888, entropy=0.0511, kl_div=0.0000
    Epoch 1: policy_loss=-0.0205, value_loss=0.9888, entropy=0.0506, kl_div=-0.0414

=== Surrogate Model Training ===
Total samples: 3826

Training on 3702 samples (removed 124 outliers)
Reward range: [9.19, 19.04], mean: 14.17
  Created 13 candidate models for data size 3702
Current R2 threshold: 0.19
  rf-tuned-l: R2 = 0.129 (std: 0.142)
  rf-tuned-xl: R2 = 0.125 (std: 0.142)
  gb-tuned-l: R2 = 0.161 (std: 0.119)
  gb-tuned-xl: R2 = 0.161 (std: 0.119)
  xgb-xl: R2 = 0.065 (std: 0.145)
  xgb-l: R2 = 0.065 (std: 0.145)
  mlp-adaptive-xl: R2 = 0.153 (std: 0.134)
  mlp-l: R2 = 0.141 (std: 0.136)
  svr-rbf-xl: R2 = 0.176 (std: 0.154)
  svr-poly-l: R2 = 0.176 (std: 0.154)
  knn-tuned-sqrt: R2 = 0.074 (std: 0.116)
  knn-tuned-l: R2 = 0.074 (std: 0.116)
  ridge: R2 = 0.104 (std: 0.082)
  Fallback: Using svr-rbf-xl with R2 = 0.176

Model-based training with 1 models
Best R2: 0.176, Mean R2: 0.123
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=244.4849, entropy=0.0508, kl_div=0.0000
    Epoch 1: policy_loss=-0.0199, value_loss=244.4853, entropy=0.0498, kl_div=-0.0676
  Round 1/3: Mean predicted reward = -0.205
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9881, entropy=0.0465, kl_div=0.0000
    Epoch 1: policy_loss=-0.0006, value_loss=0.9881, entropy=0.0462, kl_div=-0.0172
  Round 2/3: Mean predicted reward = 14.693
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9873, entropy=0.0458, kl_div=0.0000
    Epoch 1: policy_loss=-0.0109, value_loss=0.9873, entropy=0.0454, kl_div=0.0124
  Round 3/3: Mean predicted reward = 14.720

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 59 Results ---
  Mean Oracle Reward: 14.728
  Min Oracle Reward: 8.484
  Max Oracle Reward: 17.935
  Std Oracle Reward: 2.251
  Sequence Diversity: 0.797
  Models Used: 1
  Model R2 - Mean: 0.123, Max: 0.176, Count: 13
  Total Sequences Evaluated: 3826
    Oracle Count: 3776 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 60/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 3826
  Consistent improvement, increasing LR to 0.000360

--- Round 60 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  GCCAGGCCCTGCTCGAGATG
  GCACGCGCGACTTCGTGACG
  GCTCGCAGGTTCGGACCCGA
  GGCCCCCGTACGGTGCGAAT
  GCTAGTGGCGACGCTACGCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.840
  Max reward: 17.697
  With intrinsic bonuses: 14.812

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=0.0396, kl_div=0.0000
    Epoch 1: policy_loss=0.0212, value_loss=0.9881, entropy=0.0334, kl_div=-0.0702

=== Surrogate Model Training ===
Total samples: 3890

Training on 3762 samples (removed 128 outliers)
Reward range: [9.29, 19.04], mean: 14.18
  Created 13 candidate models for data size 3762
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.141 (std: 0.128)
  rf-tuned-xl: R2 = 0.139 (std: 0.129)
  gb-tuned-l: R2 = 0.171 (std: 0.115)
  gb-tuned-xl: R2 = 0.171 (std: 0.115)
  xgb-xl: R2 = 0.086 (std: 0.128)
  xgb-l: R2 = 0.086 (std: 0.128)
  mlp-adaptive-xl: R2 = 0.160 (std: 0.109)
  mlp-l: R2 = 0.147 (std: 0.134)
  svr-rbf-xl: R2 = 0.191 (std: 0.139)
  svr-poly-l: R2 = 0.191 (std: 0.139)
  knn-tuned-sqrt: R2 = 0.086 (std: 0.125)
  knn-tuned-l: R2 = 0.086 (std: 0.125)
  ridge: R2 = 0.106 (std: 0.090)
  Fallback: Using svr-rbf-xl with R2 = 0.191

Model-based training with 1 models
Best R2: 0.191, Mean R2: 0.135
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=316.5786, entropy=0.0321, kl_div=0.0000
    Epoch 1: policy_loss=-0.0011, value_loss=316.5823, entropy=0.0294, kl_div=-0.1512
  Round 1/3: Mean predicted reward = -5.196
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9874, entropy=0.0286, kl_div=0.0000
    Epoch 1: policy_loss=-0.0016, value_loss=0.9874, entropy=0.0291, kl_div=0.0384
  Round 2/3: Mean predicted reward = 15.172
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:1.000 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9873, entropy=0.0288, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5227
  Round 3/3: Mean predicted reward = 14.970

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 60 Results ---
  Mean Oracle Reward: 14.841
  Min Oracle Reward: 8.308
  Max Oracle Reward: 17.649
  Std Oracle Reward: 2.105
  Sequence Diversity: 0.766
  Models Used: 1
  Model R2 - Mean: 0.135, Max: 0.191, Count: 13
  Total Sequences Evaluated: 3890
    Oracle Count: 3840 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 61/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 3890
  Consistent improvement, increasing LR to 0.000327

--- Round 61 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GGGACTGGCCGTCTAACCCG
  GCAATGCCTGCGAGCCCGTG
  GCGCCAGATCCGGTGACCGT
  CCCAGTGACCTGGGCGTGAC
  CCCCTCTGGTCGAGGAGAGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.469
  Max reward: 17.529
  With intrinsic bonuses: 14.498

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9882, entropy=0.0319, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3742

=== Surrogate Model Training ===
Total samples: 3954

Training on 3828 samples (removed 126 outliers)
Reward range: [9.19, 19.05], mean: 14.19
  Created 13 candidate models for data size 3828
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.153 (std: 0.120)
  rf-tuned-xl: R2 = 0.154 (std: 0.120)
  gb-tuned-l: R2 = 0.190 (std: 0.109)
  gb-tuned-xl: R2 = 0.190 (std: 0.109)
  xgb-xl: R2 = 0.106 (std: 0.120)
  xgb-l: R2 = 0.106 (std: 0.120)
  mlp-adaptive-xl: R2 = 0.165 (std: 0.120)
  mlp-l: R2 = 0.182 (std: 0.120)
  svr-rbf-xl: R2 = 0.206 (std: 0.125)
  svr-poly-l: R2 = 0.206 (std: 0.125)
  knn-tuned-sqrt: R2 = 0.096 (std: 0.120)
  knn-tuned-l: R2 = 0.096 (std: 0.120)
  ridge: R2 = 0.115 (std: 0.100)

Model-based training with 2 models
Best R2: 0.206, Mean R2: 0.151
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:0.500 svr-poly-l:0.500 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=456.4626, entropy=0.0325, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2244
  Round 1/3: Mean predicted reward = -6.091
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:0.500 svr-poly-l:0.500 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9878, entropy=0.0309, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2462
  Round 2/3: Mean predicted reward = 15.067
Current Method: weighted
    Using performance-based weights
    Model weights: svr-rbf-xl:0.500 svr-poly-l:0.500 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9883, entropy=0.0295, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4031
  Round 3/3: Mean predicted reward = 14.734

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 61 Results ---
  Mean Oracle Reward: 14.489
  Min Oracle Reward: 6.850
  Max Oracle Reward: 17.689
  Std Oracle Reward: 2.348
  Sequence Diversity: 0.719
  Models Used: 2
  Model R2 - Mean: 0.151, Max: 0.206, Count: 13
  Total Sequences Evaluated: 3954
    Oracle Count: 3904 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 62/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 3954

--- Round 62 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.703) ---
  TGGATCACGGCCCTGACCGG
  CCGAATAGGGGTGCCGCCTC
  CCAGCTGTCGGCTCGAGAGC
  GGCCGCTAGACGCTCGCTGA
  GCAAATGCGGCCGCTTCGCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.693
  Max reward: 17.992
  With intrinsic bonuses: 14.601

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9875, entropy=0.0286, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1001

=== Surrogate Model Training ===
Total samples: 4018

Training on 3897 samples (removed 121 outliers)
Reward range: [9.19, 19.10], mean: 14.21
  Created 13 candidate models for data size 3897
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.166 (std: 0.115)
  rf-tuned-xl: R2 = 0.167 (std: 0.115)
  gb-tuned-l: R2 = 0.204 (std: 0.108)
  gb-tuned-xl: R2 = 0.204 (std: 0.108)
  xgb-xl: R2 = 0.130 (std: 0.112)
  xgb-l: R2 = 0.130 (std: 0.112)
  mlp-adaptive-xl: R2 = 0.184 (std: 0.115)
  mlp-l: R2 = 0.194 (std: 0.106)
  svr-rbf-xl: R2 = 0.222 (std: 0.117)
  svr-poly-l: R2 = 0.222 (std: 0.117)
  knn-tuned-sqrt: R2 = 0.121 (std: 0.106)
  knn-tuned-l: R2 = 0.121 (std: 0.106)
  ridge: R2 = 0.118 (std: 0.116)

Model-based training with 4 models
Best R2: 0.222, Mean R2: 0.168
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.248 gb-tuned-xl:0.248 svr-rbf-xl:0.252 svr-poly-l:0.252 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1169.1702, entropy=0.0328, kl_div=0.0000
    Epoch 1: policy_loss=0.0102, value_loss=1169.1753, entropy=0.0306, kl_div=-0.0178
  Round 1/3: Mean predicted reward = -8.075
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.248 gb-tuned-xl:0.248 svr-rbf-xl:0.252 svr-poly-l:0.252 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9877, entropy=0.0246, kl_div=0.0000
    Epoch 1: policy_loss=-0.0104, value_loss=0.9876, entropy=0.0240, kl_div=0.0069
  Round 2/3: Mean predicted reward = 14.711
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.248 gb-tuned-xl:0.248 svr-rbf-xl:0.252 svr-poly-l:0.252 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9871, entropy=0.0219, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2320
  Round 3/3: Mean predicted reward = 14.887

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 62 Results ---
  Mean Oracle Reward: 14.617
  Min Oracle Reward: 6.139
  Max Oracle Reward: 17.766
  Std Oracle Reward: 2.684
  Sequence Diversity: 0.703
  Models Used: 4
  Model R2 - Mean: 0.168, Max: 0.222, Count: 13
  Total Sequences Evaluated: 4018
    Oracle Count: 3968 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 63/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 4018

--- Round 63 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  CCCGGGATCGATCCGTAGCG
  TCCCGCAGTGCAGCTGGGCA
  GACCCACCCGTGTTGGAGCG
  CTCCCGTAGCGGCGTACGAG
  ATGAGTACCGCCGCGCTGCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.653
  Max reward: 17.746
  With intrinsic bonuses: 14.625

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9882, entropy=0.0306, kl_div=0.0000
    Epoch 1: policy_loss=-0.0039, value_loss=0.9882, entropy=0.0304, kl_div=0.0499

=== Surrogate Model Training ===
Total samples: 4082

Training on 3960 samples (removed 122 outliers)
Reward range: [9.19, 19.10], mean: 14.22
  Created 13 candidate models for data size 3960
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.183 (std: 0.096)
  rf-tuned-xl: R2 = 0.182 (std: 0.103)
  gb-tuned-l: R2 = 0.210 (std: 0.095)
  gb-tuned-xl: R2 = 0.210 (std: 0.095)
  xgb-xl: R2 = 0.135 (std: 0.094)
  xgb-l: R2 = 0.135 (std: 0.094)
  mlp-adaptive-xl: R2 = 0.191 (std: 0.098)
  mlp-l: R2 = 0.201 (std: 0.094)
  svr-rbf-xl: R2 = 0.231 (std: 0.101)
  svr-poly-l: R2 = 0.231 (std: 0.101)
  knn-tuned-sqrt: R2 = 0.126 (std: 0.106)
  knn-tuned-l: R2 = 0.126 (std: 0.106)
  ridge: R2 = 0.125 (std: 0.120)

Model-based training with 5 models
Best R2: 0.231, Mean R2: 0.176
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.199 gb-tuned-xl:0.199 mlp-l:0.197 svr-rbf-xl:0.203 svr-poly-l:0.203 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=1028.9365, entropy=0.0288, kl_div=0.0000
    Epoch 1: policy_loss=-0.0053, value_loss=1028.9388, entropy=0.0271, kl_div=-0.0831
  Round 1/3: Mean predicted reward = -9.800
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.199 gb-tuned-xl:0.199 mlp-l:0.197 svr-rbf-xl:0.203 svr-poly-l:0.203 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.0264, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0698
  Round 2/3: Mean predicted reward = 14.907
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.199 gb-tuned-xl:0.199 mlp-l:0.197 svr-rbf-xl:0.203 svr-poly-l:0.203 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.0274, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0987
  Round 3/3: Mean predicted reward = 14.851

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 63 Results ---
  Mean Oracle Reward: 14.685
  Min Oracle Reward: 8.239
  Max Oracle Reward: 17.617
  Std Oracle Reward: 2.184
  Sequence Diversity: 0.750
  Models Used: 5
  Model R2 - Mean: 0.176, Max: 0.231, Count: 13
  Total Sequences Evaluated: 4082
    Oracle Count: 4032 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 64/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 4082
  Performance plateaued, reducing LR to 0.000019

--- Round 64 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  TCAAGCGCCGGCATGGCTGC
  ATTGCGCCGCTCGCCGGGAA
  GTTCCCGGAACCGTGGCGAC
  GCATCGGATCGCCGATGCCG
  CGGATCTCGGGCCGAGTACC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.930
  Max reward: 19.195
  With intrinsic bonuses: 14.906

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9879, entropy=0.0233, kl_div=0.0000
    Epoch 1: policy_loss=-0.0018, value_loss=0.9879, entropy=0.0233, kl_div=0.0051

=== Surrogate Model Training ===
Total samples: 4146

Training on 4028 samples (removed 118 outliers)
Reward range: [9.15, 19.13], mean: 14.23
  Created 13 candidate models for data size 4028
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.197 (std: 0.095)
  rf-tuned-xl: R2 = 0.194 (std: 0.091)
  gb-tuned-l: R2 = 0.224 (std: 0.098)
  gb-tuned-xl: R2 = 0.224 (std: 0.098)
  xgb-xl: R2 = 0.160 (std: 0.101)
  xgb-l: R2 = 0.160 (std: 0.101)
  mlp-adaptive-xl: R2 = 0.206 (std: 0.087)
  mlp-l: R2 = 0.203 (std: 0.100)
  svr-rbf-xl: R2 = 0.245 (std: 0.096)
  svr-poly-l: R2 = 0.245 (std: 0.096)
  knn-tuned-sqrt: R2 = 0.134 (std: 0.108)
  knn-tuned-l: R2 = 0.134 (std: 0.108)
  ridge: R2 = 0.134 (std: 0.129)

Model-based training with 6 models
Best R2: 0.245, Mean R2: 0.189
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.167 gb-tuned-xl:0.167 mlp-adaptive-xl:0.164 mlp-l:0.163 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=1666.1024, entropy=0.0217, kl_div=0.0000
    Epoch 1: policy_loss=-0.0113, value_loss=1666.1030, entropy=0.0214, kl_div=-0.0204
  Round 1/3: Mean predicted reward = -14.362
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.167 gb-tuned-xl:0.167 mlp-adaptive-xl:0.164 mlp-l:0.163 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9874, entropy=0.0260, kl_div=0.0000
    Epoch 1: policy_loss=-0.0006, value_loss=0.9874, entropy=0.0257, kl_div=-0.0115
  Round 2/3: Mean predicted reward = 14.618
Current Method: weighted
    Using performance-based weights
    Model weights: gb-tuned-l:0.167 gb-tuned-xl:0.167 mlp-adaptive-xl:0.164 mlp-l:0.163 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9878, entropy=0.0252, kl_div=0.0000
    Epoch 1: policy_loss=-0.0056, value_loss=0.9878, entropy=0.0252, kl_div=0.0113
  Round 3/3: Mean predicted reward = 14.717

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 64 Results ---
  Mean Oracle Reward: 14.885
  Min Oracle Reward: 9.408
  Max Oracle Reward: 19.266
  Std Oracle Reward: 2.268
  Sequence Diversity: 0.719
  Models Used: 6
  Model R2 - Mean: 0.189, Max: 0.245, Count: 13
  Total Sequences Evaluated: 4146
    Oracle Count: 4096 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 65/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 4146

--- Round 65 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  ATCGGCTAGCAGGCGTGCCC
  CCTCCGTGAGCATGGCAGCG
  CGTCACGATCCGTGCGAGCG
  CTGGAAAGGCGCGTTCCGCC
  AGGGGGCAGCTACTGCCCTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.702
  Max reward: 17.566
  With intrinsic bonuses: 14.723

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9872, entropy=0.0231, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0736

=== Surrogate Model Training ===
Total samples: 4210

Training on 4090 samples (removed 120 outliers)
Reward range: [9.16, 19.13], mean: 14.24
  Created 13 candidate models for data size 4090
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.197 (std: 0.092)
  rf-tuned-xl: R2 = 0.205 (std: 0.087)
  gb-tuned-l: R2 = 0.225 (std: 0.094)
  gb-tuned-xl: R2 = 0.225 (std: 0.094)
  xgb-xl: R2 = 0.152 (std: 0.107)
  xgb-l: R2 = 0.152 (std: 0.107)
  mlp-adaptive-xl: R2 = 0.223 (std: 0.098)
  mlp-l: R2 = 0.201 (std: 0.093)
  svr-rbf-xl: R2 = 0.250 (std: 0.091)
  svr-poly-l: R2 = 0.250 (std: 0.091)
  knn-tuned-sqrt: R2 = 0.140 (std: 0.101)
  knn-tuned-l: R2 = 0.140 (std: 0.101)
  ridge: R2 = 0.134 (std: 0.125)

Model-based training with 7 models
Best R2: 0.250, Mean R2: 0.192
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-xl:0.140 gb-tuned-l:0.143 gb-tuned-xl:0.143 mlp-adaptive-xl:0.142 mlp-l:0.139 svr-rbf-xl:0.146 svr-poly-l:0.146 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2283.1824, entropy=0.0255, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=2283.1926, entropy=0.0235, kl_div=-0.1580
  Round 1/3: Mean predicted reward = -16.623
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-xl:0.140 gb-tuned-l:0.143 gb-tuned-xl:0.143 mlp-adaptive-xl:0.142 mlp-l:0.139 svr-rbf-xl:0.146 svr-poly-l:0.146 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9877, entropy=0.0209, kl_div=0.0000
    Epoch 1: policy_loss=0.0129, value_loss=0.9876, entropy=0.0203, kl_div=-0.0516
  Round 2/3: Mean predicted reward = 14.889
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-xl:0.140 gb-tuned-l:0.143 gb-tuned-xl:0.143 mlp-adaptive-xl:0.142 mlp-l:0.139 svr-rbf-xl:0.146 svr-poly-l:0.146 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9872, entropy=0.0149, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3552
  Round 3/3: Mean predicted reward = 14.655

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 65 Results ---
  Mean Oracle Reward: 14.728
  Min Oracle Reward: 7.504
  Max Oracle Reward: 17.783
  Std Oracle Reward: 2.130
  Sequence Diversity: 0.766
  Models Used: 7
  Model R2 - Mean: 0.192, Max: 0.250, Count: 13
  Total Sequences Evaluated: 4210
    Oracle Count: 4160 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 66/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 4210

--- Round 66 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GGAGGCCCCGTAATCGTCGC
  CGTCCAGTCCGGAGCTGCAG
  CGGATCGGGTGAATCCGCCC
  GACGGGTCGTCCGCCTGACA
  AGTATGCGGGGCCCGACTCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.977
  Max reward: 17.633
  With intrinsic bonuses: 14.971

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9876, entropy=0.0185, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3664

=== Surrogate Model Training ===
Total samples: 4274

Training on 4154 samples (removed 120 outliers)
Reward range: [9.16, 19.13], mean: 14.25
  Created 13 candidate models for data size 4154
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.203 (std: 0.106)
  rf-tuned-xl: R2 = 0.204 (std: 0.106)
  gb-tuned-l: R2 = 0.223 (std: 0.102)
  gb-tuned-xl: R2 = 0.223 (std: 0.102)
  xgb-xl: R2 = 0.159 (std: 0.126)
  xgb-l: R2 = 0.159 (std: 0.126)
  mlp-adaptive-xl: R2 = 0.232 (std: 0.112)
  mlp-l: R2 = 0.223 (std: 0.099)
  svr-rbf-xl: R2 = 0.259 (std: 0.106)
  svr-poly-l: R2 = 0.259 (std: 0.106)
  knn-tuned-sqrt: R2 = 0.146 (std: 0.108)
  knn-tuned-l: R2 = 0.146 (std: 0.108)
  ridge: R2 = 0.134 (std: 0.131)

Model-based training with 8 models
Best R2: 0.259, Mean R2: 0.198
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=4096.9707, entropy=0.0227, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1086
  Round 1/3: Mean predicted reward = -19.686
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9868, entropy=0.0209, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3512
  Round 2/3: Mean predicted reward = 14.804
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9873, entropy=0.0248, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4856
  Round 3/3: Mean predicted reward = 14.797

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 66 Results ---
  Mean Oracle Reward: 14.993
  Min Oracle Reward: 10.687
  Max Oracle Reward: 17.591
  Std Oracle Reward: 1.993
  Sequence Diversity: 0.719
  Models Used: 8
  Model R2 - Mean: 0.198, Max: 0.259, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 4274
    Oracle Count: 4224 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 67/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 4274

--- Round 67 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  ACGCCGCCGTGTCCTAGAGG
  AGGTAGGGACCCGTCGCCCT
  TAGGATTGCGCCGACGCGCC
  AGGGATCGTCTCGCCGCCAG
  TATAGCAGGGCCCCTGCGCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.813
  Max reward: 17.746
  With intrinsic bonuses: 14.827

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9875, entropy=0.0302, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1618

=== Surrogate Model Training ===
Total samples: 4338

Training on 4221 samples (removed 117 outliers)
Reward range: [9.15, 19.20], mean: 14.26
  Created 13 candidate models for data size 4221
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.206 (std: 0.114)
  rf-tuned-xl: R2 = 0.207 (std: 0.113)
  gb-tuned-l: R2 = 0.230 (std: 0.111)
  gb-tuned-xl: R2 = 0.230 (std: 0.111)
  xgb-xl: R2 = 0.165 (std: 0.119)
  xgb-l: R2 = 0.165 (std: 0.119)
  mlp-adaptive-xl: R2 = 0.222 (std: 0.114)
  mlp-l: R2 = 0.223 (std: 0.117)
  svr-rbf-xl: R2 = 0.262 (std: 0.119)
  svr-poly-l: R2 = 0.262 (std: 0.119)
  knn-tuned-sqrt: R2 = 0.162 (std: 0.129)
  knn-tuned-l: R2 = 0.162 (std: 0.129)
  ridge: R2 = 0.134 (std: 0.139)

Model-based training with 8 models
Best R2: 0.262, Mean R2: 0.202
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2403.6638, entropy=0.0297, kl_div=0.0000
    Epoch 1: policy_loss=0.0022, value_loss=2403.6711, entropy=0.0293, kl_div=-0.0082
  Round 1/3: Mean predicted reward = -18.067
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9875, entropy=0.0311, kl_div=0.0000
    Epoch 1: policy_loss=-0.0104, value_loss=0.9875, entropy=0.0309, kl_div=0.0087
  Round 2/3: Mean predicted reward = 14.634
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9869, entropy=0.0295, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1809
  Round 3/3: Mean predicted reward = 14.486

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 67 Results ---
  Mean Oracle Reward: 14.826
  Min Oracle Reward: 9.259
  Max Oracle Reward: 17.795
  Std Oracle Reward: 2.025
  Sequence Diversity: 0.750
  Models Used: 8
  Model R2 - Mean: 0.202, Max: 0.262, Count: 13
  Total Sequences Evaluated: 4338
    Oracle Count: 4288 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 68/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 4338

--- Round 68 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.781) ---
  TCAAACGGCCGTCCCGGGTG
  CTACGGGGGTACAGCCGTCC
  GGCAGTCGGTCGACCCAGTC
  GAGGCCGGCCTGCAATGTCC
  CAACGGTTCCGCGTCGGGCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.859
  Max reward: 17.971
  With intrinsic bonuses: 14.819

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9869, entropy=0.0346, kl_div=0.0000
    Epoch 1: policy_loss=0.0098, value_loss=0.9869, entropy=0.0348, kl_div=0.0407

=== Surrogate Model Training ===
Total samples: 4402

Training on 4284 samples (removed 118 outliers)
Reward range: [9.15, 19.20], mean: 14.27
  Created 13 candidate models for data size 4284
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.217 (std: 0.121)
  rf-tuned-xl: R2 = 0.215 (std: 0.122)
  gb-tuned-l: R2 = 0.235 (std: 0.118)
  gb-tuned-xl: R2 = 0.235 (std: 0.118)
  xgb-xl: R2 = 0.174 (std: 0.129)
  xgb-l: R2 = 0.174 (std: 0.129)
  mlp-adaptive-xl: R2 = 0.235 (std: 0.116)
  mlp-l: R2 = 0.227 (std: 0.127)
  svr-rbf-xl: R2 = 0.270 (std: 0.131)
  svr-poly-l: R2 = 0.270 (std: 0.131)
  knn-tuned-sqrt: R2 = 0.163 (std: 0.137)
  knn-tuned-l: R2 = 0.163 (std: 0.137)
  ridge: R2 = 0.135 (std: 0.142)

Model-based training with 8 models
Best R2: 0.270, Mean R2: 0.209
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2126.5503, entropy=0.0318, kl_div=0.0000
    Epoch 1: policy_loss=0.0056, value_loss=2126.5537, entropy=0.0292, kl_div=-0.0673
  Round 1/3: Mean predicted reward = -18.278
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9871, entropy=0.0327, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0544
  Round 2/3: Mean predicted reward = 14.883
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9872, entropy=0.0294, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0838
  Round 3/3: Mean predicted reward = 14.736

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 68 Results ---
  Mean Oracle Reward: 14.834
  Min Oracle Reward: 8.078
  Max Oracle Reward: 17.983
  Std Oracle Reward: 1.915
  Sequence Diversity: 0.781
  Models Used: 8
  Model R2 - Mean: 0.209, Max: 0.270, Count: 13
  Total Sequences Evaluated: 4402
    Oracle Count: 4352 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 69/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 4402
  Performance plateaued, reducing LR to 0.000019

--- Round 69 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  ATCCTGGGCTGACGCGGCCA
  GGCCAGGCATATCTCGGCGC
  ACGCCGGCCTGGGTCAGCTA
  CCGTATGCCACCGGAGGTCG
  ATCCGGGGCTGACGCCTCGA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.700
  Max reward: 17.558
  With intrinsic bonuses: 14.730

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9866, entropy=0.0335, kl_div=0.0000
    Epoch 1: policy_loss=-0.0020, value_loss=0.9866, entropy=0.0334, kl_div=0.0043

=== Surrogate Model Training ===
Total samples: 4466

Training on 4348 samples (removed 118 outliers)
Reward range: [9.15, 19.23], mean: 14.28
  Created 13 candidate models for data size 4348
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.216 (std: 0.141)
  rf-tuned-xl: R2 = 0.211 (std: 0.142)
  gb-tuned-l: R2 = 0.235 (std: 0.126)
  gb-tuned-xl: R2 = 0.235 (std: 0.126)
  xgb-xl: R2 = 0.170 (std: 0.144)
  xgb-l: R2 = 0.170 (std: 0.144)
  mlp-adaptive-xl: R2 = 0.236 (std: 0.129)
  mlp-l: R2 = 0.230 (std: 0.131)
  svr-rbf-xl: R2 = 0.280 (std: 0.140)
  svr-poly-l: R2 = 0.280 (std: 0.140)
  knn-tuned-sqrt: R2 = 0.158 (std: 0.152)
  knn-tuned-l: R2 = 0.158 (std: 0.152)
  ridge: R2 = 0.134 (std: 0.139)

Model-based training with 8 models
Best R2: 0.280, Mean R2: 0.209
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=3252.4075, entropy=0.0284, kl_div=0.0000
    Epoch 1: policy_loss=-0.0050, value_loss=3252.4087, entropy=0.0279, kl_div=-0.0195
  Round 1/3: Mean predicted reward = -21.618
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9872, entropy=0.0357, kl_div=0.0000
    Epoch 1: policy_loss=-0.0022, value_loss=0.9872, entropy=0.0350, kl_div=-0.0155
  Round 2/3: Mean predicted reward = 14.616
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.124 gb-tuned-xl:0.124 mlp-adaptive-xl:0.124 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9872, entropy=0.0311, kl_div=0.0000
    Epoch 1: policy_loss=-0.0045, value_loss=0.9872, entropy=0.0303, kl_div=-0.0113
  Round 3/3: Mean predicted reward = 14.729

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 69 Results ---
  Mean Oracle Reward: 14.711
  Min Oracle Reward: 9.175
  Max Oracle Reward: 17.549
  Std Oracle Reward: 1.953
  Sequence Diversity: 0.766
  Models Used: 8
  Model R2 - Mean: 0.209, Max: 0.280, Count: 13
  Total Sequences Evaluated: 4466
    Oracle Count: 4416 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 70/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 4466
  Performance plateaued, reducing LR to 0.000150

--- Round 70 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  ACGCCTGCATCGGAGTCGGC
  GAGCACGCGCAGTTCTCGGC
  GGTGCACAGCGACCTCCGTG
  AGGCCCGGCTACCCTGTAGG
  ACCGCGGAGACCCTGTCGTG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 15.327
  Max reward: 17.966
  With intrinsic bonuses: 15.277

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9867, entropy=0.0294, kl_div=0.0000
    Epoch 1: policy_loss=-0.0162, value_loss=0.9867, entropy=0.0235, kl_div=-0.1688

=== Surrogate Model Training ===
Total samples: 4530

Training on 4410 samples (removed 120 outliers)
Reward range: [9.19, 19.24], mean: 14.30
  Created 13 candidate models for data size 4410
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.211 (std: 0.142)
  rf-tuned-xl: R2 = 0.207 (std: 0.142)
  gb-tuned-l: R2 = 0.238 (std: 0.122)
  gb-tuned-xl: R2 = 0.238 (std: 0.122)
  xgb-xl: R2 = 0.170 (std: 0.136)
  xgb-l: R2 = 0.170 (std: 0.136)
  mlp-adaptive-xl: R2 = 0.220 (std: 0.135)
  mlp-l: R2 = 0.232 (std: 0.138)
  svr-rbf-xl: R2 = 0.279 (std: 0.145)
  svr-poly-l: R2 = 0.279 (std: 0.145)
  knn-tuned-sqrt: R2 = 0.157 (std: 0.147)
  knn-tuned-l: R2 = 0.157 (std: 0.147)
  ridge: R2 = 0.129 (std: 0.132)

Model-based training with 8 models
Best R2: 0.279, Mean R2: 0.207
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=4535.0400, entropy=0.0163, kl_div=0.0000
    Epoch 1: policy_loss=-0.0001, value_loss=4535.0503, entropy=0.0141, kl_div=-0.1001
  Round 1/3: Mean predicted reward = -26.635
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9870, entropy=0.0137, kl_div=0.0000
    Epoch 1: policy_loss=-0.0144, value_loss=0.9869, entropy=0.0136, kl_div=0.0497
  Round 2/3: Mean predicted reward = 14.823
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.124 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9868, entropy=0.0175, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2059
  Round 3/3: Mean predicted reward = 14.895

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 70 Results ---
  Mean Oracle Reward: 15.297
  Min Oracle Reward: 11.318
  Max Oracle Reward: 17.672
  Std Oracle Reward: 1.619
  Sequence Diversity: 0.734
  Models Used: 8
  Model R2 - Mean: 0.207, Max: 0.279, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 4530
    Oracle Count: 4480 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 71/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 4530

--- Round 71 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  CCCCGCTTAGCGGTAGGGAC
  GTCAAGCCGCGTCGCCTGGA
  GCGCGCAGTCATCCCGTAGG
  TCGGCTCGGCAGAGACCGCT
  CAGCCCGAATCCGTTGGGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.706
  Max reward: 17.652
  With intrinsic bonuses: 14.669

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9868, entropy=0.0168, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1230

=== Surrogate Model Training ===
Total samples: 4594

Training on 4471 samples (removed 123 outliers)
Reward range: [9.19, 19.24], mean: 14.31
  Created 13 candidate models for data size 4471
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.207 (std: 0.167)
  rf-tuned-xl: R2 = 0.205 (std: 0.166)
  gb-tuned-l: R2 = 0.233 (std: 0.137)
  gb-tuned-xl: R2 = 0.233 (std: 0.137)
  xgb-xl: R2 = 0.164 (std: 0.171)
  xgb-l: R2 = 0.164 (std: 0.171)
  mlp-adaptive-xl: R2 = 0.223 (std: 0.155)
  mlp-l: R2 = 0.234 (std: 0.152)
  svr-rbf-xl: R2 = 0.275 (std: 0.164)
  svr-poly-l: R2 = 0.275 (std: 0.164)
  knn-tuned-sqrt: R2 = 0.163 (std: 0.170)
  knn-tuned-l: R2 = 0.163 (std: 0.170)
  ridge: R2 = 0.125 (std: 0.123)

Model-based training with 8 models
Best R2: 0.275, Mean R2: 0.205
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6119.3672, entropy=0.0168, kl_div=0.0000
    Epoch 1: policy_loss=0.0042, value_loss=6119.3872, entropy=0.0160, kl_div=-0.0095
  Round 1/3: Mean predicted reward = -29.827
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9869, entropy=0.0127, kl_div=0.0000
    Epoch 1: policy_loss=-0.0136, value_loss=0.9869, entropy=0.0124, kl_div=0.0286
  Round 2/3: Mean predicted reward = 14.938
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.123 mlp-l:0.125 svr-rbf-xl:0.130 svr-poly-l:0.130 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9871, entropy=0.0137, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1815
  Round 3/3: Mean predicted reward = 14.823

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 71 Results ---
  Mean Oracle Reward: 14.694
  Min Oracle Reward: 5.498
  Max Oracle Reward: 17.549
  Std Oracle Reward: 2.540
  Sequence Diversity: 0.734
  Models Used: 8
  Model R2 - Mean: 0.205, Max: 0.275, Count: 13
  Total Sequences Evaluated: 4594
    Oracle Count: 4544 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 72/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 4594

--- Round 72 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  CCGAATGGGGCCCCTTGCAG
  AGCGCCAAGGTCGGTTCGCC
  CCGCTGCGATGACCTCGGGA
  CTCCTCGGGGGGTCAACCAG
  GGTCTCCAGGCCGCGTACAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.865
  Max reward: 17.558
  With intrinsic bonuses: 14.819

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9867, entropy=0.0150, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0741

=== Surrogate Model Training ===
Total samples: 4658

Training on 4533 samples (removed 125 outliers)
Reward range: [9.19, 19.24], mean: 14.32
  Created 13 candidate models for data size 4533
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.202 (std: 0.186)
  rf-tuned-xl: R2 = 0.202 (std: 0.186)
  gb-tuned-l: R2 = 0.231 (std: 0.155)
  gb-tuned-xl: R2 = 0.231 (std: 0.155)
  xgb-xl: R2 = 0.158 (std: 0.182)
  xgb-l: R2 = 0.158 (std: 0.182)
  mlp-adaptive-xl: R2 = 0.231 (std: 0.170)
  mlp-l: R2 = 0.235 (std: 0.174)
  svr-rbf-xl: R2 = 0.260 (std: 0.195)
  svr-poly-l: R2 = 0.260 (std: 0.195)
  knn-tuned-sqrt: R2 = 0.170 (std: 0.196)
  knn-tuned-l: R2 = 0.170 (std: 0.196)
  ridge: R2 = 0.120 (std: 0.129)

Model-based training with 8 models
Best R2: 0.260, Mean R2: 0.202
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=4435.0156, entropy=0.0165, kl_div=0.0000
    Epoch 1: policy_loss=-0.0053, value_loss=4435.0264, entropy=0.0160, kl_div=-0.0291
  Round 1/3: Mean predicted reward = -31.497
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9865, entropy=0.0154, kl_div=0.0000
    Epoch 1: policy_loss=-0.0035, value_loss=0.9865, entropy=0.0155, kl_div=0.0043
  Round 2/3: Mean predicted reward = 14.782
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9866, entropy=0.0145, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2009
  Round 3/3: Mean predicted reward = 14.986

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 72 Results ---
  Mean Oracle Reward: 14.838
  Min Oracle Reward: 7.741
  Max Oracle Reward: 17.575
  Std Oracle Reward: 2.412
  Sequence Diversity: 0.719
  Models Used: 8
  Model R2 - Mean: 0.202, Max: 0.260, Count: 13
  Total Sequences Evaluated: 4658
    Oracle Count: 4608 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 73/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 4658

--- Round 73 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GTGTATCCACGGGCGCGCCA
  TGGCCAGACTCGGCTCAGCG
  GTCGCGAACGACCTCTGGGC
  ACCTGACTCGTGGGAGCCCG
  GCCTGGGCCTCAAAGGGCCT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 15.152
  Max reward: 17.723
  With intrinsic bonuses: 15.152

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9863, entropy=0.0216, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0697

=== Surrogate Model Training ===
Total samples: 4722

Training on 4597 samples (removed 125 outliers)
Reward range: [9.19, 19.28], mean: 14.33
  Created 13 candidate models for data size 4597
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.203 (std: 0.192)
  rf-tuned-xl: R2 = 0.200 (std: 0.192)
  gb-tuned-l: R2 = 0.224 (std: 0.168)
  gb-tuned-xl: R2 = 0.224 (std: 0.168)
  xgb-xl: R2 = 0.158 (std: 0.192)
  xgb-l: R2 = 0.158 (std: 0.192)
  mlp-adaptive-xl: R2 = 0.224 (std: 0.198)
  mlp-l: R2 = 0.232 (std: 0.185)
  svr-rbf-xl: R2 = 0.264 (std: 0.198)
  svr-poly-l: R2 = 0.264 (std: 0.198)
  knn-tuned-sqrt: R2 = 0.164 (std: 0.214)
  knn-tuned-l: R2 = 0.164 (std: 0.214)
  ridge: R2 = 0.116 (std: 0.129)

Model-based training with 7 models
Best R2: 0.264, Mean R2: 0.199
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 gb-tuned-l:0.141 gb-tuned-xl:0.141 mlp-adaptive-xl:0.141 mlp-l:0.143 svr-rbf-xl:0.147 svr-poly-l:0.147 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6443.4609, entropy=0.0212, kl_div=0.0000
    Epoch 1: policy_loss=0.0030, value_loss=6443.4697, entropy=0.0212, kl_div=0.0097
  Round 1/3: Mean predicted reward = -31.266
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 gb-tuned-l:0.141 gb-tuned-xl:0.141 mlp-adaptive-xl:0.141 mlp-l:0.143 svr-rbf-xl:0.147 svr-poly-l:0.147 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9866, entropy=0.0205, kl_div=0.0000
    Epoch 1: policy_loss=-0.0082, value_loss=0.9866, entropy=0.0205, kl_div=0.0140
  Round 2/3: Mean predicted reward = 14.884
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 gb-tuned-l:0.141 gb-tuned-xl:0.141 mlp-adaptive-xl:0.141 mlp-l:0.143 svr-rbf-xl:0.147 svr-poly-l:0.147 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9864, entropy=0.0196, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0754
  Round 3/3: Mean predicted reward = 14.653

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 73 Results ---
  Mean Oracle Reward: 15.146
  Min Oracle Reward: 5.392
  Max Oracle Reward: 17.943
  Std Oracle Reward: 2.370
  Sequence Diversity: 0.719
  Models Used: 7
  Model R2 - Mean: 0.199, Max: 0.264, Count: 13
  Total Sequences Evaluated: 4722
    Oracle Count: 4672 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 74/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 4722
  Consistent improvement, increasing LR to 0.000045

--- Round 74 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  CGGCATGCGGACATCGCCTG
  AACCGGGCCTGGCTTCCAGG
  TCCCAGGGTGGCCGTAGCCA
  AGCGCGTGTACGGCCATCGC
  AGAGGGCACCTTGGGCCTCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 15.129
  Max reward: 18.159
  With intrinsic bonuses: 15.132

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0240, kl_div=0.0000
    Epoch 1: policy_loss=0.0043, value_loss=0.9859, entropy=0.0241, kl_div=0.0125

=== Surrogate Model Training ===
Total samples: 4786

Training on 4663 samples (removed 123 outliers)
Reward range: [9.16, 19.31], mean: 14.34
  Created 13 candidate models for data size 4663
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.205 (std: 0.196)
  rf-tuned-xl: R2 = 0.206 (std: 0.195)
  gb-tuned-l: R2 = 0.232 (std: 0.165)
  gb-tuned-xl: R2 = 0.232 (std: 0.165)
  xgb-xl: R2 = 0.173 (std: 0.188)
  xgb-l: R2 = 0.173 (std: 0.188)
  mlp-adaptive-xl: R2 = 0.234 (std: 0.187)
  mlp-l: R2 = 0.223 (std: 0.174)
  svr-rbf-xl: R2 = 0.269 (std: 0.199)
  svr-poly-l: R2 = 0.269 (std: 0.199)
  knn-tuned-sqrt: R2 = 0.176 (std: 0.205)
  knn-tuned-l: R2 = 0.176 (std: 0.205)
  ridge: R2 = 0.121 (std: 0.137)

Model-based training with 8 models
Best R2: 0.269, Mean R2: 0.207
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=7206.6602, entropy=0.0216, kl_div=0.0000
    Epoch 1: policy_loss=-0.0070, value_loss=7206.6631, entropy=0.0207, kl_div=-0.0425
  Round 1/3: Mean predicted reward = -38.559
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9864, entropy=0.0239, kl_div=0.0000
    Epoch 1: policy_loss=-0.0029, value_loss=0.9864, entropy=0.0241, kl_div=-0.0002
  Round 2/3: Mean predicted reward = 14.829
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9863, entropy=0.0190, kl_div=0.0000
    Epoch 1: policy_loss=-0.0118, value_loss=0.9863, entropy=0.0200, kl_div=0.0322
  Round 3/3: Mean predicted reward = 14.710

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 74 Results ---
  Mean Oracle Reward: 15.147
  Min Oracle Reward: 8.934
  Max Oracle Reward: 18.306
  Std Oracle Reward: 2.091
  Sequence Diversity: 0.719
  Models Used: 8
  Model R2 - Mean: 0.207, Max: 0.269, Count: 13
  Total Sequences Evaluated: 4786
    Oracle Count: 4736 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 75/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 4786

--- Round 75 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  GACCCGTGGAACCGGTCGCT
  GGCGGCGCGACACGTCTCTA
  CCACTGGGGCCTTAAGGGCC
  TGTTCCGAAGACCGCGCGCG
  ACGGGGCACTGTTCGGCACC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.722
  Max reward: 17.525
  With intrinsic bonuses: 14.732

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9865, entropy=0.0241, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6230

=== Surrogate Model Training ===
Total samples: 4850

Training on 4726 samples (removed 124 outliers)
Reward range: [9.16, 19.31], mean: 14.35
  Created 13 candidate models for data size 4726
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.208 (std: 0.195)
  rf-tuned-xl: R2 = 0.208 (std: 0.196)
  gb-tuned-l: R2 = 0.237 (std: 0.166)
  gb-tuned-xl: R2 = 0.237 (std: 0.166)
  xgb-xl: R2 = 0.172 (std: 0.190)
  xgb-l: R2 = 0.172 (std: 0.190)
  mlp-adaptive-xl: R2 = 0.244 (std: 0.174)
  mlp-l: R2 = 0.237 (std: 0.185)
  svr-rbf-xl: R2 = 0.273 (std: 0.196)
  svr-poly-l: R2 = 0.273 (std: 0.196)
  knn-tuned-sqrt: R2 = 0.186 (std: 0.203)
  knn-tuned-l: R2 = 0.186 (std: 0.203)
  ridge: R2 = 0.129 (std: 0.138)

Model-based training with 8 models
Best R2: 0.273, Mean R2: 0.212
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.126 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2899.1133, entropy=0.0423, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2826
  Round 1/3: Mean predicted reward = -17.901
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.126 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9864, entropy=0.0541, kl_div=0.0000
    Epoch 1: policy_loss=0.0338, value_loss=0.9864, entropy=0.0561, kl_div=0.0333
  Round 2/3: Mean predicted reward = 14.623
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.126 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0550, kl_div=0.0000
    Epoch 1: policy_loss=-0.0477, value_loss=0.9859, entropy=0.0511, kl_div=0.0246
  Round 3/3: Mean predicted reward = 14.562

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 75 Results ---
  Mean Oracle Reward: 14.726
  Min Oracle Reward: 7.687
  Max Oracle Reward: 17.683
  Std Oracle Reward: 2.059
  Sequence Diversity: 0.750
  Models Used: 8
  Model R2 - Mean: 0.212, Max: 0.273, Count: 13
  Total Sequences Evaluated: 4850
    Oracle Count: 4800 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 76/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 4850

--- Round 76 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  TCGGAGACCCGTCCGGTAGC
  CGAGTATACTGCCGCGCGGC
  CGGAGGCGGCCACATTCTGC
  GGGCACCCCGAGCATTGGTC
  CGGCGCTTAGACGGCTGCCA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.149
  Max reward: 17.392
  With intrinsic bonuses: 14.170

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9857, entropy=0.0533, kl_div=0.0000
    Epoch 1: policy_loss=-0.0678, value_loss=0.9857, entropy=0.0496, kl_div=-0.0459

=== Surrogate Model Training ===
Total samples: 4914

Training on 4787 samples (removed 127 outliers)
Reward range: [9.19, 19.31], mean: 14.35
  Created 13 candidate models for data size 4787
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.213 (std: 0.187)
  rf-tuned-xl: R2 = 0.214 (std: 0.188)
  gb-tuned-l: R2 = 0.244 (std: 0.166)
  gb-tuned-xl: R2 = 0.244 (std: 0.166)
  xgb-xl: R2 = 0.182 (std: 0.184)
  xgb-l: R2 = 0.182 (std: 0.184)
  mlp-adaptive-xl: R2 = 0.241 (std: 0.178)
  mlp-l: R2 = 0.239 (std: 0.177)
  svr-rbf-xl: R2 = 0.277 (std: 0.186)
  svr-poly-l: R2 = 0.277 (std: 0.186)
  knn-tuned-sqrt: R2 = 0.185 (std: 0.193)
  knn-tuned-l: R2 = 0.185 (std: 0.193)
  ridge: R2 = 0.138 (std: 0.142)

Model-based training with 8 models
Best R2: 0.277, Mean R2: 0.217
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=13126.9844, entropy=0.0368, kl_div=0.0000
    Epoch 1: policy_loss=-0.0211, value_loss=13127.0088, entropy=0.0335, kl_div=-0.0975
  Round 1/3: Mean predicted reward = -32.220
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9859, entropy=0.0359, kl_div=0.0000
    Epoch 1: policy_loss=0.0107, value_loss=0.9859, entropy=0.0355, kl_div=-0.0521
  Round 2/3: Mean predicted reward = 14.617
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.121 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.124 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9861, entropy=0.0362, kl_div=0.0000
    Epoch 1: policy_loss=-0.0446, value_loss=0.9861, entropy=0.0366, kl_div=-0.0063
  Round 3/3: Mean predicted reward = 14.710

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 76 Results ---
  Mean Oracle Reward: 14.199
  Min Oracle Reward: 9.151
  Max Oracle Reward: 17.484
  Std Oracle Reward: 1.918
  Sequence Diversity: 0.844
  Models Used: 8
  Model R2 - Mean: 0.217, Max: 0.277, Count: 13
  Total Sequences Evaluated: 4914
    Oracle Count: 4864 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 77/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 4914

--- Round 77 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.781) ---
  TGGCGTGCCGGCCGTACACA
  CCTCGGGTGGAGACGCACCT
  CTAGCAGGGCGTTGCCCCGA
  CGGTAAGGCCCCTGGCGCTA
  ACTCAGCCCCGGGGTCGGAT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.503
  Max reward: 17.825
  With intrinsic bonuses: 14.511

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9859, entropy=0.0397, kl_div=0.0000
    Epoch 1: policy_loss=-0.0435, value_loss=0.9859, entropy=0.0394, kl_div=0.0035

=== Surrogate Model Training ===
Total samples: 4978

Training on 4853 samples (removed 125 outliers)
Reward range: [9.16, 19.31], mean: 14.35
  Created 13 candidate models for data size 4853
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.229 (std: 0.172)
  rf-tuned-xl: R2 = 0.231 (std: 0.171)
  gb-tuned-l: R2 = 0.252 (std: 0.159)
  gb-tuned-xl: R2 = 0.252 (std: 0.159)
  xgb-xl: R2 = 0.187 (std: 0.165)
  xgb-l: R2 = 0.187 (std: 0.165)
  mlp-adaptive-xl: R2 = 0.251 (std: 0.167)
  mlp-l: R2 = 0.253 (std: 0.161)
  svr-rbf-xl: R2 = 0.282 (std: 0.175)
  svr-poly-l: R2 = 0.282 (std: 0.175)
  knn-tuned-sqrt: R2 = 0.193 (std: 0.182)
  knn-tuned-l: R2 = 0.193 (std: 0.182)
  ridge: R2 = 0.149 (std: 0.147)

Model-based training with 8 models
Best R2: 0.282, Mean R2: 0.226
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=6821.3213, entropy=0.0321, kl_div=0.0000
    Epoch 1: policy_loss=0.0479, value_loss=6821.3335, entropy=0.0270, kl_div=-0.2062
  Round 1/3: Mean predicted reward = -30.727
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9860, entropy=0.0250, kl_div=0.0000
    Epoch 1: policy_loss=-0.0043, value_loss=0.9860, entropy=0.0260, kl_div=0.0218
  Round 2/3: Mean predicted reward = 14.873
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.129 svr-poly-l:0.129 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9860, entropy=0.0311, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2558
  Round 3/3: Mean predicted reward = 14.701

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 77 Results ---
  Mean Oracle Reward: 14.507
  Min Oracle Reward: 10.147
  Max Oracle Reward: 17.448
  Std Oracle Reward: 1.879
  Sequence Diversity: 0.781
  Models Used: 8
  Model R2 - Mean: 0.226, Max: 0.282, Count: 13
  Total Sequences Evaluated: 4978
    Oracle Count: 4928 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 78/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 4978

--- Round 78 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.828) ---
  GCTAACGCGAGCTGGCCCGT
  TCTGTCGGGCAGCAGCCGAC
  CCGTCCGGCCGGACGGTTAA
  AGCGGCCAGCCCGAGCTTTG
  GGAGTAGCGCCGTCTACCCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.734
  Max reward: 18.157
  With intrinsic bonuses: 14.739

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9855, entropy=0.0403, kl_div=0.0000
    Epoch 1: policy_loss=0.0515, value_loss=0.9855, entropy=0.0408, kl_div=0.0308

=== Surrogate Model Training ===
Total samples: 5042

Training on 4914 samples (removed 128 outliers)
Reward range: [9.19, 19.31], mean: 14.36
  Created 13 candidate models for data size 4914
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.228 (std: 0.164)
  rf-tuned-xl: R2 = 0.228 (std: 0.163)
  gb-tuned-l: R2 = 0.249 (std: 0.155)
  gb-tuned-xl: R2 = 0.249 (std: 0.155)
  xgb-xl: R2 = 0.191 (std: 0.158)
  xgb-l: R2 = 0.191 (std: 0.158)
  mlp-adaptive-xl: R2 = 0.252 (std: 0.158)
  mlp-l: R2 = 0.256 (std: 0.158)
  svr-rbf-xl: R2 = 0.280 (std: 0.170)
  svr-poly-l: R2 = 0.280 (std: 0.170)
  knn-tuned-sqrt: R2 = 0.187 (std: 0.179)
  knn-tuned-l: R2 = 0.187 (std: 0.179)
  ridge: R2 = 0.147 (std: 0.145)

Model-based training with 8 models
Best R2: 0.280, Mean R2: 0.225
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.128 svr-poly-l:0.128 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=5444.4854, entropy=0.0375, kl_div=0.0000
    Epoch 1: policy_loss=-0.0150, value_loss=5444.4897, entropy=0.0373, kl_div=-0.0170
  Round 1/3: Mean predicted reward = -21.566
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.128 svr-poly-l:0.128 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9860, entropy=0.0418, kl_div=0.0000
    Epoch 1: policy_loss=-0.0160, value_loss=0.9860, entropy=0.0412, kl_div=0.0182
  Round 2/3: Mean predicted reward = 14.836
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.122 rf-tuned-xl:0.122 gb-tuned-l:0.125 gb-tuned-xl:0.125 mlp-adaptive-xl:0.125 mlp-l:0.125 svr-rbf-xl:0.128 svr-poly-l:0.128 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9861, entropy=0.0363, kl_div=0.0000
    Epoch 1: policy_loss=-0.0226, value_loss=0.9861, entropy=0.0349, kl_div=-0.0916
  Round 3/3: Mean predicted reward = 14.905

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 78 Results ---
  Mean Oracle Reward: 14.693
  Min Oracle Reward: 9.072
  Max Oracle Reward: 18.312
  Std Oracle Reward: 2.036
  Sequence Diversity: 0.828
  Models Used: 8
  Model R2 - Mean: 0.225, Max: 0.280, Count: 13
  Total Sequences Evaluated: 5042
    Oracle Count: 4992 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 79/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 5042
  Consistent improvement, increasing LR to 0.000045

--- Round 79 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.781) ---
  TCACGCGTAGCCGCGGTCGA
  CGCCGACATGACGCTGGTGC
  GGGTCACCCCGCGATTGACG
  CCGCGCGGCAGCTGTAACGT
  GGATCGGACTCGGTCCCACG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.592
  Max reward: 17.643
  With intrinsic bonuses: 14.621

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9858, entropy=0.0339, kl_div=0.0000
    Epoch 1: policy_loss=-0.0355, value_loss=0.9858, entropy=0.0329, kl_div=-0.0276

=== Surrogate Model Training ===
Total samples: 5106

Training on 4978 samples (removed 128 outliers)
Reward range: [9.19, 19.31], mean: 14.36
  Created 13 candidate models for data size 4978
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.235 (std: 0.158)
  rf-tuned-xl: R2 = 0.235 (std: 0.162)
  gb-tuned-l: R2 = 0.260 (std: 0.152)
  gb-tuned-xl: R2 = 0.260 (std: 0.152)
  xgb-xl: R2 = 0.201 (std: 0.164)
  xgb-l: R2 = 0.201 (std: 0.164)
  mlp-adaptive-xl: R2 = 0.253 (std: 0.155)
  mlp-l: R2 = 0.258 (std: 0.155)
  svr-rbf-xl: R2 = 0.285 (std: 0.167)
  svr-poly-l: R2 = 0.285 (std: 0.167)
  knn-tuned-sqrt: R2 = 0.198 (std: 0.166)
  knn-tuned-l: R2 = 0.198 (std: 0.166)
  ridge: R2 = 0.156 (std: 0.148)

Model-based training with 10 models
Best R2: 0.285, Mean R2: 0.233
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.099 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.100 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=4885.6929, entropy=0.0302, kl_div=0.0000
    Epoch 1: policy_loss=0.0100, value_loss=4885.6943, entropy=0.0289, kl_div=-0.0722
  Round 1/3: Mean predicted reward = -32.020
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.099 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.100 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9861, entropy=0.0258, kl_div=0.0000
    Epoch 1: policy_loss=0.0023, value_loss=0.9861, entropy=0.0254, kl_div=-0.0178
  Round 2/3: Mean predicted reward = 14.957
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.099 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.100 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9856, entropy=0.0238, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0502
  Round 3/3: Mean predicted reward = 14.940

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 79 Results ---
  Mean Oracle Reward: 14.617
  Min Oracle Reward: 10.553
  Max Oracle Reward: 17.426
  Std Oracle Reward: 1.762
  Sequence Diversity: 0.781
  Models Used: 10
  Model R2 - Mean: 0.233, Max: 0.285, Count: 13
  Total Sequences Evaluated: 5106
    Oracle Count: 5056 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 80/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 5106
  Performance plateaued, reducing LR to 0.000150

--- Round 80 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  GCGTAGGCCGCCGGCTAATC
  CGTAAGCTCCGCGCGGTAGC
  GCGCAAGGCGCCGTCTCTAG
  ATATGGCCCGCGAGGCGCCT
  GGGACATTCGGCCCTGCGAC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.711
  Max reward: 17.552
  With intrinsic bonuses: 14.742

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0259, kl_div=0.0000
    Epoch 1: policy_loss=-0.0089, value_loss=0.9859, entropy=0.0250, kl_div=0.0218

=== Surrogate Model Training ===
Total samples: 5170

Training on 5038 samples (removed 132 outliers)
Reward range: [9.20, 19.31], mean: 14.37
  Created 13 candidate models for data size 5038
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.235 (std: 0.150)
  rf-tuned-xl: R2 = 0.236 (std: 0.151)
  gb-tuned-l: R2 = 0.267 (std: 0.143)
  gb-tuned-xl: R2 = 0.267 (std: 0.143)
  xgb-xl: R2 = 0.205 (std: 0.150)
  xgb-l: R2 = 0.205 (std: 0.150)
  mlp-adaptive-xl: R2 = 0.271 (std: 0.143)
  mlp-l: R2 = 0.263 (std: 0.147)
  svr-rbf-xl: R2 = 0.290 (std: 0.153)
  svr-poly-l: R2 = 0.290 (std: 0.153)
  knn-tuned-sqrt: R2 = 0.198 (std: 0.157)
  knn-tuned-l: R2 = 0.198 (std: 0.157)
  ridge: R2 = 0.163 (std: 0.142)

Model-based training with 10 models
Best R2: 0.290, Mean R2: 0.238
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.102 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6133.0884, entropy=0.0262, kl_div=0.0000
    Epoch 1: policy_loss=0.0256, value_loss=6133.0952, entropy=0.0245, kl_div=-0.1638
  Round 1/3: Mean predicted reward = -39.919
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.102 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9859, entropy=0.0192, kl_div=0.0000
    Epoch 1: policy_loss=-0.0113, value_loss=0.9859, entropy=0.0196, kl_div=0.0203
  Round 2/3: Mean predicted reward = 14.960
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.101 gb-tuned-xl:0.101 xgb-xl:0.095 xgb-l:0.095 mlp-adaptive-xl:0.102 mlp-l:0.101 svr-rbf-xl:0.104 svr-poly-l:0.104 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9858, entropy=0.0208, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1478
  Round 3/3: Mean predicted reward = 14.907

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 80 Results ---
  Mean Oracle Reward: 14.694
  Min Oracle Reward: 6.783
  Max Oracle Reward: 17.531
  Std Oracle Reward: 2.076
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.238, Max: 0.290, Count: 13
  Total Sequences Evaluated: 5170
    Oracle Count: 5120 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 81/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 5170
  Performance plateaued, reducing LR to 0.000136

--- Round 81 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  CTGCTGAACTCGCGGGGACC
  TGCAGACCTTCCAGCGCGGG
  GAGACGGTCACGCGCTTCGC
  CAGAACCTCCGGGTGTCGGC
  AGCTTGGCCAGCGTCGACCG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.876
  Max reward: 17.733
  With intrinsic bonuses: 14.857

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9856, entropy=0.0216, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0512

=== Surrogate Model Training ===
Total samples: 5234

Training on 5100 samples (removed 134 outliers)
Reward range: [9.22, 19.31], mean: 14.38
  Created 13 candidate models for data size 5100
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.242 (std: 0.143)
  rf-tuned-xl: R2 = 0.241 (std: 0.144)
  gb-tuned-l: R2 = 0.273 (std: 0.134)
  gb-tuned-xl: R2 = 0.273 (std: 0.134)
  xgb-xl: R2 = 0.208 (std: 0.141)
  xgb-l: R2 = 0.208 (std: 0.141)
  mlp-adaptive-xl: R2 = 0.265 (std: 0.146)
  mlp-l: R2 = 0.264 (std: 0.147)
  svr-rbf-xl: R2 = 0.295 (std: 0.143)
  svr-poly-l: R2 = 0.295 (std: 0.143)
  knn-tuned-sqrt: R2 = 0.206 (std: 0.150)
  knn-tuned-l: R2 = 0.206 (std: 0.150)
  ridge: R2 = 0.168 (std: 0.140)

Model-based training with 12 models
Best R2: 0.295, Mean R2: 0.242
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=9769.2021, entropy=0.0239, kl_div=0.0000
    Epoch 1: policy_loss=0.0021, value_loss=9769.2100, entropy=0.0234, kl_div=-0.0079
  Round 1/3: Mean predicted reward = -39.851
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0204, kl_div=0.0000
    Epoch 1: policy_loss=-0.0084, value_loss=0.9859, entropy=0.0206, kl_div=0.0285
  Round 2/3: Mean predicted reward = 14.946
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0222, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1583
  Round 3/3: Mean predicted reward = 14.863

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 81 Results ---
  Mean Oracle Reward: 14.896
  Min Oracle Reward: 8.056
  Max Oracle Reward: 17.991
  Std Oracle Reward: 1.892
  Sequence Diversity: 0.734
  Models Used: 12
  Model R2 - Mean: 0.242, Max: 0.295, Count: 13
  Total Sequences Evaluated: 5234
    Oracle Count: 5184 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 82/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 5234
  Consistent improvement, increasing LR to 0.000240

--- Round 82 Configuration ---
Learning rate: 0.000240
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  CGCATCGGGCATCAGTGCGC
  TAGCGCACGGGTCACGGCTC
  GTAGCCCGGTTACGGACCGC
  GGACCTATCGCTGAGCGCCG
  TGGCGACAAGCGCTCCGCGT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.646
  Max reward: 19.038
  With intrinsic bonuses: 14.623

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9857, entropy=0.0236, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0625

=== Surrogate Model Training ===
Total samples: 5298

Training on 5164 samples (removed 134 outliers)
Reward range: [9.22, 19.31], mean: 14.38
  Created 13 candidate models for data size 5164
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.244 (std: 0.139)
  rf-tuned-xl: R2 = 0.243 (std: 0.139)
  gb-tuned-l: R2 = 0.278 (std: 0.135)
  gb-tuned-xl: R2 = 0.278 (std: 0.135)
  xgb-xl: R2 = 0.218 (std: 0.141)
  xgb-l: R2 = 0.218 (std: 0.141)
  mlp-adaptive-xl: R2 = 0.281 (std: 0.136)
  mlp-l: R2 = 0.273 (std: 0.143)
  svr-rbf-xl: R2 = 0.301 (std: 0.139)
  svr-poly-l: R2 = 0.301 (std: 0.139)
  knn-tuned-sqrt: R2 = 0.212 (std: 0.143)
  knn-tuned-l: R2 = 0.212 (std: 0.143)
  ridge: R2 = 0.176 (std: 0.148)

Model-based training with 12 models
Best R2: 0.301, Mean R2: 0.249
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=8192.3271, entropy=0.0210, kl_div=0.0000
    Epoch 1: policy_loss=0.0193, value_loss=8192.3428, entropy=0.0194, kl_div=-0.0947
  Round 1/5: Mean predicted reward = -43.079
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9859, entropy=0.0200, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1331
  Round 2/5: Mean predicted reward = 14.753
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9859, entropy=0.0226, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0834
  Round 3/5: Mean predicted reward = 14.781
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9858, entropy=0.0207, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2095
  Round 4/5: Mean predicted reward = 14.848
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9858, entropy=0.0270, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2979
  Round 5/5: Mean predicted reward = 14.994

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 82 Results ---
  Mean Oracle Reward: 14.615
  Min Oracle Reward: 9.541
  Max Oracle Reward: 19.016
  Std Oracle Reward: 2.110
  Sequence Diversity: 0.750
  Models Used: 12
  Model R2 - Mean: 0.249, Max: 0.301, Count: 13
  Total Sequences Evaluated: 5298
    Oracle Count: 5248 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 83/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 5298
  Performance plateaued, reducing LR to 0.000055

--- Round 83 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  AGCGGTGGCGCCTCACGACT
  CGAGCTTCCGCAGTCGCGGA
  GGCCAAGGCCGACGTTTGCC
  TAGCGCGTAGTGGCCCACGC
  GCAGGCTGCTGCACGATCGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.438
  Max reward: 17.456
  With intrinsic bonuses: 14.439

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0325, kl_div=0.0000
    Epoch 1: policy_loss=0.0172, value_loss=0.9855, entropy=0.0331, kl_div=0.0225

=== Surrogate Model Training ===
Total samples: 5362

Training on 5227 samples (removed 135 outliers)
Reward range: [9.22, 19.31], mean: 14.38
  Created 13 candidate models for data size 5227
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.249 (std: 0.138)
  rf-tuned-xl: R2 = 0.245 (std: 0.137)
  gb-tuned-l: R2 = 0.279 (std: 0.134)
  gb-tuned-xl: R2 = 0.279 (std: 0.134)
  xgb-xl: R2 = 0.223 (std: 0.142)
  xgb-l: R2 = 0.223 (std: 0.142)
  mlp-adaptive-xl: R2 = 0.280 (std: 0.141)
  mlp-l: R2 = 0.277 (std: 0.143)
  svr-rbf-xl: R2 = 0.302 (std: 0.137)
  svr-poly-l: R2 = 0.302 (std: 0.137)
  knn-tuned-sqrt: R2 = 0.215 (std: 0.130)
  knn-tuned-l: R2 = 0.215 (std: 0.130)
  ridge: R2 = 0.179 (std: 0.146)

Model-based training with 12 models
Best R2: 0.302, Mean R2: 0.251
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=5730.8301, entropy=0.0317, kl_div=0.0000
    Epoch 1: policy_loss=-0.0126, value_loss=5730.8330, entropy=0.0315, kl_div=0.0004
  Round 1/5: Mean predicted reward = -35.027
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9857, entropy=0.0346, kl_div=0.0000
    Epoch 1: policy_loss=0.0004, value_loss=0.9857, entropy=0.0343, kl_div=-0.0125
  Round 2/5: Mean predicted reward = 14.774
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9857, entropy=0.0319, kl_div=0.0000
    Epoch 1: policy_loss=0.0003, value_loss=0.9856, entropy=0.0318, kl_div=-0.0096
  Round 3/5: Mean predicted reward = 14.743
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9858, entropy=0.0339, kl_div=0.0000
    Epoch 1: policy_loss=-0.0047, value_loss=0.9858, entropy=0.0342, kl_div=0.0093
  Round 4/5: Mean predicted reward = 14.715
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.080 knn-tuned-l:0.080 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9858, entropy=0.0318, kl_div=0.0000
    Epoch 1: policy_loss=-0.0148, value_loss=0.9858, entropy=0.0319, kl_div=0.0401
  Round 5/5: Mean predicted reward = 14.889

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 83 Results ---
  Mean Oracle Reward: 14.447
  Min Oracle Reward: 9.316
  Max Oracle Reward: 17.426
  Std Oracle Reward: 1.723
  Sequence Diversity: 0.766
  Models Used: 12
  Model R2 - Mean: 0.251, Max: 0.302, Count: 13
  Total Sequences Evaluated: 5362
    Oracle Count: 5312 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 84/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 5362

--- Round 84 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  CGGGACTCGCCGTTCCGAGA
  AAGCCCGGCCGTCGCGTATG
  GGCACGCCCGTGCATACTGG
  GCAGCAATCGTGCCTGGCGC
  CCCTCTAGGAGCGGGCGCAT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.558
  Max reward: 17.706
  With intrinsic bonuses: 14.536

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0324, kl_div=0.0000
    Epoch 1: policy_loss=-0.0221, value_loss=0.9854, entropy=0.0314, kl_div=-0.0004

=== Surrogate Model Training ===
Total samples: 5426

Training on 5289 samples (removed 137 outliers)
Reward range: [9.29, 19.31], mean: 14.39
  Created 13 candidate models for data size 5289
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.260 (std: 0.124)
  rf-tuned-xl: R2 = 0.258 (std: 0.127)
  gb-tuned-l: R2 = 0.283 (std: 0.127)
  gb-tuned-xl: R2 = 0.283 (std: 0.127)
  xgb-xl: R2 = 0.224 (std: 0.129)
  xgb-l: R2 = 0.224 (std: 0.129)
  mlp-adaptive-xl: R2 = 0.289 (std: 0.134)
  mlp-l: R2 = 0.284 (std: 0.141)
  svr-rbf-xl: R2 = 0.309 (std: 0.127)
  svr-poly-l: R2 = 0.309 (std: 0.127)
  knn-tuned-sqrt: R2 = 0.211 (std: 0.113)
  knn-tuned-l: R2 = 0.211 (std: 0.113)
  ridge: R2 = 0.187 (std: 0.140)

Model-based training with 12 models
Best R2: 0.309, Mean R2: 0.256
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.086 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=11571.6855, entropy=0.0270, kl_div=0.0000
    Epoch 1: policy_loss=0.0110, value_loss=11571.6904, entropy=0.0259, kl_div=-0.0233
  Round 1/5: Mean predicted reward = -51.220
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.086 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9858, entropy=0.0273, kl_div=0.0000
    Epoch 1: policy_loss=-0.0066, value_loss=0.9858, entropy=0.0273, kl_div=0.0024
  Round 2/5: Mean predicted reward = 14.827
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.086 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9857, entropy=0.0262, kl_div=0.0000
    Epoch 1: policy_loss=-0.0023, value_loss=0.9857, entropy=0.0267, kl_div=0.0173
  Round 3/5: Mean predicted reward = 14.836
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.086 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9856, entropy=0.0281, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0540
  Round 4/5: Mean predicted reward = 14.966
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.080 xgb-l:0.080 mlp-adaptive-xl:0.086 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9856, entropy=0.0294, kl_div=0.0000
    Epoch 1: policy_loss=-0.0096, value_loss=0.9856, entropy=0.0303, kl_div=0.0349
  Round 5/5: Mean predicted reward = 14.796

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 84 Results ---
  Mean Oracle Reward: 14.573
  Min Oracle Reward: 7.534
  Max Oracle Reward: 17.603
  Std Oracle Reward: 2.022
  Sequence Diversity: 0.750
  Models Used: 12
  Model R2 - Mean: 0.256, Max: 0.309, Count: 13
  Total Sequences Evaluated: 5426
    Oracle Count: 5376 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 85/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 5426
  Performance plateaued, reducing LR to 0.000150

--- Round 85 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  GCCGCTCCGCGGACGTGATA
  GGGCGACTTCCAGCAGTCCG
  CTTGGCCAGACGGAGTGCCC
  CCGACTGGCGTGTACACCGG
  TGGAGTCGGATCGACCGCCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.419
  Max reward: 17.703
  With intrinsic bonuses: 14.425

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0321, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0683

=== Surrogate Model Training ===
Total samples: 5490

Training on 5351 samples (removed 139 outliers)
Reward range: [9.29, 19.31], mean: 14.39
  Created 13 candidate models for data size 5351
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.255 (std: 0.124)
  rf-tuned-xl: R2 = 0.258 (std: 0.121)
  gb-tuned-l: R2 = 0.287 (std: 0.120)
  gb-tuned-xl: R2 = 0.287 (std: 0.120)
  xgb-xl: R2 = 0.231 (std: 0.129)
  xgb-l: R2 = 0.231 (std: 0.129)
  mlp-adaptive-xl: R2 = 0.282 (std: 0.130)
  mlp-l: R2 = 0.285 (std: 0.124)
  svr-rbf-xl: R2 = 0.311 (std: 0.120)
  svr-poly-l: R2 = 0.311 (std: 0.120)
  knn-tuned-sqrt: R2 = 0.210 (std: 0.105)
  knn-tuned-l: R2 = 0.210 (std: 0.105)
  ridge: R2 = 0.189 (std: 0.135)

Model-based training with 12 models
Best R2: 0.311, Mean R2: 0.258
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=6662.3486, entropy=0.0341, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1279
  Round 1/5: Mean predicted reward = -40.374
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9856, entropy=0.0389, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0942
  Round 2/5: Mean predicted reward = 14.612
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0391, kl_div=0.0000
    Epoch 1: policy_loss=-0.0099, value_loss=0.9855, entropy=0.0385, kl_div=0.0101
  Round 3/5: Mean predicted reward = 14.611
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0354, kl_div=0.0000
    Epoch 1: policy_loss=0.0297, value_loss=0.9855, entropy=0.0325, kl_div=0.0136
  Round 4/5: Mean predicted reward = 14.759
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.083 rf-tuned-xl:0.083 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9856, entropy=0.0275, kl_div=0.0000
    Epoch 1: policy_loss=-0.0320, value_loss=0.9856, entropy=0.0254, kl_div=-0.0071
  Round 5/5: Mean predicted reward = 14.846

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 85 Results ---
  Mean Oracle Reward: 14.417
  Min Oracle Reward: 9.025
  Max Oracle Reward: 17.634
  Std Oracle Reward: 1.881
  Sequence Diversity: 0.766
  Models Used: 12
  Model R2 - Mean: 0.258, Max: 0.311, Count: 13
  Total Sequences Evaluated: 5490
    Oracle Count: 5440 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 86/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 5490
  Performance plateaued, reducing LR to 0.000136

--- Round 86 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  GCTAATCTCGCGGGAGCCGC
  GCCATTAGGATCCCGCGGCG
  GCTTCGGCCCGACGGATGCA
  GAGCCTACTCCCTAGGGGGC
  GCGGACATCTTCGAGCCGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.513
  Max reward: 17.615
  With intrinsic bonuses: 14.486

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0254, kl_div=0.0000
    Epoch 1: policy_loss=-0.0087, value_loss=0.9854, entropy=0.0240, kl_div=0.0009

=== Surrogate Model Training ===
Total samples: 5554

Training on 5413 samples (removed 141 outliers)
Reward range: [9.29, 19.28], mean: 14.39
  Created 13 candidate models for data size 5413
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.262 (std: 0.120)
  rf-tuned-xl: R2 = 0.261 (std: 0.119)
  gb-tuned-l: R2 = 0.294 (std: 0.120)
  gb-tuned-xl: R2 = 0.294 (std: 0.120)
  xgb-xl: R2 = 0.251 (std: 0.123)
  xgb-l: R2 = 0.251 (std: 0.123)
  mlp-adaptive-xl: R2 = 0.291 (std: 0.119)
  mlp-l: R2 = 0.287 (std: 0.125)
  svr-rbf-xl: R2 = 0.320 (std: 0.119)
  svr-poly-l: R2 = 0.320 (std: 0.119)
  knn-tuned-sqrt: R2 = 0.218 (std: 0.096)
  knn-tuned-l: R2 = 0.218 (std: 0.096)
  ridge: R2 = 0.196 (std: 0.136)

Model-based training with 12 models
Best R2: 0.320, Mean R2: 0.266
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.082 xgb-l:0.082 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=11983.7500, entropy=0.0231, kl_div=0.0000
    Epoch 1: policy_loss=0.0070, value_loss=11983.7617, entropy=0.0222, kl_div=-0.0472
  Round 1/5: Mean predicted reward = -45.498
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.082 xgb-l:0.082 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0246, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0624
  Round 2/5: Mean predicted reward = 14.980
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.082 xgb-l:0.082 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9856, entropy=0.0250, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0871
  Round 3/5: Mean predicted reward = 14.910
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.082 xgb-l:0.082 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0243, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1352
  Round 4/5: Mean predicted reward = 14.753
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.082 gb-tuned-l:0.085 gb-tuned-xl:0.085 xgb-xl:0.082 xgb-l:0.082 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.087 svr-poly-l:0.087 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9855, entropy=0.0278, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1402
  Round 5/5: Mean predicted reward = 14.841

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 86 Results ---
  Mean Oracle Reward: 14.475
  Min Oracle Reward: 9.154
  Max Oracle Reward: 17.835
  Std Oracle Reward: 2.070
  Sequence Diversity: 0.734
  Models Used: 12
  Model R2 - Mean: 0.266, Max: 0.320, Count: 13
  Total Sequences Evaluated: 5554
    Oracle Count: 5504 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 87/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 5554
  Performance plateaued, reducing LR to 0.000100

--- Round 87 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  GCAGCCCGCTAGTGTAGGCC
  CATGAGGCCCGCTCGGCAGT
  TGCGAGGGAACCGTGCCTCC
  CAACGACTTCCGGTGCGCGG
  GAGGTCTCCGCCCAGGTCGA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.367
  Max reward: 19.033
  With intrinsic bonuses: 14.370

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9852, entropy=0.0298, kl_div=0.0000
    Epoch 1: policy_loss=0.0664, value_loss=0.9852, entropy=0.0312, kl_div=0.0472

=== Surrogate Model Training ===
Total samples: 5618

Training on 5476 samples (removed 142 outliers)
Reward range: [9.29, 19.28], mean: 14.39
  Created 13 candidate models for data size 5476
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.258 (std: 0.122)
  rf-tuned-xl: R2 = 0.259 (std: 0.123)
  gb-tuned-l: R2 = 0.294 (std: 0.119)
  gb-tuned-xl: R2 = 0.294 (std: 0.119)
  xgb-xl: R2 = 0.236 (std: 0.128)
  xgb-l: R2 = 0.236 (std: 0.128)
  mlp-adaptive-xl: R2 = 0.290 (std: 0.117)
  mlp-l: R2 = 0.283 (std: 0.121)
  svr-rbf-xl: R2 = 0.319 (std: 0.118)
  svr-poly-l: R2 = 0.319 (std: 0.118)
  knn-tuned-sqrt: R2 = 0.211 (std: 0.091)
  knn-tuned-l: R2 = 0.211 (std: 0.091)
  ridge: R2 = 0.197 (std: 0.137)

Model-based training with 12 models
Best R2: 0.319, Mean R2: 0.262
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.086 gb-tuned-xl:0.086 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.088 svr-poly-l:0.088 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6364.7935, entropy=0.0343, kl_div=0.0000
    Epoch 1: policy_loss=-0.0197, value_loss=6364.7988, entropy=0.0358, kl_div=0.0119
  Round 1/5: Mean predicted reward = -31.589
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.086 gb-tuned-xl:0.086 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.088 svr-poly-l:0.088 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9853, entropy=0.0400, kl_div=0.0000
    Epoch 1: policy_loss=0.0177, value_loss=0.9853, entropy=0.0412, kl_div=-0.0111
  Round 2/5: Mean predicted reward = 14.438
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.086 gb-tuned-xl:0.086 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.088 svr-poly-l:0.088 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9856, entropy=0.0381, kl_div=0.0000
    Epoch 1: policy_loss=-0.0158, value_loss=0.9856, entropy=0.0370, kl_div=-0.0511
  Round 3/5: Mean predicted reward = 14.626
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.086 gb-tuned-xl:0.086 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.088 svr-poly-l:0.088 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9855, entropy=0.0352, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=0.9855, entropy=0.0329, kl_div=-0.0568
  Round 4/5: Mean predicted reward = 14.760
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.082 rf-tuned-xl:0.083 gb-tuned-l:0.086 gb-tuned-xl:0.086 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.085 mlp-l:0.085 svr-rbf-xl:0.088 svr-poly-l:0.088 knn-tuned-sqrt:0.079 knn-tuned-l:0.079 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0270, kl_div=0.0000
    Epoch 1: policy_loss=0.0035, value_loss=0.9855, entropy=0.0252, kl_div=-0.0528
  Round 5/5: Mean predicted reward = 14.798

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 87 Results ---
  Mean Oracle Reward: 14.365
  Min Oracle Reward: 8.632
  Max Oracle Reward: 18.880
  Std Oracle Reward: 1.745
  Sequence Diversity: 0.766
  Models Used: 12
  Model R2 - Mean: 0.262, Max: 0.319, Count: 13
  Total Sequences Evaluated: 5618
    Oracle Count: 5568 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 88/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 5618
  Performance plateaued, reducing LR to 0.000055

--- Round 88 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.766) ---
  CCGGGACACCTATCGGCGTG
  CAGCTGCGCACTGGGGCACT
  CTCGCGGTGTCGCGCCAAGA
  CCGGATACACGCGCTGGGCT
  TGACCCGCGCCGAGATCTGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.321
  Max reward: 17.508
  With intrinsic bonuses: 14.344

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9852, entropy=0.0263, kl_div=0.0000
    Epoch 1: policy_loss=-0.0134, value_loss=0.9852, entropy=0.0257, kl_div=-0.0124

=== Surrogate Model Training ===
Total samples: 5682

Training on 5537 samples (removed 145 outliers)
Reward range: [9.29, 19.28], mean: 14.39
  Created 13 candidate models for data size 5537
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.262 (std: 0.119)
  rf-tuned-xl: R2 = 0.261 (std: 0.117)
  gb-tuned-l: R2 = 0.301 (std: 0.118)
  gb-tuned-xl: R2 = 0.301 (std: 0.118)
  xgb-xl: R2 = 0.244 (std: 0.121)
  xgb-l: R2 = 0.244 (std: 0.121)
  mlp-adaptive-xl: R2 = 0.291 (std: 0.124)
  mlp-l: R2 = 0.284 (std: 0.126)
  svr-rbf-xl: R2 = 0.320 (std: 0.121)
  svr-poly-l: R2 = 0.320 (std: 0.121)
  knn-tuned-sqrt: R2 = 0.208 (std: 0.094)
  knn-tuned-l: R2 = 0.208 (std: 0.094)
  ridge: R2 = 0.200 (std: 0.138)

Model-based training with 13 models
Best R2: 0.320, Mean R2: 0.265
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.077 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=10333.5068, entropy=0.0222, kl_div=0.0000
    Epoch 1: policy_loss=-0.0108, value_loss=10333.5098, entropy=0.0218, kl_div=-0.0052
  Round 1/5: Mean predicted reward = -54.724
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.077 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0222, kl_div=0.0000
    Epoch 1: policy_loss=0.0025, value_loss=0.9854, entropy=0.0218, kl_div=-0.0008
  Round 2/5: Mean predicted reward = 14.793
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.077 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0213, kl_div=0.0000
    Epoch 1: policy_loss=-0.0030, value_loss=0.9855, entropy=0.0211, kl_div=-0.0009
  Round 3/5: Mean predicted reward = 14.791
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.077 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=0.0242, kl_div=0.0000
    Epoch 1: policy_loss=-0.0182, value_loss=0.9854, entropy=0.0242, kl_div=0.0272
  Round 4/5: Mean predicted reward = 14.821
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.077 gb-tuned-l:0.080 gb-tuned-xl:0.080 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0220, kl_div=0.0000
    Epoch 1: policy_loss=-0.0049, value_loss=0.9854, entropy=0.0224, kl_div=0.0467
  Round 5/5: Mean predicted reward = 14.678

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 88 Results ---
  Mean Oracle Reward: 14.304
  Min Oracle Reward: 7.398
  Max Oracle Reward: 17.632
  Std Oracle Reward: 2.058
  Sequence Diversity: 0.766
  Models Used: 13
  Model R2 - Mean: 0.265, Max: 0.320, Count: 13
  Total Sequences Evaluated: 5682
    Oracle Count: 5632 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 89/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 5682
  Performance plateaued, reducing LR to 0.000019

--- Round 89 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  TAGGCGATCGCTGGGCCCAC
  AGGGCCATGCGCACGTCTCG
  GAGCATCAGTGCGGCCGTCC
  CCTACTGGCGCATGAGCGGC
  GGTCGATGCGCACCCCGAGT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.510
  Max reward: 18.369
  With intrinsic bonuses: 14.529

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9852, entropy=0.0232, kl_div=0.0000
    Epoch 1: policy_loss=-0.0008, value_loss=0.9852, entropy=0.0233, kl_div=0.0018

=== Surrogate Model Training ===
Total samples: 5746

Training on 5598 samples (removed 148 outliers)
Reward range: [9.30, 19.28], mean: 14.40
  Created 13 candidate models for data size 5598
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.262 (std: 0.114)
  rf-tuned-xl: R2 = 0.262 (std: 0.111)
  gb-tuned-l: R2 = 0.300 (std: 0.112)
  gb-tuned-xl: R2 = 0.300 (std: 0.112)
  xgb-xl: R2 = 0.242 (std: 0.114)
  xgb-l: R2 = 0.242 (std: 0.114)
  mlp-adaptive-xl: R2 = 0.298 (std: 0.119)
  mlp-l: R2 = 0.286 (std: 0.120)
  svr-rbf-xl: R2 = 0.324 (std: 0.115)
  svr-poly-l: R2 = 0.324 (std: 0.115)
  knn-tuned-sqrt: R2 = 0.218 (std: 0.103)
  knn-tuned-l: R2 = 0.218 (std: 0.103)
  ridge: R2 = 0.204 (std: 0.132)

Model-based training with 13 models
Best R2: 0.324, Mean R2: 0.268
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=11133.0908, entropy=0.0254, kl_div=0.0000
    Epoch 1: policy_loss=0.0017, value_loss=11133.0918, entropy=0.0254, kl_div=-0.0006
  Round 1/5: Mean predicted reward = -48.430
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9855, entropy=0.0249, kl_div=0.0000
    Epoch 1: policy_loss=-0.0015, value_loss=0.9855, entropy=0.0249, kl_div=-0.0012
  Round 2/5: Mean predicted reward = 14.876
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=0.0254, kl_div=0.0000
    Epoch 1: policy_loss=-0.0030, value_loss=0.9854, entropy=0.0252, kl_div=-0.0010
  Round 3/5: Mean predicted reward = 14.667
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=0.0254, kl_div=0.0000
    Epoch 1: policy_loss=-0.0028, value_loss=0.9854, entropy=0.0252, kl_div=0.0027
  Round 4/5: Mean predicted reward = 14.926
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=0.0236, kl_div=0.0000
    Epoch 1: policy_loss=-0.0005, value_loss=0.9854, entropy=0.0235, kl_div=0.0059
  Round 5/5: Mean predicted reward = 14.814

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 89 Results ---
  Mean Oracle Reward: 14.520
  Min Oracle Reward: 9.609
  Max Oracle Reward: 18.327
  Std Oracle Reward: 1.734
  Sequence Diversity: 0.734
  Models Used: 13
  Model R2 - Mean: 0.268, Max: 0.324, Count: 13
  Total Sequences Evaluated: 5746
    Oracle Count: 5696 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 90/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 5746
  Performance plateaued, reducing LR to 0.000150

--- Round 90 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  AGTGGCCCTGCCCTCGGAAG
  CCGGGGACGTCCTATGCCAG
  GCGGAGGGCCTTATCCACCG
  TAACGTCCGGCCGCGGTACG
  CCCTGCCCGGGTGCGTAAAG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.566
  Max reward: 18.788
  With intrinsic bonuses: 14.567

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9851, entropy=0.0240, kl_div=0.0000
    Epoch 1: policy_loss=-0.0491, value_loss=0.9851, entropy=0.0238, kl_div=0.0007

=== Surrogate Model Training ===
Total samples: 5810

Training on 5658 samples (removed 152 outliers)
Reward range: [9.32, 19.24], mean: 14.40
  Created 13 candidate models for data size 5658
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.256 (std: 0.123)
  rf-tuned-xl: R2 = 0.259 (std: 0.120)
  gb-tuned-l: R2 = 0.296 (std: 0.113)
  gb-tuned-xl: R2 = 0.296 (std: 0.113)
  xgb-xl: R2 = 0.235 (std: 0.125)
  xgb-l: R2 = 0.235 (std: 0.125)
  mlp-adaptive-xl: R2 = 0.285 (std: 0.123)
  mlp-l: R2 = 0.290 (std: 0.116)
  svr-rbf-xl: R2 = 0.322 (std: 0.118)
  svr-poly-l: R2 = 0.322 (std: 0.118)
  knn-tuned-sqrt: R2 = 0.215 (std: 0.108)
  knn-tuned-l: R2 = 0.215 (std: 0.108)
  ridge: R2 = 0.204 (std: 0.131)

Model-based training with 13 models
Best R2: 0.322, Mean R2: 0.264
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.077 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=24358.6836, entropy=0.0226, kl_div=0.0000
    Epoch 1: policy_loss=0.0004, value_loss=24358.7070, entropy=0.0217, kl_div=-0.0970
  Round 1/5: Mean predicted reward = -65.968
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.077 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9854, entropy=0.0200, kl_div=0.0000
    Epoch 1: policy_loss=-0.0098, value_loss=0.9854, entropy=0.0188, kl_div=0.0344
  Round 2/5: Mean predicted reward = 14.631
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.077 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9854, entropy=0.0186, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0980
  Round 3/5: Mean predicted reward = 14.771
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.077 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9853, entropy=0.0198, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1097
  Round 4/5: Mean predicted reward = 14.795
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.077 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9853, entropy=0.0183, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2006
  Round 5/5: Mean predicted reward = 14.692

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 90 Results ---
  Mean Oracle Reward: 14.548
  Min Oracle Reward: 9.973
  Max Oracle Reward: 18.854
  Std Oracle Reward: 1.730
  Sequence Diversity: 0.750
  Models Used: 13
  Model R2 - Mean: 0.264, Max: 0.322, Count: 13
  Total Sequences Evaluated: 5810
    Oracle Count: 5760 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}]

======================================================================
EXPERIMENT ROUND 91/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 5810
  Consistent improvement, increasing LR to 0.000327

--- Round 91 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  AGCGTAATGCCCCGCGCGTG
  CGGATCCGGTACGCGGCCAT
  TCGCTCCGCCTAGAGGGGAC
  GCCGGAAGGTCTCCCGTGCA
  CGCGTCAGAGTGGCCATGCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.459
  Max reward: 20.184
  With intrinsic bonuses: 14.430

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9850, entropy=0.0229, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1416

=== Surrogate Model Training ===
Total samples: 5874

Training on 5716 samples (removed 158 outliers)
Reward range: [9.35, 19.24], mean: 14.41
  Created 13 candidate models for data size 5716
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.252 (std: 0.125)
  rf-tuned-xl: R2 = 0.254 (std: 0.128)
  gb-tuned-l: R2 = 0.295 (std: 0.113)
  gb-tuned-xl: R2 = 0.295 (std: 0.113)
  xgb-xl: R2 = 0.237 (std: 0.117)
  xgb-l: R2 = 0.237 (std: 0.117)
  mlp-adaptive-xl: R2 = 0.278 (std: 0.125)
  mlp-l: R2 = 0.287 (std: 0.118)
  svr-rbf-xl: R2 = 0.319 (std: 0.126)
  svr-poly-l: R2 = 0.319 (std: 0.126)
  knn-tuned-sqrt: R2 = 0.218 (std: 0.112)
  knn-tuned-l: R2 = 0.218 (std: 0.112)
  ridge: R2 = 0.204 (std: 0.128)

Model-based training with 13 models
Best R2: 0.319, Mean R2: 0.263
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.073 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=7451.8149, entropy=0.0270, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1900
  Round 1/5: Mean predicted reward = -37.953
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9852, entropy=0.0350, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0731
  Round 2/5: Mean predicted reward = 14.544
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9851, entropy=0.0356, kl_div=0.0000
    Epoch 1: policy_loss=-0.0401, value_loss=0.9851, entropy=0.0333, kl_div=-0.0577
  Round 3/5: Mean predicted reward = 14.546
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9852, entropy=0.0277, kl_div=0.0000
    Epoch 1: policy_loss=0.0950, value_loss=0.9852, entropy=0.0261, kl_div=0.0118
  Round 4/5: Mean predicted reward = 14.499
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9851, entropy=0.0243, kl_div=0.0000
    Epoch 1: policy_loss=-0.0084, value_loss=0.9851, entropy=0.0234, kl_div=0.0042
  Round 5/5: Mean predicted reward = 14.750

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 91 Results ---
  Mean Oracle Reward: 14.420
  Min Oracle Reward: 8.827
  Max Oracle Reward: 20.273
  Std Oracle Reward: 1.694
  Sequence Diversity: 0.719
  Models Used: 13
  Model R2 - Mean: 0.263, Max: 0.319, Count: 13
  Total Sequences Evaluated: 5874
    Oracle Count: 5824 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 92/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 5874
  Performance plateaued, reducing LR to 0.000100

--- Round 92 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GTCTCAGGGGGCCCTCCAAG
  AATGCTTGCCCCGGGCAGGC
  CGCCTGACCGGTCAGTCAGG
  CGAGTCCGTCCGACGATGCG
  TCGTCAAATCGCCGCGCGGG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.111
  Max reward: 17.555
  With intrinsic bonuses: 14.144

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=0.0208, kl_div=0.0000
    Epoch 1: policy_loss=-0.0305, value_loss=0.9849, entropy=0.0203, kl_div=-0.0142

=== Surrogate Model Training ===
Total samples: 5938

Training on 5776 samples (removed 162 outliers)
Reward range: [9.38, 19.23], mean: 14.40
  Created 13 candidate models for data size 5776
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.256 (std: 0.125)
  rf-tuned-xl: R2 = 0.252 (std: 0.125)
  gb-tuned-l: R2 = 0.289 (std: 0.110)
  gb-tuned-xl: R2 = 0.289 (std: 0.110)
  xgb-xl: R2 = 0.233 (std: 0.120)
  xgb-l: R2 = 0.233 (std: 0.120)
  mlp-adaptive-xl: R2 = 0.285 (std: 0.122)
  mlp-l: R2 = 0.288 (std: 0.129)
  svr-rbf-xl: R2 = 0.319 (std: 0.124)
  svr-poly-l: R2 = 0.319 (std: 0.124)
  knn-tuned-sqrt: R2 = 0.214 (std: 0.115)
  knn-tuned-l: R2 = 0.214 (std: 0.115)
  ridge: R2 = 0.204 (std: 0.123)

Model-based training with 13 models
Best R2: 0.319, Mean R2: 0.261
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=9041.3223, entropy=0.0205, kl_div=0.0000
    Epoch 1: policy_loss=0.0004, value_loss=9041.3271, entropy=0.0207, kl_div=-0.0351
  Round 1/5: Mean predicted reward = -52.911
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9851, entropy=0.0211, kl_div=0.0000
    Epoch 1: policy_loss=0.0059, value_loss=0.9851, entropy=0.0209, kl_div=-0.0105
  Round 2/5: Mean predicted reward = 14.752
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9851, entropy=0.0205, kl_div=0.0000
    Epoch 1: policy_loss=-0.0113, value_loss=0.9851, entropy=0.0201, kl_div=0.0017
  Round 3/5: Mean predicted reward = 14.890
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9851, entropy=0.0193, kl_div=0.0000
    Epoch 1: policy_loss=-0.0159, value_loss=0.9851, entropy=0.0187, kl_div=0.0025
  Round 4/5: Mean predicted reward = 14.832
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.073 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9851, entropy=0.0183, kl_div=0.0000
    Epoch 1: policy_loss=-0.0133, value_loss=0.9851, entropy=0.0169, kl_div=0.0075
  Round 5/5: Mean predicted reward = 14.978

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 92 Results ---
  Mean Oracle Reward: 14.163
  Min Oracle Reward: 8.475
  Max Oracle Reward: 17.556
  Std Oracle Reward: 1.794
  Sequence Diversity: 0.719
  Models Used: 13
  Model R2 - Mean: 0.261, Max: 0.319, Count: 13
  Total Sequences Evaluated: 5938
    Oracle Count: 5888 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 93/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 5938

--- Round 93 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GTGCCTGAAGGCCTCGCCAG
  TCGGACCACACTCGGGGGCT
  GCTAAGTCCCGCATCGGGCG
  CTCCTTCCGGACGAGGAGGC
  GAGCCCCAACGGGCTTGCTG
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.901
  Max reward: 17.595
  With intrinsic bonuses: 14.843

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9850, entropy=0.0150, kl_div=0.0000
    Epoch 1: policy_loss=-0.0217, value_loss=0.9850, entropy=0.0124, kl_div=-0.0448

=== Surrogate Model Training ===
Total samples: 6002

Training on 5839 samples (removed 163 outliers)
Reward range: [9.38, 19.24], mean: 14.41
  Created 13 candidate models for data size 5839
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.256 (std: 0.126)
  rf-tuned-xl: R2 = 0.256 (std: 0.125)
  gb-tuned-l: R2 = 0.297 (std: 0.116)
  gb-tuned-xl: R2 = 0.297 (std: 0.116)
  xgb-xl: R2 = 0.241 (std: 0.129)
  xgb-l: R2 = 0.241 (std: 0.129)
  mlp-adaptive-xl: R2 = 0.288 (std: 0.137)
  mlp-l: R2 = 0.291 (std: 0.130)
  svr-rbf-xl: R2 = 0.323 (std: 0.132)
  svr-poly-l: R2 = 0.323 (std: 0.132)
  knn-tuned-sqrt: R2 = 0.221 (std: 0.123)
  knn-tuned-l: R2 = 0.221 (std: 0.123)
  ridge: R2 = 0.206 (std: 0.129)

Model-based training with 13 models
Best R2: 0.323, Mean R2: 0.266
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=21604.6582, entropy=0.0084, kl_div=0.0000
    Epoch 1: policy_loss=0.0119, value_loss=21604.6738, entropy=0.0073, kl_div=-0.0408
  Round 1/5: Mean predicted reward = -83.859
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9850, entropy=0.0067, kl_div=0.0000
    Epoch 1: policy_loss=-0.0066, value_loss=0.9850, entropy=0.0065, kl_div=-0.0101
  Round 2/5: Mean predicted reward = 14.772
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9850, entropy=0.0064, kl_div=0.0000
    Epoch 1: policy_loss=-0.0084, value_loss=0.9850, entropy=0.0066, kl_div=0.0410
  Round 3/5: Mean predicted reward = 14.754
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9850, entropy=0.0062, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1048
  Round 4/5: Mean predicted reward = 14.922
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9850, entropy=0.0071, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1360
  Round 5/5: Mean predicted reward = 14.871

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 93 Results ---
  Mean Oracle Reward: 14.913
  Min Oracle Reward: 4.343
  Max Oracle Reward: 17.787
  Std Oracle Reward: 2.431
  Sequence Diversity: 0.719
  Models Used: 13
  Model R2 - Mean: 0.266, Max: 0.323, Count: 13
  Total Sequences Evaluated: 6002
    Oracle Count: 5952 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 94/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 6002

--- Round 94 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.703) ---
  GGAAGACGTTCCTCCGCCGG
  GCACGGATCGTGTGCCGACC
  CTCAGGGCACGCAGCCGTTG
  CCGGATCGCCGGTAGCCTAG
  GTGGCTCAGCCCCGGATGAC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.866
  Max reward: 17.634
  With intrinsic bonuses: 14.900

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9850, entropy=0.0079, kl_div=0.0000
    Epoch 1: policy_loss=0.0017, value_loss=0.9849, entropy=0.0081, kl_div=0.0097

=== Surrogate Model Training ===
Total samples: 6066

Training on 5902 samples (removed 164 outliers)
Reward range: [9.38, 19.24], mean: 14.42
  Created 13 candidate models for data size 5902
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.261 (std: 0.135)
  rf-tuned-xl: R2 = 0.259 (std: 0.137)
  gb-tuned-l: R2 = 0.297 (std: 0.121)
  gb-tuned-xl: R2 = 0.297 (std: 0.121)
  xgb-xl: R2 = 0.242 (std: 0.127)
  xgb-l: R2 = 0.242 (std: 0.127)
  mlp-adaptive-xl: R2 = 0.294 (std: 0.133)
  mlp-l: R2 = 0.278 (std: 0.119)
  svr-rbf-xl: R2 = 0.326 (std: 0.137)
  svr-poly-l: R2 = 0.326 (std: 0.137)
  knn-tuned-sqrt: R2 = 0.223 (std: 0.138)
  knn-tuned-l: R2 = 0.223 (std: 0.138)
  ridge: R2 = 0.209 (std: 0.130)

Model-based training with 13 models
Best R2: 0.326, Mean R2: 0.267
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=49965.4336, entropy=0.0096, kl_div=0.0000
    Epoch 1: policy_loss=-0.0012, value_loss=49965.4375, entropy=0.0094, kl_div=-0.0050
  Round 1/5: Mean predicted reward = -85.688
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=0.0094, kl_div=0.0000
    Epoch 1: policy_loss=-0.0031, value_loss=0.9849, entropy=0.0089, kl_div=-0.0236
  Round 2/5: Mean predicted reward = 14.836
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9850, entropy=0.0071, kl_div=0.0000
    Epoch 1: policy_loss=-0.0034, value_loss=0.9850, entropy=0.0069, kl_div=0.0023
  Round 3/5: Mean predicted reward = 14.828
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=0.0062, kl_div=0.0000
    Epoch 1: policy_loss=-0.0033, value_loss=0.9849, entropy=0.0061, kl_div=0.0213
  Round 4/5: Mean predicted reward = 14.918
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.078 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9849, entropy=0.0070, kl_div=0.0000
    Epoch 1: policy_loss=-0.0053, value_loss=0.9849, entropy=0.0071, kl_div=0.0124
  Round 5/5: Mean predicted reward = 14.728

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 94 Results ---
  Mean Oracle Reward: 14.890
  Min Oracle Reward: 8.839
  Max Oracle Reward: 17.567
  Std Oracle Reward: 2.006
  Sequence Diversity: 0.703
  Models Used: 13
  Model R2 - Mean: 0.267, Max: 0.326, Count: 13
  Total Sequences Evaluated: 6066
    Oracle Count: 6016 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 95/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 6066

--- Round 95 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.688) ---
  TACCGGTGTCGAGCGCAGCC
  CGTGAGCCACTGAGGCGCTC
  CCTCGGGCTAAGCCGCGTAG
  GACGGCCGATTGGCATGCCC
  GGCCGCACTGACTCGGGCAT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 15.045
  Max reward: 17.827
  With intrinsic bonuses: 15.079

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9849, entropy=0.0065, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3158

=== Surrogate Model Training ===
Total samples: 6130

Training on 5965 samples (removed 165 outliers)
Reward range: [9.38, 19.24], mean: 14.43
  Created 13 candidate models for data size 5965
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.272 (std: 0.142)
  rf-tuned-xl: R2 = 0.266 (std: 0.144)
  gb-tuned-l: R2 = 0.302 (std: 0.129)
  gb-tuned-xl: R2 = 0.302 (std: 0.129)
  xgb-xl: R2 = 0.254 (std: 0.135)
  xgb-l: R2 = 0.254 (std: 0.135)
  mlp-adaptive-xl: R2 = 0.308 (std: 0.141)
  mlp-l: R2 = 0.300 (std: 0.145)
  svr-rbf-xl: R2 = 0.331 (std: 0.142)
  svr-poly-l: R2 = 0.331 (std: 0.142)
  knn-tuned-sqrt: R2 = 0.226 (std: 0.142)
  knn-tuned-l: R2 = 0.226 (std: 0.142)
  ridge: R2 = 0.211 (std: 0.138)

Model-based training with 13 models
Best R2: 0.331, Mean R2: 0.276
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=20695.7012, entropy=0.0090, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0866
  Round 1/5: Mean predicted reward = -89.646
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=0.0090, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1593
  Round 2/5: Mean predicted reward = 14.859
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9849, entropy=0.0086, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1862
  Round 3/5: Mean predicted reward = 14.916
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9849, entropy=0.0165, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1481
  Round 4/5: Mean predicted reward = 14.818
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.077 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9849, entropy=0.0183, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2392
  Round 5/5: Mean predicted reward = 14.824

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 95 Results ---
  Mean Oracle Reward: 15.036
  Min Oracle Reward: 8.083
  Max Oracle Reward: 17.674
  Std Oracle Reward: 2.191
  Sequence Diversity: 0.688
  Models Used: 13
  Model R2 - Mean: 0.276, Max: 0.331, Count: 13
  Total Sequences Evaluated: 6130
    Oracle Count: 6080 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 96/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 6130
  Performance plateaued, reducing LR to 0.000136

--- Round 96 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.703) ---
  CGTGCTCCCGGGTGACAGAC
  AGGCCTGCACCGTGACCTGG
  TCGACCGGGCCAGTATCGCG
  ACCGCTCGCGATCCTGGGAG
  GACTGCCATGGGTCGGACCC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.689
  Max reward: 17.900
  With intrinsic bonuses: 14.693

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9847, entropy=0.0236, kl_div=0.0000
    Epoch 1: policy_loss=0.0132, value_loss=0.9847, entropy=0.0240, kl_div=0.0341

=== Surrogate Model Training ===
Total samples: 6194

Training on 6028 samples (removed 166 outliers)
Reward range: [9.38, 19.24], mean: 14.43
  Created 13 candidate models for data size 6028
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.259 (std: 0.151)
  rf-tuned-xl: R2 = 0.260 (std: 0.153)
  gb-tuned-l: R2 = 0.297 (std: 0.135)
  gb-tuned-xl: R2 = 0.297 (std: 0.135)
  xgb-xl: R2 = 0.245 (std: 0.145)
  xgb-l: R2 = 0.245 (std: 0.145)
  mlp-adaptive-xl: R2 = 0.306 (std: 0.150)
  mlp-l: R2 = 0.291 (std: 0.151)
  svr-rbf-xl: R2 = 0.327 (std: 0.150)
  svr-poly-l: R2 = 0.327 (std: 0.150)
  knn-tuned-sqrt: R2 = 0.223 (std: 0.164)
  knn-tuned-l: R2 = 0.223 (std: 0.164)
  ridge: R2 = 0.211 (std: 0.140)

Model-based training with 13 models
Best R2: 0.327, Mean R2: 0.270
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.080 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=26615.5938, entropy=0.0233, kl_div=0.0000
    Epoch 1: policy_loss=-0.0313, value_loss=26615.6133, entropy=0.0236, kl_div=-0.0031
  Round 1/5: Mean predicted reward = -60.679
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.080 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9848, entropy=0.0239, kl_div=0.0000
    Epoch 1: policy_loss=0.0126, value_loss=0.9848, entropy=0.0233, kl_div=-0.0078
  Round 2/5: Mean predicted reward = 14.801
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.080 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9848, entropy=0.0175, kl_div=0.0000
    Epoch 1: policy_loss=-0.0314, value_loss=0.9847, entropy=0.0182, kl_div=-0.1872
  Round 3/5: Mean predicted reward = 14.738
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.080 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9848, entropy=0.0235, kl_div=0.0000
    Epoch 1: policy_loss=-0.0066, value_loss=0.9848, entropy=0.0232, kl_div=-0.0022
  Round 4/5: Mean predicted reward = 14.856
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.080 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9848, entropy=0.0206, kl_div=0.0000
    Epoch 1: policy_loss=-0.0246, value_loss=0.9848, entropy=0.0200, kl_div=0.0044
  Round 5/5: Mean predicted reward = 14.820

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 96 Results ---
  Mean Oracle Reward: 14.672
  Min Oracle Reward: 9.300
  Max Oracle Reward: 17.911
  Std Oracle Reward: 1.624
  Sequence Diversity: 0.703
  Models Used: 13
  Model R2 - Mean: 0.270, Max: 0.327, Count: 13
  Total Sequences Evaluated: 6194
    Oracle Count: 6144 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 96, 'cumulative_calls': 6144, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 97/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 6194

--- Round 97 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.703) ---
  GGCGCGAGCCACTCACGTGT
  CTGTCCTCGGGGGCCCGAAA
  CGCGGATGTTCGACGCCCAG
  GAGGCGCATTACCCCCGGTG
  GAGTCTCGCTGCCCCGGGAA
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.779
  Max reward: 17.731
  With intrinsic bonuses: 14.710

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9847, entropy=0.0189, kl_div=0.0000
    Epoch 1: policy_loss=0.0322, value_loss=0.9847, entropy=0.0150, kl_div=-0.0577

=== Surrogate Model Training ===
Total samples: 6258

Training on 6089 samples (removed 169 outliers)
Reward range: [9.39, 19.24], mean: 14.44
  Created 13 candidate models for data size 6089
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.266 (std: 0.155)
  rf-tuned-xl: R2 = 0.264 (std: 0.155)
  gb-tuned-l: R2 = 0.303 (std: 0.139)
  gb-tuned-xl: R2 = 0.303 (std: 0.139)
  xgb-xl: R2 = 0.247 (std: 0.141)
  xgb-l: R2 = 0.247 (std: 0.141)
  mlp-adaptive-xl: R2 = 0.294 (std: 0.147)
  mlp-l: R2 = 0.301 (std: 0.143)
  svr-rbf-xl: R2 = 0.331 (std: 0.146)
  svr-poly-l: R2 = 0.331 (std: 0.146)
  knn-tuned-sqrt: R2 = 0.229 (std: 0.161)
  knn-tuned-l: R2 = 0.229 (std: 0.161)
  ridge: R2 = 0.212 (std: 0.145)

Model-based training with 13 models
Best R2: 0.331, Mean R2: 0.273
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=59869.7031, entropy=0.0102, kl_div=0.0000
    Epoch 1: policy_loss=0.0206, value_loss=59869.7266, entropy=0.0078, kl_div=-0.0630
  Round 1/5: Mean predicted reward = -93.746
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9847, entropy=0.0070, kl_div=0.0000
    Epoch 1: policy_loss=-0.0109, value_loss=0.9847, entropy=0.0064, kl_div=0.0111
  Round 2/5: Mean predicted reward = 14.856
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0080, kl_div=0.0000
    Epoch 1: policy_loss=0.0046, value_loss=0.9847, entropy=0.0074, kl_div=0.0451
  Round 3/5: Mean predicted reward = 14.971
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0048, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0842
  Round 4/5: Mean predicted reward = 14.871
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.074 knn-tuned-l:0.074 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0049, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1123
  Round 5/5: Mean predicted reward = 14.745

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 97 Results ---
  Mean Oracle Reward: 14.786
  Min Oracle Reward: 9.978
  Max Oracle Reward: 17.638
  Std Oracle Reward: 1.954
  Sequence Diversity: 0.703
  Models Used: 13
  Model R2 - Mean: 0.273, Max: 0.331, Count: 13
  Total Sequences Evaluated: 6258
    Oracle Count: 6208 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 96, 'cumulative_calls': 6144, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 97, 'cumulative_calls': 6208, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 98/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 6258

--- Round 98 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  TGGGGACAGTCGCCAGCTCC
  GACCCGGCCCTTGGGGACTA
  CGGCATACCCGTCTCGGGGA
  CGCCTGGCTACAGGCCGGAT
  CGGGGACCCTTAATCCGGGC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 15.024
  Max reward: 18.006
  With intrinsic bonuses: 15.050

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9846, entropy=0.0105, kl_div=0.0000
    Epoch 1: policy_loss=0.0078, value_loss=0.9846, entropy=0.0107, kl_div=0.0153

=== Surrogate Model Training ===
Total samples: 6322

Training on 6154 samples (removed 168 outliers)
Reward range: [9.39, 19.24], mean: 14.44
  Created 13 candidate models for data size 6154
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.271 (std: 0.152)
  rf-tuned-xl: R2 = 0.271 (std: 0.152)
  gb-tuned-l: R2 = 0.309 (std: 0.138)
  gb-tuned-xl: R2 = 0.309 (std: 0.138)
  xgb-xl: R2 = 0.256 (std: 0.152)
  xgb-l: R2 = 0.256 (std: 0.152)
  mlp-adaptive-xl: R2 = 0.300 (std: 0.142)
  mlp-l: R2 = 0.303 (std: 0.145)
  svr-rbf-xl: R2 = 0.339 (std: 0.143)
  svr-poly-l: R2 = 0.339 (std: 0.143)
  knn-tuned-sqrt: R2 = 0.230 (std: 0.161)
  knn-tuned-l: R2 = 0.230 (std: 0.161)
  ridge: R2 = 0.218 (std: 0.145)

Model-based training with 13 models
Best R2: 0.339, Mean R2: 0.280
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=40546.4219, entropy=0.0079, kl_div=0.0000
    Epoch 1: policy_loss=0.0005, value_loss=40546.4336, entropy=0.0070, kl_div=-0.0259
  Round 1/5: Mean predicted reward = -98.651
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0069, kl_div=0.0000
    Epoch 1: policy_loss=-0.0131, value_loss=0.9847, entropy=0.0073, kl_div=0.0069
  Round 2/5: Mean predicted reward = 14.886
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0086, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0583
  Round 3/5: Mean predicted reward = 14.782
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9847, entropy=0.0106, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0915
  Round 4/5: Mean predicted reward = 15.003
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.078 mlp-l:0.079 svr-rbf-xl:0.082 svr-poly-l:0.082 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9847, entropy=0.0146, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0560
  Round 5/5: Mean predicted reward = 14.874

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 98 Results ---
  Mean Oracle Reward: 15.016
  Min Oracle Reward: 9.548
  Max Oracle Reward: 17.756
  Std Oracle Reward: 2.206
  Sequence Diversity: 0.734
  Models Used: 13
  Model R2 - Mean: 0.280, Max: 0.339, Count: 13
  Total Sequences Evaluated: 6322
    Oracle Count: 6272 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 96, 'cumulative_calls': 6144, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 97, 'cumulative_calls': 6208, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 98, 'cumulative_calls': 6272, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 99/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 6322
  Consistent improvement, increasing LR to 0.000045

--- Round 99 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.734) ---
  AGGGGTTCTGAGCCCCGCCA
  GGGCCGTGCACGGTCACCAT
  CCTTGGGGGCGTACACCACG
  AAGCCTCGCGCGTGGACTCG
  TAGCCCAGCGGCCGAGCGTT
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.668
  Max reward: 18.066
  With intrinsic bonuses: 14.697

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9846, entropy=0.0163, kl_div=0.0000
    Epoch 1: policy_loss=0.0004, value_loss=0.9846, entropy=0.0163, kl_div=0.0044

=== Surrogate Model Training ===
Total samples: 6386

Training on 6217 samples (removed 169 outliers)
Reward range: [9.39, 19.24], mean: 14.45
  Created 13 candidate models for data size 6217
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.280 (std: 0.157)
  rf-tuned-xl: R2 = 0.277 (std: 0.159)
  gb-tuned-l: R2 = 0.315 (std: 0.142)
  gb-tuned-xl: R2 = 0.315 (std: 0.142)
  xgb-xl: R2 = 0.271 (std: 0.156)
  xgb-l: R2 = 0.271 (std: 0.156)
  mlp-adaptive-xl: R2 = 0.309 (std: 0.156)
  mlp-l: R2 = 0.312 (std: 0.153)
  svr-rbf-xl: R2 = 0.343 (std: 0.145)
  svr-poly-l: R2 = 0.343 (std: 0.145)
  knn-tuned-sqrt: R2 = 0.239 (std: 0.167)
  knn-tuned-l: R2 = 0.239 (std: 0.167)
  ridge: R2 = 0.221 (std: 0.147)

Model-based training with 13 models
Best R2: 0.343, Mean R2: 0.287
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=29101.3125, entropy=0.0157, kl_div=0.0000
    Epoch 1: policy_loss=0.0036, value_loss=29101.3145, entropy=0.0149, kl_div=-0.0160
  Round 1/5: Mean predicted reward = -95.663
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9846, entropy=0.0144, kl_div=0.0000
    Epoch 1: policy_loss=-0.0052, value_loss=0.9846, entropy=0.0136, kl_div=-0.0088
  Round 2/5: Mean predicted reward = 14.893
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0121, kl_div=0.0000
    Epoch 1: policy_loss=-0.0014, value_loss=0.9846, entropy=0.0110, kl_div=-0.0128
  Round 3/5: Mean predicted reward = 14.859
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0106, kl_div=0.0000
    Epoch 1: policy_loss=-0.0079, value_loss=0.9846, entropy=0.0106, kl_div=-0.0038
  Round 4/5: Mean predicted reward = 14.938
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.079 mlp-l:0.079 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9846, entropy=0.0128, kl_div=0.0000
    Epoch 1: policy_loss=-0.0035, value_loss=0.9846, entropy=0.0139, kl_div=0.0360
  Round 5/5: Mean predicted reward = 14.929

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 99 Results ---
  Mean Oracle Reward: 14.685
  Min Oracle Reward: 6.944
  Max Oracle Reward: 17.902
  Std Oracle Reward: 2.233
  Sequence Diversity: 0.734
  Models Used: 13
  Model R2 - Mean: 0.287, Max: 0.343, Count: 13
  Total Sequences Evaluated: 6386
    Oracle Count: 6336 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 96, 'cumulative_calls': 6144, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 97, 'cumulative_calls': 6208, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 98, 'cumulative_calls': 6272, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 99, 'cumulative_calls': 6336, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
EXPERIMENT ROUND 100/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 6386

--- Round 100 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.703) ---
  GCCTCTGACCAGCGTGAGGC
  GGCTAGCGTGCTGGCAACCC
  GGAGGCCTCGCGTCACATGC
  GCTGACGCCCAGGCTGACTG
  TAAGGGACCCCTCGGGCGTC
  ... (64 total)

Oracle Evaluation:
  Mean reward: 14.889
  Max reward: 18.102
  With intrinsic bonuses: 14.883

Policy Update:
  Adaptive update: clip_ratio=0.10, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0120, kl_div=0.0000
    Epoch 1: policy_loss=-0.0002, value_loss=0.9846, entropy=0.0115, kl_div=-0.0553

=== Surrogate Model Training ===
Total samples: 6450

Training on 6280 samples (removed 170 outliers)
Reward range: [9.39, 19.24], mean: 14.45
  Created 13 candidate models for data size 6280
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.285 (std: 0.158)
  rf-tuned-xl: R2 = 0.283 (std: 0.158)
  gb-tuned-l: R2 = 0.319 (std: 0.140)
  gb-tuned-xl: R2 = 0.319 (std: 0.140)
  xgb-xl: R2 = 0.273 (std: 0.151)
  xgb-l: R2 = 0.273 (std: 0.151)
  mlp-adaptive-xl: R2 = 0.318 (std: 0.148)
  mlp-l: R2 = 0.328 (std: 0.157)
  svr-rbf-xl: R2 = 0.347 (std: 0.142)
  svr-poly-l: R2 = 0.347 (std: 0.142)
  knn-tuned-sqrt: R2 = 0.244 (std: 0.162)
  knn-tuned-l: R2 = 0.244 (std: 0.162)
  ridge: R2 = 0.225 (std: 0.146)

Model-based training with 13 models
Best R2: 0.347, Mean R2: 0.293
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=46570.6328, entropy=0.0086, kl_div=0.0000
    Epoch 1: policy_loss=0.0159, value_loss=46570.6758, entropy=0.0056, kl_div=0.0406
  Round 1/5: Mean predicted reward = -102.055
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0026, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0590
  Round 2/5: Mean predicted reward = 14.977
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0050, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1338
  Round 3/5: Mean predicted reward = 14.906
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9846, entropy=0.0071, kl_div=0.0000
    Epoch 1: policy_loss=-0.0034, value_loss=0.9846, entropy=0.0073, kl_div=0.0422
  Round 4/5: Mean predicted reward = 14.879
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.076 rf-tuned-xl:0.076 gb-tuned-l:0.079 gb-tuned-xl:0.079 xgb-xl:0.075 xgb-l:0.075 mlp-adaptive-xl:0.079 mlp-l:0.080 svr-rbf-xl:0.081 svr-poly-l:0.081 knn-tuned-sqrt:0.073 knn-tuned-l:0.073 ridge:0.072 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9846, entropy=0.0045, kl_div=0.0000
    Epoch 1: policy_loss=-0.0119, value_loss=0.9846, entropy=0.0047, kl_div=0.0379
  Round 5/5: Mean predicted reward = 14.992

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 100 Results ---
  Mean Oracle Reward: 14.894
  Min Oracle Reward: 7.263
  Max Oracle Reward: 17.896
  Std Oracle Reward: 2.283
  Sequence Diversity: 0.703
  Models Used: 13
  Model R2 - Mean: 0.293, Max: 0.347, Count: 13
  Total Sequences Evaluated: 6450
    Oracle Count: 6400 [{'round': 1, 'cumulative_calls': 64, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 2, 'cumulative_calls': 128, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 3, 'cumulative_calls': 192, 'new_calls': 64, 'best_reward_so_far': 17.878697618156487}, {'round': 4, 'cumulative_calls': 256, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 5, 'cumulative_calls': 320, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 6, 'cumulative_calls': 384, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 7, 'cumulative_calls': 448, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 8, 'cumulative_calls': 512, 'new_calls': 64, 'best_reward_so_far': 18.60122847203668}, {'round': 9, 'cumulative_calls': 576, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 10, 'cumulative_calls': 640, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 11, 'cumulative_calls': 704, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 12, 'cumulative_calls': 768, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 13, 'cumulative_calls': 832, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 14, 'cumulative_calls': 896, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 15, 'cumulative_calls': 960, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 16, 'cumulative_calls': 1024, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 17, 'cumulative_calls': 1088, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 18, 'cumulative_calls': 1152, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 19, 'cumulative_calls': 1216, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 20, 'cumulative_calls': 1280, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 21, 'cumulative_calls': 1344, 'new_calls': 64, 'best_reward_so_far': 19.28250012960932}, {'round': 22, 'cumulative_calls': 1408, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 23, 'cumulative_calls': 1472, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 24, 'cumulative_calls': 1536, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 25, 'cumulative_calls': 1600, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 26, 'cumulative_calls': 1664, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 27, 'cumulative_calls': 1728, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 28, 'cumulative_calls': 1792, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 29, 'cumulative_calls': 1856, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 30, 'cumulative_calls': 1920, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 31, 'cumulative_calls': 1984, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 32, 'cumulative_calls': 2048, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 33, 'cumulative_calls': 2112, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 34, 'cumulative_calls': 2176, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 35, 'cumulative_calls': 2240, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 36, 'cumulative_calls': 2304, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 37, 'cumulative_calls': 2368, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 38, 'cumulative_calls': 2432, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 39, 'cumulative_calls': 2496, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 40, 'cumulative_calls': 2560, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 41, 'cumulative_calls': 2624, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 42, 'cumulative_calls': 2688, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 43, 'cumulative_calls': 2752, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 44, 'cumulative_calls': 2816, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 45, 'cumulative_calls': 2880, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 46, 'cumulative_calls': 2944, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 47, 'cumulative_calls': 3008, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 48, 'cumulative_calls': 3072, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 49, 'cumulative_calls': 3136, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 50, 'cumulative_calls': 3200, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 51, 'cumulative_calls': 3264, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 52, 'cumulative_calls': 3328, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 53, 'cumulative_calls': 3392, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 54, 'cumulative_calls': 3456, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 55, 'cumulative_calls': 3520, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 56, 'cumulative_calls': 3584, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 57, 'cumulative_calls': 3648, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 58, 'cumulative_calls': 3712, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 59, 'cumulative_calls': 3776, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 60, 'cumulative_calls': 3840, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 61, 'cumulative_calls': 3904, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 62, 'cumulative_calls': 3968, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 63, 'cumulative_calls': 4032, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 64, 'cumulative_calls': 4096, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 65, 'cumulative_calls': 4160, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 66, 'cumulative_calls': 4224, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 67, 'cumulative_calls': 4288, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 68, 'cumulative_calls': 4352, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 69, 'cumulative_calls': 4416, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 70, 'cumulative_calls': 4480, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 71, 'cumulative_calls': 4544, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 72, 'cumulative_calls': 4608, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 73, 'cumulative_calls': 4672, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 74, 'cumulative_calls': 4736, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 75, 'cumulative_calls': 4800, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 76, 'cumulative_calls': 4864, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 77, 'cumulative_calls': 4928, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 78, 'cumulative_calls': 4992, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 79, 'cumulative_calls': 5056, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 80, 'cumulative_calls': 5120, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 81, 'cumulative_calls': 5184, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 82, 'cumulative_calls': 5248, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 83, 'cumulative_calls': 5312, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 84, 'cumulative_calls': 5376, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 85, 'cumulative_calls': 5440, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 86, 'cumulative_calls': 5504, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 87, 'cumulative_calls': 5568, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 88, 'cumulative_calls': 5632, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 89, 'cumulative_calls': 5696, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 90, 'cumulative_calls': 5760, 'new_calls': 64, 'best_reward_so_far': 20.21559898139278}, {'round': 91, 'cumulative_calls': 5824, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 92, 'cumulative_calls': 5888, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 93, 'cumulative_calls': 5952, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 94, 'cumulative_calls': 6016, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 95, 'cumulative_calls': 6080, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 96, 'cumulative_calls': 6144, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 97, 'cumulative_calls': 6208, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 98, 'cumulative_calls': 6272, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 99, 'cumulative_calls': 6336, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}, {'round': 100, 'cumulative_calls': 6400, 'new_calls': 64, 'best_reward_so_far': 20.336098346531912}]

======================================================================
DyNA PPO ALGORITHM COMPLETE! Time used 6404.48 seconds
======================================================================
Total rounds executed: 100
Total sequences evaluated: 6450
Best mean reward: 15.297 (achieved at round 70)

==================================================
TRAINING SUMMARY
==================================================
Total Rounds: 100
Final Mean Reward: 14.8945
Best Mean Reward: 15.2972
Best Max Reward: 20.2733
Initial Lr: 0.0003
Final Lr: 0.0003
Total Updates: 630
Final Diversity: 0.7031
Convergence Round: 5
==================================================

Generating learning curves...
Learning curves saved to experiments/20250924191508_r100_b64_l20_weighted_dynamic/metrices_plot_weighted_r100_b64_l20.png
Saving training metrics...
Metrics saved to experiments/20250924191508_r100_b64_l20_weighted_dynamic/metrices_data_weighted_r100_b64_l20.json

======================================================================
FINAL OPTIMIZED SEQUENCES
======================================================================

Deterministic (Exploitation):
  GGGGGCGGCCGGCCGGCCGG: 17.393
  GGGGGCGGCCGGCCGGCCGG: 17.404
  GGGGGCGGCCGGCCGGCCGG: 17.277
  GGGGGCGGCCGGCCGGCCGG: 17.324
  GGGGGCGGCCGGCCGGCCGG: 17.217

Stochastic (Exploration):
  GGGGGCGGCCGGCCGGCCGG: 17.438
  GGGGGCGGCCGGCCGGCCGG: 16.981
  GGGGGCGGCCGGCCGGCCGG: 17.489
  GGGGGCGGCCGGCCGGCCGG: 17.420
  GGGGGCGGCCGGCCGGCCGG: 17.280

Final Performance:
  Mean reward: 17.322
  Max reward: 17.489
  Std reward: 0.139

Best sequence found: GGGGGCGGCCGGCCGGCCGG
   Reward: 17.489

======================================================================
Training complete! Check 'learning_curves.png' and 'training_metrics.json'
======================================================================

=== Model Weight Evolution Analysis ===
Model evolution plot saved to experiments/20250924191508_r100_b64_l20_weighted_dynamic/model_evolution_r100_b64_l20.png

=== Model Performance Summary ===

Final weight distribution (Round 100):
  svr: 0.811
  mlp: 0.792
  gb: 0.789
  rf: 0.762
  xgb: 0.754
  knn: 0.732
  ridge: 0.359

Overall model importance (average weight across all rounds):
  svr: 0.781
  gb: 0.606
  mlp: 0.536
  rf: 0.534
  knn: 0.372
  xgb: 0.364
  ridge: 0.167
Detailed performance data saved to model_performance_details.csv
