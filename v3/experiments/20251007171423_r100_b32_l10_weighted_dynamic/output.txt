======================================================================
RUNNING IMPROVED DyNA PPO WITH BETTER SURROGATE LEARNING
======================================================================
======================================================================
IMPROVED DyNA PPO ALGORITHM
======================================================================
Configuration:
  Number of experiment rounds N = 100
  Number of model-based training rounds M = 5
  Minimum model score τ = 0.2
  Batch size B = 32
  Warm-up phase: True
  Surrogate model method: weighted
======================================================================

=== WARM-UP PHASE ===
Generating 50 warm-up samples...
Warm-up statistics:
  Mean reward: 7.272
  Std reward: 2.751
  Min/Max: 0.000 / 10.204

Pre-training surrogate models on warm-up data...

Training on 45 samples (removed 5 outliers)
Reward range: [3.14, 10.20], mean: 8.01
  Created 8 candidate models for data size 45
Current R2 threshold: -0.3
  rf-xs: R2 = -0.753 (std: 0.682)
  rf-s: R2 = -0.582 (std: 0.529)
  knn-xs: R2 = -0.285 (std: 0.358)
  knn-s: R2 = -0.285 (std: 0.358)
  ridge: R2 = -0.260 (std: 0.193)
  gb-xs: R2 = -1.109 (std: 0.744)
  gp: R2 = -33.876 (std: 20.133)
  svr-rbf-s: R2 = -0.052 (std: 0.326)
Initial models trained: 4
Initial R2 scores - Mean: -4.650, Max: -0.052

======================================================================
EXPERIMENT ROUND 1/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.300
Total data collected: 50

--- Round 1 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  TACAGTAGGG
  GTGAAGCCTA
  GAGTTTGTTT
  AGTAAGTTTG
  GCACGTCTGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.311
  Max reward: 11.136
  With intrinsic bonuses: 8.274

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=0.0000, value_loss=0.9710, entropy=1.3845, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1042

=== Surrogate Model Training ===
Total samples: 82

Training on 76 samples (removed 6 outliers)
Reward range: [3.60, 10.86], mean: 8.18
  Created 8 candidate models for data size 76
Current R2 threshold: -0.3
  rf-xs: R2 = -0.316 (std: 0.208)
  rf-s: R2 = -0.208 (std: 0.161)
  knn-xs: R2 = -0.164 (std: 0.212)
  knn-s: R2 = -0.164 (std: 0.212)
  ridge: R2 = -0.075 (std: 0.048)
  gb-xs: R2 = -0.764 (std: 0.518)
  gp: R2 = -41.674 (std: 16.706)
  svr-rbf-s: R2 = -0.060 (std: 0.060)

Model-based training with 5 models
Best R2: -0.060, Mean R2: -5.428
Running 2 virtual training rounds
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9759, entropy=1.3822, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.299
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9735, entropy=1.3779, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.332

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 1 Results ---
  Mean Oracle Reward: 8.299
  Min Oracle Reward: 5.668
  Max Oracle Reward: 10.966
  Std Oracle Reward: 1.223
  Sequence Diversity: 1.000
  Models Used: 5
  Model R2 - Mean: -5.428, Max: -0.060, Count: 8
  New best mean reward!
  Total Sequences Evaluated: 82
    Oracle Count: 32 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}]

======================================================================
EXPERIMENT ROUND 2/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.300
Total data collected: 82

--- Round 2 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  GGGTGACGTC
  GACGTAGTTC
  TAACCGGGGT
  CTCGTGCCCA
  AATGACCGCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.343
  Max reward: 10.101
  With intrinsic bonuses: 8.336

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=-0.0000, value_loss=0.9800, entropy=1.3707, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0717

=== Surrogate Model Training ===
Total samples: 114

Training on 107 samples (removed 7 outliers)
Reward range: [4.70, 10.86], mean: 8.27
  Created 11 candidate models for data size 107
Current R2 threshold: -0.3
  rf-m: R2 = -0.051 (std: 0.314)
  rf-l: R2 = -0.103 (std: 0.412)
  gb-m: R2 = -0.389 (std: 0.593)
  gb-l: R2 = -0.404 (std: 0.609)
  xgb-m: R2 = -0.553 (std: 0.541)
  knn-m: R2 = -0.223 (std: 0.465)
  knn-tuned: R2 = -0.223 (std: 0.465)
  mlp-m: R2 = -1.362 (std: 0.876)
  svr-rbf: R2 = -0.062 (std: 0.177)
  svr-poly: R2 = -0.062 (std: 0.177)
  ridge: R2 = -0.093 (std: 0.119)

Model-based training with 7 models
Best R2: -0.051, Mean R2: -0.320
Running 2 virtual training rounds
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9644, entropy=1.3657, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.293
Current Method: weighted
    Using uniform weights (insufficient data)
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9669, entropy=1.3549, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.356

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 2 Results ---
  Mean Oracle Reward: 8.344
  Min Oracle Reward: 5.042
  Max Oracle Reward: 9.890
  Std Oracle Reward: 1.092
  Sequence Diversity: 1.000
  Models Used: 7
  Model R2 - Mean: -0.320, Max: -0.051, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 114
    Oracle Count: 64 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}]

======================================================================
EXPERIMENT ROUND 3/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.300
Total data collected: 114

--- Round 3 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0200
Exploration rate: 0.300

--- Generated Sequences (Diversity: 1.000) ---
  CGGGTTGTGT
  ACTTGGGGGG
  TTAGCTTATT
  TGTAACTCCG
  GCATTGCTTC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.834
  Max reward: 10.863
  With intrinsic bonuses: 8.901

Policy Update:
  Adaptive update: clip_ratio=0.30, entropy_coef=0.020
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=1.3461, kl_div=0.0000
    Epoch 2: policy_loss=-0.0388, value_loss=0.9686, entropy=1.3412, kl_div=0.0116
    Epoch 4: policy_loss=-0.1052, value_loss=0.9685, entropy=1.3411, kl_div=0.0138
    Epoch 6: policy_loss=-0.1535, value_loss=0.9684, entropy=1.3438, kl_div=0.0059

=== Surrogate Model Training ===
Total samples: 146

Training on 137 samples (removed 9 outliers)
Reward range: [4.70, 11.22], mean: 8.45
  Created 11 candidate models for data size 137
Current R2 threshold: -0.3
  rf-m: R2 = -0.096 (std: 0.331)
  rf-l: R2 = -0.104 (std: 0.328)
  gb-m: R2 = -0.237 (std: 0.256)
  gb-l: R2 = -0.250 (std: 0.267)
  xgb-m: R2 = -0.148 (std: 0.393)
  knn-m: R2 = -0.203 (std: 0.416)
  knn-tuned: R2 = -0.203 (std: 0.416)
  mlp-m: R2 = -1.422 (std: 1.014)
  svr-rbf: R2 = -0.085 (std: 0.208)
  svr-poly: R2 = -0.085 (std: 0.208)
  ridge: R2 = -0.251 (std: 0.333)

Model-based training with 10 models
Best R2: -0.085, Mean R2: -0.280
Running 2 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.163 rf-l:0.152 gb-m:0.040 gb-l:0.035 xgb-m:0.097 knn-m:0.056 knn-tuned:0.056 svr-rbf:0.183 svr-poly:0.183 ridge:0.035 
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9664, entropy=1.3485, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.689
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.163 rf-l:0.152 gb-m:0.040 gb-l:0.035 xgb-m:0.097 knn-m:0.056 knn-tuned:0.056 svr-rbf:0.183 svr-poly:0.183 ridge:0.035 
  Adaptive update: clip_ratio=0.30, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9680, entropy=1.3485, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.577

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 3 Results ---
  Mean Oracle Reward: 8.805
  Min Oracle Reward: 3.691
  Max Oracle Reward: 11.234
  Std Oracle Reward: 1.646
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.280, Max: -0.085, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 146
    Oracle Count: 96 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}]

======================================================================
EXPERIMENT ROUND 4/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.280
Total data collected: 146
  Consistent improvement, increasing LR to 0.000045

--- Round 4 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0100
Exploration rate: 0.280

--- Generated Sequences (Diversity: 1.000) ---
  GCTCACATGG
  CATCTGAGGA
  GGCTATCGCA
  GACCTTGGAC
  GAGATTCCGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.218
  Max reward: 12.160
  With intrinsic bonuses: 9.350

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9703, entropy=1.3470, kl_div=0.0000
    Epoch 1: policy_loss=-0.0034, value_loss=0.9703, entropy=1.3470, kl_div=-0.0002
    Epoch 2: policy_loss=-0.0092, value_loss=0.9703, entropy=1.3471, kl_div=-0.0001
    Epoch 3: policy_loss=-0.0169, value_loss=0.9702, entropy=1.3472, kl_div=0.0003

=== Surrogate Model Training ===
Total samples: 178

Training on 164 samples (removed 14 outliers)
Reward range: [5.70, 11.22], mean: 8.67
  Created 11 candidate models for data size 164
Current R2 threshold: -0.3
  rf-m: R2 = -0.222 (std: 0.388)
  rf-l: R2 = -0.218 (std: 0.317)
  gb-m: R2 = -0.360 (std: 0.468)
  gb-l: R2 = -0.381 (std: 0.478)
  xgb-m: R2 = -0.529 (std: 0.630)
  knn-m: R2 = -0.294 (std: 0.380)
  knn-tuned: R2 = -0.294 (std: 0.380)
  mlp-m: R2 = -2.158 (std: 2.666)
  svr-rbf: R2 = -0.202 (std: 0.345)
  svr-poly: R2 = -0.202 (std: 0.345)
  ridge: R2 = -0.226 (std: 0.285)

Model-based training with 7 models
Best R2: -0.202, Mean R2: -0.462
Running 2 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.155 rf-l:0.162 knn-m:0.076 knn-tuned:0.076 svr-rbf:0.191 svr-poly:0.191 ridge:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=1.3491, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.881
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.155 rf-l:0.162 knn-m:0.076 knn-tuned:0.076 svr-rbf:0.191 svr-poly:0.191 ridge:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9678, entropy=1.3485, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.869

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 4 Results ---
  Mean Oracle Reward: 9.229
  Min Oracle Reward: 7.864
  Max Oracle Reward: 12.362
  Std Oracle Reward: 0.954
  Sequence Diversity: 1.000
  Models Used: 7
  Model R2 - Mean: -0.462, Max: -0.202, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 178
    Oracle Count: 128 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}]

======================================================================
EXPERIMENT ROUND 5/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.250
Total data collected: 178
  Consistent improvement, increasing LR to 0.000360

--- Round 5 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0100
Exploration rate: 0.250

--- Generated Sequences (Diversity: 1.000) ---
  TGCGACTGAC
  AACTCGGGTA
  GACTATACGG
  ACCTTGAGGA
  TGCAGCTGCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.926
  Max reward: 11.038
  With intrinsic bonuses: 9.017

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=-0.0000, value_loss=0.9703, entropy=1.3468, kl_div=0.0000
    Epoch 1: policy_loss=-0.0430, value_loss=0.9702, entropy=1.3483, kl_div=-0.0034
    Epoch 2: policy_loss=-0.0874, value_loss=0.9701, entropy=1.3497, kl_div=-0.0016
    Epoch 3: policy_loss=-0.1123, value_loss=0.9700, entropy=1.3504, kl_div=0.0050

=== Surrogate Model Training ===
Total samples: 210

Training on 196 samples (removed 14 outliers)
Reward range: [5.70, 11.22], mean: 8.71
  Created 11 candidate models for data size 196
Current R2 threshold: -0.3
  rf-m: R2 = -0.094 (std: 0.289)
  rf-l: R2 = -0.073 (std: 0.275)
  gb-m: R2 = -0.227 (std: 0.476)
  gb-l: R2 = -0.234 (std: 0.475)
  xgb-m: R2 = -0.140 (std: 0.228)
  knn-m: R2 = -0.132 (std: 0.347)
  knn-tuned: R2 = -0.132 (std: 0.347)
  mlp-m: R2 = -1.384 (std: 0.982)
  svr-rbf: R2 = -0.136 (std: 0.252)
  svr-poly: R2 = -0.136 (std: 0.252)
  ridge: R2 = -0.190 (std: 0.197)

Model-based training with 10 models
Best R2: -0.073, Mean R2: -0.262
Running 2 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.155 rf-l:0.191 gb-m:0.041 gb-l:0.038 xgb-m:0.098 knn-m:0.107 knn-tuned:0.107 svr-rbf:0.102 svr-poly:0.102 ridge:0.060 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9676, entropy=1.3478, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.856
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.155 rf-l:0.191 gb-m:0.041 gb-l:0.038 xgb-m:0.098 knn-m:0.107 knn-tuned:0.107 svr-rbf:0.102 svr-poly:0.102 ridge:0.060 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=1.3495, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.857

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 5 Results ---
  Mean Oracle Reward: 8.921
  Min Oracle Reward: 6.021
  Max Oracle Reward: 11.229
  Std Oracle Reward: 1.284
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.262, Max: -0.073, Count: 11
  Total Sequences Evaluated: 210
    Oracle Count: 160 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}]

======================================================================
EXPERIMENT ROUND 6/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.220
Total data collected: 210

--- Round 6 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0100
Exploration rate: 0.220

--- Generated Sequences (Diversity: 1.000) ---
  GATCTCGAGA
  ACGCGACTGT
  GTGCAATACG
  TGGCACATGA
  CCAGTAGTGA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.203
  Max reward: 12.436
  With intrinsic bonuses: 9.207

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9671, entropy=1.3459, kl_div=0.0000
    Epoch 1: policy_loss=-0.0355, value_loss=0.9671, entropy=1.3434, kl_div=0.0150
    Epoch 2: policy_loss=-0.0697, value_loss=0.9671, entropy=1.3399, kl_div=0.0360
    Early stopping at epoch 3: KL divergence = 0.0509

=== Surrogate Model Training ===
Total samples: 242

Training on 227 samples (removed 15 outliers)
Reward range: [5.70, 11.22], mean: 8.75
  Created 11 candidate models for data size 227
Current R2 threshold: -0.3
  rf-m: R2 = -0.048 (std: 0.158)
  rf-l: R2 = -0.046 (std: 0.181)
  gb-m: R2 = -0.092 (std: 0.224)
  gb-l: R2 = -0.093 (std: 0.221)
  xgb-m: R2 = -0.116 (std: 0.195)
  knn-m: R2 = -0.021 (std: 0.193)
  knn-tuned: R2 = -0.021 (std: 0.193)
  mlp-m: R2 = -0.983 (std: 1.021)
  svr-rbf: R2 = -0.061 (std: 0.165)
  svr-poly: R2 = -0.061 (std: 0.165)
  ridge: R2 = -0.149 (std: 0.136)

Model-based training with 10 models
Best R2: -0.021, Mean R2: -0.154
Running 2 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.117 rf-l:0.119 gb-m:0.075 gb-l:0.075 xgb-m:0.059 knn-m:0.154 knn-tuned:0.154 svr-rbf:0.102 svr-poly:0.102 ridge:0.043 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9714, entropy=1.3424, kl_div=0.0000
  Round 1/2: Mean predicted reward = 8.942
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.117 rf-l:0.119 gb-m:0.075 gb-l:0.075 xgb-m:0.059 knn-m:0.154 knn-tuned:0.154 svr-rbf:0.102 svr-poly:0.102 ridge:0.043 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=1.3393, kl_div=0.0000
  Round 2/2: Mean predicted reward = 8.972

  === Progress Analysis ===
  Status: WARNING
  • R2 scores negative. Models struggling to learn. Try collecting more diverse data.

--- Round 6 Results ---
  Mean Oracle Reward: 9.127
  Min Oracle Reward: 5.885
  Max Oracle Reward: 11.924
  Std Oracle Reward: 1.167
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.154, Max: -0.021, Count: 11
  Total Sequences Evaluated: 242
    Oracle Count: 192 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 7/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.190
Total data collected: 242

--- Round 7 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0100
Exploration rate: 0.190

--- Generated Sequences (Diversity: 1.000) ---
  GCTGCCATGA
  AGTAGAGTCC
  AGGGCTAATC
  GAGATTCAGC
  TGGCCATAAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.045
  Max reward: 10.920
  With intrinsic bonuses: 9.126

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.010
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=1.3310, kl_div=0.0000
    Epoch 1: policy_loss=-0.0384, value_loss=0.9688, entropy=1.3264, kl_div=0.0319
    Early stopping at epoch 2: KL divergence = 0.0745

=== Surrogate Model Training ===
Total samples: 274

Training on 257 samples (removed 17 outliers)
Reward range: [5.73, 11.22], mean: 8.81
  Created 11 candidate models for data size 257
Current R2 threshold: -0.3
  rf-m: R2 = 0.011 (std: 0.166)
  rf-l: R2 = 0.041 (std: 0.149)
  gb-m: R2 = 0.054 (std: 0.222)
  gb-l: R2 = 0.058 (std: 0.224)
  xgb-m: R2 = 0.020 (std: 0.169)
  knn-m: R2 = 0.045 (std: 0.154)
  knn-tuned: R2 = 0.045 (std: 0.154)
  mlp-m: R2 = -0.559 (std: 0.389)
  svr-rbf: R2 = 0.003 (std: 0.142)
  svr-poly: R2 = 0.003 (std: 0.142)
  ridge: R2 = -0.185 (std: 0.084)

Model-based training with 10 models
Best R2: 0.058, Mean R2: -0.042
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.088 rf-l:0.119 gb-m:0.135 gb-l:0.140 xgb-m:0.096 knn-m:0.123 knn-tuned:0.123 svr-rbf:0.081 svr-poly:0.081 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9672, entropy=1.3207, kl_div=0.0000
    Epoch 1: policy_loss=-0.0406, value_loss=0.9672, entropy=1.3134, kl_div=0.0476
  Round 1/3: Mean predicted reward = 8.817
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.088 rf-l:0.119 gb-m:0.135 gb-l:0.140 xgb-m:0.096 knn-m:0.123 knn-tuned:0.123 svr-rbf:0.081 svr-poly:0.081 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9682, entropy=1.3021, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0521
  Round 2/3: Mean predicted reward = 8.886
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.088 rf-l:0.119 gb-m:0.135 gb-l:0.140 xgb-m:0.096 knn-m:0.123 knn-tuned:0.123 svr-rbf:0.081 svr-poly:0.081 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9665, entropy=1.3036, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0526
  Round 3/3: Mean predicted reward = 8.752

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 7 Results ---
  Mean Oracle Reward: 8.992
  Min Oracle Reward: 4.095
  Max Oracle Reward: 11.102
  Std Oracle Reward: 1.306
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.042, Max: 0.058, Count: 11
  Total Sequences Evaluated: 274
    Oracle Count: 224 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 8/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.080
Total data collected: 274

--- Round 8 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.080

--- Generated Sequences (Diversity: 1.000) ---
  ATAGCGTAGC
  AGTCAGATCG
  GCCGATTAAG
  CTAGCATAGG
  GTCCAACTGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.304
  Max reward: 11.291
  With intrinsic bonuses: 9.340

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9718, entropy=1.2936, kl_div=0.0000
    Epoch 1: policy_loss=-0.0156, value_loss=0.9718, entropy=1.2896, kl_div=0.0208

=== Surrogate Model Training ===
Total samples: 306

Training on 288 samples (removed 18 outliers)
Reward range: [6.00, 11.22], mean: 8.87
  Created 11 candidate models for data size 288
Current R2 threshold: -0.3
  rf-m: R2 = 0.041 (std: 0.207)
  rf-l: R2 = 0.012 (std: 0.225)
  gb-m: R2 = 0.044 (std: 0.097)
  gb-l: R2 = 0.045 (std: 0.097)
  xgb-m: R2 = -0.022 (std: 0.235)
  knn-m: R2 = 0.019 (std: 0.179)
  knn-tuned: R2 = 0.019 (std: 0.179)
  mlp-m: R2 = -0.476 (std: 0.492)
  svr-rbf: R2 = 0.022 (std: 0.134)
  svr-poly: R2 = 0.022 (std: 0.134)
  ridge: R2 = -0.192 (std: 0.123)

Model-based training with 10 models
Best R2: 0.045, Mean R2: -0.042
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.130 rf-l:0.097 gb-m:0.134 gb-l:0.135 xgb-m:0.069 knn-m:0.105 knn-tuned:0.105 svr-rbf:0.107 svr-poly:0.107 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=1.2908, kl_div=0.0000
    Epoch 1: policy_loss=-0.0226, value_loss=0.9685, entropy=1.2865, kl_div=0.0272
  Round 1/3: Mean predicted reward = 8.902
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.130 rf-l:0.097 gb-m:0.134 gb-l:0.135 xgb-m:0.069 knn-m:0.105 knn-tuned:0.105 svr-rbf:0.107 svr-poly:0.107 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9719, entropy=1.2693, kl_div=0.0000
    Epoch 1: policy_loss=-0.0394, value_loss=0.9719, entropy=1.2625, kl_div=0.0356
  Round 2/3: Mean predicted reward = 8.951
Current Method: weighted
    Using performance-based weights
    Model weights: rf-m:0.130 rf-l:0.097 gb-m:0.134 gb-l:0.135 xgb-m:0.069 knn-m:0.105 knn-tuned:0.105 svr-rbf:0.107 svr-poly:0.107 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9668, entropy=1.2577, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0565
  Round 3/3: Mean predicted reward = 8.937

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 8 Results ---
  Mean Oracle Reward: 9.303
  Min Oracle Reward: 6.418
  Max Oracle Reward: 11.340
  Std Oracle Reward: 1.126
  Sequence Diversity: 1.000
  Models Used: 10
  Model R2 - Mean: -0.042, Max: 0.045, Count: 11
  New best mean reward!
  Total Sequences Evaluated: 306
    Oracle Count: 256 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 9/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.065
Total data collected: 306

--- Round 9 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.065

--- Generated Sequences (Diversity: 1.000) ---
  GTGCAGATCC
  CGTGCAATAG
  GACAGCTCGT
  GTAGATACGC
  GCTGCAGATC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.229
  Max reward: 11.214
  With intrinsic bonuses: 9.221

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9700, entropy=1.2495, kl_div=0.0000
    Epoch 1: policy_loss=-0.0134, value_loss=0.9700, entropy=1.2466, kl_div=0.0150

=== Surrogate Model Training ===
Total samples: 338

Training on 317 samples (removed 21 outliers)
Reward range: [6.24, 11.30], mean: 8.94
  Created 14 candidate models for data size 317
Current R2 threshold: -0.3
  rf-tuned-l: R2 = -0.016 (std: 0.132)
  rf-tuned-xl: R2 = -0.015 (std: 0.125)
  gb-tuned-l: R2 = -0.034 (std: 0.176)
  gb-tuned-xl: R2 = -0.042 (std: 0.171)
  xgb-xl: R2 = -0.039 (std: 0.147)
  xgb-l: R2 = -0.039 (std: 0.147)
  mlp-adaptive-xl: R2 = -0.563 (std: 0.288)
  mlp-l: R2 = -0.667 (std: 0.369)
  svr-rbf-xl: R2 = 0.027 (std: 0.155)
  svr-poly-l: R2 = 0.027 (std: 0.155)
  knn-tuned-sqrt: R2 = -0.058 (std: 0.080)
  knn-tuned-l: R2 = -0.058 (std: 0.080)
  ridge: R2 = -0.212 (std: 0.103)
  gp: R2 = -81.253 (std: 19.490)

Model-based training with 11 models
Best R2: 0.027, Mean R2: -5.924
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.104 gb-tuned-l:0.085 gb-tuned-xl:0.079 xgb-xl:0.082 xgb-l:0.082 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.067 knn-tuned-l:0.067 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9663, entropy=1.2454, kl_div=0.0000
    Epoch 1: policy_loss=-0.0203, value_loss=0.9663, entropy=1.2421, kl_div=0.0216
  Round 1/3: Mean predicted reward = 8.919
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.104 gb-tuned-l:0.085 gb-tuned-xl:0.079 xgb-xl:0.082 xgb-l:0.082 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.067 knn-tuned-l:0.067 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=1.2440, kl_div=0.0000
    Epoch 1: policy_loss=-0.0155, value_loss=0.9689, entropy=1.2401, kl_div=0.0349
  Round 2/3: Mean predicted reward = 9.000
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.104 gb-tuned-l:0.085 gb-tuned-xl:0.079 xgb-xl:0.082 xgb-l:0.082 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.067 knn-tuned-l:0.067 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9705, entropy=1.2347, kl_div=0.0000
    Epoch 1: policy_loss=-0.0116, value_loss=0.9705, entropy=1.2303, kl_div=0.0350
  Round 3/3: Mean predicted reward = 9.014

  === Progress Analysis ===
  Status: NORMAL

--- Round 9 Results ---
  Mean Oracle Reward: 9.231
  Min Oracle Reward: 5.479
  Max Oracle Reward: 11.226
  Std Oracle Reward: 0.974
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -5.924, Max: 0.027, Count: 14
  Total Sequences Evaluated: 338
    Oracle Count: 288 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 10/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 338

--- Round 10 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GATACCGTGA
  CAAATGTGGC
  GTCACAAGTG
  ATATGCGGAC
  CTGACGAAGT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.267
  Max reward: 11.015
  With intrinsic bonuses: 9.286

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9723, entropy=1.2277, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2711

=== Surrogate Model Training ===
Total samples: 370

Training on 349 samples (removed 21 outliers)
Reward range: [6.24, 11.30], mean: 8.97
  Created 14 candidate models for data size 349
Current R2 threshold: -0.3
  rf-tuned-l: R2 = 0.048 (std: 0.085)
  rf-tuned-xl: R2 = 0.021 (std: 0.112)
  gb-tuned-l: R2 = 0.004 (std: 0.175)
  gb-tuned-xl: R2 = 0.002 (std: 0.175)
  xgb-xl: R2 = -0.040 (std: 0.225)
  xgb-l: R2 = -0.040 (std: 0.225)
  mlp-adaptive-xl: R2 = -0.435 (std: 0.413)
  mlp-l: R2 = -0.308 (std: 0.154)
  svr-rbf-xl: R2 = 0.046 (std: 0.091)
  svr-poly-l: R2 = 0.046 (std: 0.091)
  knn-tuned-sqrt: R2 = -0.026 (std: 0.144)
  knn-tuned-l: R2 = -0.026 (std: 0.144)
  ridge: R2 = -0.169 (std: 0.123)
  gp: R2 = -83.643 (std: 20.681)

Model-based training with 11 models
Best R2: 0.048, Mean R2: -6.037
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.110 gb-tuned-l:0.093 gb-tuned-xl:0.092 xgb-xl:0.060 xgb-l:0.060 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9723, entropy=1.1794, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4703
  Round 1/3: Mean predicted reward = 8.969
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.110 gb-tuned-l:0.093 gb-tuned-xl:0.092 xgb-xl:0.060 xgb-l:0.060 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=1.1442, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5008
  Round 2/3: Mean predicted reward = 9.007
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.110 gb-tuned-l:0.093 gb-tuned-xl:0.092 xgb-xl:0.060 xgb-l:0.060 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.069 knn-tuned-l:0.069 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=1.1026, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6715
  Round 3/3: Mean predicted reward = 8.963

  === Progress Analysis ===
  Status: NORMAL

--- Round 10 Results ---
  Mean Oracle Reward: 9.251
  Min Oracle Reward: 7.496
  Max Oracle Reward: 10.750
  Std Oracle Reward: 0.767
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -6.037, Max: 0.048, Count: 14
  Total Sequences Evaluated: 370
    Oracle Count: 320 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 11/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 370
  Performance plateaued, reducing LR to 0.000136

--- Round 11 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GATATGCCCG
  GACGGTAACT
  GAACTTGCGA
  CTGAAAGCTG
  CGACGAGTAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.130
  Max reward: 10.893
  With intrinsic bonuses: 9.119

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9667, entropy=1.0472, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2396

=== Surrogate Model Training ===
Total samples: 402

Training on 381 samples (removed 21 outliers)
Reward range: [6.24, 11.30], mean: 8.98
  Created 14 candidate models for data size 381
Current R2 threshold: -0.29
  rf-tuned-l: R2 = 0.056 (std: 0.057)
  rf-tuned-xl: R2 = 0.047 (std: 0.052)
  gb-tuned-l: R2 = 0.106 (std: 0.065)
  gb-tuned-xl: R2 = 0.106 (std: 0.065)
  xgb-xl: R2 = -0.001 (std: 0.085)
  xgb-l: R2 = -0.001 (std: 0.085)
  mlp-adaptive-xl: R2 = -0.317 (std: 0.318)
  mlp-l: R2 = -0.347 (std: 0.186)
  svr-rbf-xl: R2 = 0.073 (std: 0.084)
  svr-poly-l: R2 = 0.073 (std: 0.084)
  knn-tuned-sqrt: R2 = -0.029 (std: 0.126)
  knn-tuned-l: R2 = -0.029 (std: 0.126)
  ridge: R2 = -0.154 (std: 0.125)
  gp: R2 = -84.370 (std: 19.847)

Model-based training with 11 models
Best R2: 0.106, Mean R2: -6.056
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.095 gb-tuned-l:0.170 gb-tuned-xl:0.170 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.123 svr-poly-l:0.123 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=1.0304, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2428
  Round 1/3: Mean predicted reward = 8.956
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.095 gb-tuned-l:0.170 gb-tuned-xl:0.170 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.123 svr-poly-l:0.123 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9718, entropy=1.0095, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2293
  Round 2/3: Mean predicted reward = 8.966
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.095 gb-tuned-l:0.170 gb-tuned-xl:0.170 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.123 svr-poly-l:0.123 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9681, entropy=0.9931, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2261
  Round 3/3: Mean predicted reward = 8.758

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 11 Results ---
  Mean Oracle Reward: 9.118
  Min Oracle Reward: 6.852
  Max Oracle Reward: 10.953
  Std Oracle Reward: 0.922
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -6.056, Max: 0.106, Count: 14
  Total Sequences Evaluated: 402
    Oracle Count: 352 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 12/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 402
  Performance plateaued, reducing LR to 0.000100

--- Round 12 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ACCACGGTTG
  CACAGGCTGT
  ACGTATCCGG
  TGAATGAGCC
  GAGCAACTTG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.759
  Max reward: 11.013
  With intrinsic bonuses: 8.776

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9751, entropy=0.9790, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1755

=== Surrogate Model Training ===
Total samples: 434

Training on 412 samples (removed 22 outliers)
Reward range: [6.24, 11.30], mean: 8.98
  Created 14 candidate models for data size 412
Current R2 threshold: -0.27999999999999997
  rf-tuned-l: R2 = 0.078 (std: 0.100)
  rf-tuned-xl: R2 = 0.102 (std: 0.113)
  gb-tuned-l: R2 = 0.055 (std: 0.072)
  gb-tuned-xl: R2 = 0.057 (std: 0.071)
  xgb-xl: R2 = 0.069 (std: 0.111)
  xgb-l: R2 = 0.069 (std: 0.111)
  mlp-adaptive-xl: R2 = -0.334 (std: 0.310)
  mlp-l: R2 = -0.362 (std: 0.335)
  svr-rbf-xl: R2 = 0.085 (std: 0.068)
  svr-poly-l: R2 = 0.085 (std: 0.068)
  knn-tuned-sqrt: R2 = 0.014 (std: 0.175)
  knn-tuned-l: R2 = 0.014 (std: 0.175)
  ridge: R2 = -0.134 (std: 0.154)
  gp: R2 = -83.313 (std: 15.028)

Model-based training with 11 models
Best R2: 0.102, Mean R2: -5.965
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.140 gb-tuned-l:0.088 gb-tuned-xl:0.089 xgb-xl:0.101 xgb-l:0.101 svr-rbf-xl:0.119 svr-poly-l:0.119 knn-tuned-sqrt:0.059 knn-tuned-l:0.059 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9697, entropy=0.9599, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2022
  Round 1/3: Mean predicted reward = 8.907
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.140 gb-tuned-l:0.088 gb-tuned-xl:0.089 xgb-xl:0.101 xgb-l:0.101 svr-rbf-xl:0.119 svr-poly-l:0.119 knn-tuned-sqrt:0.059 knn-tuned-l:0.059 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.9368, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2428
  Round 2/3: Mean predicted reward = 8.990
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.140 gb-tuned-l:0.088 gb-tuned-xl:0.089 xgb-xl:0.101 xgb-l:0.101 svr-rbf-xl:0.119 svr-poly-l:0.119 knn-tuned-sqrt:0.059 knn-tuned-l:0.059 ridge:0.013 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.9552, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1954
  Round 3/3: Mean predicted reward = 8.806

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 12 Results ---
  Mean Oracle Reward: 8.781
  Min Oracle Reward: 3.618
  Max Oracle Reward: 11.036
  Std Oracle Reward: 1.306
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -5.965, Max: 0.102, Count: 14
  Total Sequences Evaluated: 434
    Oracle Count: 384 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}]

======================================================================
EXPERIMENT ROUND 13/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 434

--- Round 13 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ATACGTAGGC
  GTAGACACGT
  ATCGCGGCTA
  GCGATACTGC
  TTGGGCCACA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.096
  Max reward: 13.285
  With intrinsic bonuses: 9.113

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9722, entropy=0.9204, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2020

=== Surrogate Model Training ===
Total samples: 466

Training on 441 samples (removed 25 outliers)
Reward range: [6.37, 11.30], mean: 8.99
  Created 14 candidate models for data size 441
Current R2 threshold: -0.27
  rf-tuned-l: R2 = 0.025 (std: 0.084)
  rf-tuned-xl: R2 = 0.018 (std: 0.088)
  gb-tuned-l: R2 = -0.039 (std: 0.111)
  gb-tuned-xl: R2 = -0.038 (std: 0.112)
  xgb-xl: R2 = -0.039 (std: 0.090)
  xgb-l: R2 = -0.039 (std: 0.090)
  mlp-adaptive-xl: R2 = -0.267 (std: 0.111)
  mlp-l: R2 = -0.341 (std: 0.247)
  svr-rbf-xl: R2 = 0.018 (std: 0.087)
  svr-poly-l: R2 = 0.018 (std: 0.087)
  knn-tuned-sqrt: R2 = -0.088 (std: 0.207)
  knn-tuned-l: R2 = -0.088 (std: 0.207)
  ridge: R2 = -0.128 (std: 0.148)
  gp: R2 = -84.793 (std: 17.443)

Model-based training with 12 models
Best R2: 0.025, Mean R2: -6.127
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.136 gb-tuned-l:0.077 gb-tuned-xl:0.078 xgb-xl:0.077 xgb-l:0.077 mlp-adaptive-xl:0.008 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.047 knn-tuned-l:0.047 ridge:0.032 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9675, entropy=0.9137, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1295
  Round 1/3: Mean predicted reward = 8.881
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.136 gb-tuned-l:0.077 gb-tuned-xl:0.078 xgb-xl:0.077 xgb-l:0.077 mlp-adaptive-xl:0.008 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.047 knn-tuned-l:0.047 ridge:0.032 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9682, entropy=0.9181, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1380
  Round 2/3: Mean predicted reward = 9.002
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.136 gb-tuned-l:0.077 gb-tuned-xl:0.078 xgb-xl:0.077 xgb-l:0.077 mlp-adaptive-xl:0.008 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.047 knn-tuned-l:0.047 ridge:0.032 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9717, entropy=0.8867, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2002
  Round 3/3: Mean predicted reward = 9.047

  === Progress Analysis ===
  Status: NORMAL

--- Round 13 Results ---
  Mean Oracle Reward: 9.137
  Min Oracle Reward: 5.309
  Max Oracle Reward: 13.444
  Std Oracle Reward: 1.442
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: -6.127, Max: 0.025, Count: 14
  Total Sequences Evaluated: 466
    Oracle Count: 416 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}]

======================================================================
EXPERIMENT ROUND 14/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 466

--- Round 14 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AAGTCGGCAT
  AGATCCTGAG
  CTCTGCGAGA
  GAGCTAGCTC
  TCAGCCGATG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.925
  Max reward: 10.852
  With intrinsic bonuses: 8.906

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9700, entropy=0.8852, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0737

=== Surrogate Model Training ===
Total samples: 498

Training on 472 samples (removed 26 outliers)
Reward range: [6.24, 11.30], mean: 8.99
  Created 14 candidate models for data size 472
Current R2 threshold: -0.26
  rf-tuned-l: R2 = 0.027 (std: 0.057)
  rf-tuned-xl: R2 = 0.015 (std: 0.070)
  gb-tuned-l: R2 = -0.006 (std: 0.086)
  gb-tuned-xl: R2 = -0.008 (std: 0.085)
  xgb-xl: R2 = -0.063 (std: 0.137)
  xgb-l: R2 = -0.063 (std: 0.137)
  mlp-adaptive-xl: R2 = -0.189 (std: 0.176)
  mlp-l: R2 = -0.204 (std: 0.172)
  svr-rbf-xl: R2 = 0.024 (std: 0.097)
  svr-poly-l: R2 = 0.024 (std: 0.097)
  knn-tuned-sqrt: R2 = -0.133 (std: 0.136)
  knn-tuned-l: R2 = -0.133 (std: 0.136)
  ridge: R2 = -0.146 (std: 0.153)
  gp: R2 = -82.331 (std: 18.086)

Model-based training with 13 models
Best R2: 0.027, Mean R2: -5.942
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.129 gb-tuned-l:0.105 gb-tuned-xl:0.103 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.017 mlp-l:0.014 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.029 knn-tuned-l:0.029 ridge:0.026 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9698, entropy=0.8961, kl_div=0.0000
    Epoch 1: policy_loss=0.0008, value_loss=0.9698, entropy=0.8931, kl_div=0.0485
  Round 1/3: Mean predicted reward = 8.921
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.129 gb-tuned-l:0.105 gb-tuned-xl:0.103 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.017 mlp-l:0.014 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.029 knn-tuned-l:0.029 ridge:0.026 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9700, entropy=0.8700, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0513
  Round 2/3: Mean predicted reward = 8.979
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.129 gb-tuned-l:0.105 gb-tuned-xl:0.103 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.017 mlp-l:0.014 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.029 knn-tuned-l:0.029 ridge:0.026 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9721, entropy=0.8689, kl_div=0.0000
    Epoch 1: policy_loss=-0.0044, value_loss=0.9721, entropy=0.8670, kl_div=0.0363
  Round 3/3: Mean predicted reward = 8.967

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 14 Results ---
  Mean Oracle Reward: 8.924
  Min Oracle Reward: 4.791
  Max Oracle Reward: 11.049
  Std Oracle Reward: 1.470
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -5.942, Max: 0.027, Count: 14
  Total Sequences Evaluated: 498
    Oracle Count: 448 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}]

======================================================================
EXPERIMENT ROUND 15/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 498

--- Round 15 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TGATCGCGAC
  GTGCGAACAT
  CGCCTATAGG
  TGGGATCCCA
  TTGACGCAAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.026
  Max reward: 11.660
  With intrinsic bonuses: 9.049

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.8698, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0761

=== Surrogate Model Training ===
Total samples: 530

Training on 504 samples (removed 26 outliers)
Reward range: [6.15, 11.30], mean: 8.99
  Created 13 candidate models for data size 504
Current R2 threshold: -0.25
  rf-tuned-l: R2 = 0.062 (std: 0.052)
  rf-tuned-xl: R2 = 0.080 (std: 0.078)
  gb-tuned-l: R2 = 0.026 (std: 0.092)
  gb-tuned-xl: R2 = 0.026 (std: 0.092)
  xgb-xl: R2 = -0.026 (std: 0.106)
  xgb-l: R2 = -0.026 (std: 0.106)
  mlp-adaptive-xl: R2 = -0.337 (std: 0.252)
  mlp-l: R2 = -0.343 (std: 0.198)
  svr-rbf-xl: R2 = 0.066 (std: 0.104)
  svr-poly-l: R2 = 0.066 (std: 0.104)
  knn-tuned-sqrt: R2 = -0.070 (std: 0.139)
  knn-tuned-l: R2 = -0.070 (std: 0.139)
  ridge: R2 = -0.136 (std: 0.162)

Model-based training with 11 models
Best R2: 0.080, Mean R2: -0.052
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 rf-tuned-xl:0.167 gb-tuned-l:0.097 gb-tuned-xl:0.097 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.145 svr-poly-l:0.145 knn-tuned-sqrt:0.037 knn-tuned-l:0.037 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9678, entropy=0.8607, kl_div=0.0000
    Epoch 1: policy_loss=-0.0641, value_loss=0.9678, entropy=0.8631, kl_div=-0.0850
  Round 1/3: Mean predicted reward = 8.757
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 rf-tuned-xl:0.167 gb-tuned-l:0.097 gb-tuned-xl:0.097 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.145 svr-poly-l:0.145 knn-tuned-sqrt:0.037 knn-tuned-l:0.037 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9709, entropy=0.8670, kl_div=0.0000
    Epoch 1: policy_loss=-0.0440, value_loss=0.9709, entropy=0.8673, kl_div=-0.0573
  Round 2/3: Mean predicted reward = 9.000
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.139 rf-tuned-xl:0.167 gb-tuned-l:0.097 gb-tuned-xl:0.097 xgb-xl:0.058 xgb-l:0.058 svr-rbf-xl:0.145 svr-poly-l:0.145 knn-tuned-sqrt:0.037 knn-tuned-l:0.037 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9657, entropy=0.8615, kl_div=0.0000
    Epoch 1: policy_loss=-0.0671, value_loss=0.9657, entropy=0.8599, kl_div=-0.0068
  Round 3/3: Mean predicted reward = 8.858

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 15 Results ---
  Mean Oracle Reward: 9.070
  Min Oracle Reward: 6.199
  Max Oracle Reward: 11.818
  Std Oracle Reward: 1.433
  Sequence Diversity: 1.000
  Models Used: 11
  Model R2 - Mean: -0.052, Max: 0.080, Count: 13
  Total Sequences Evaluated: 530
    Oracle Count: 480 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}]

======================================================================
EXPERIMENT ROUND 16/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 530
  Performance plateaued, reducing LR to 0.000136

--- Round 16 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AGATGGACTC
  CGTGATCGAC
  TCGCAGAGTC
  CCAATTGAGG
  CCGAGGAATT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.161
  Max reward: 11.685
  With intrinsic bonuses: 9.172

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9706, entropy=0.8652, kl_div=0.0000
    Epoch 1: policy_loss=-0.0336, value_loss=0.9706, entropy=0.8643, kl_div=0.0026

=== Surrogate Model Training ===
Total samples: 562

Training on 533 samples (removed 29 outliers)
Reward range: [6.24, 11.30], mean: 9.00
  Created 13 candidate models for data size 533
Current R2 threshold: -0.24
  rf-tuned-l: R2 = 0.075 (std: 0.093)
  rf-tuned-xl: R2 = 0.075 (std: 0.092)
  gb-tuned-l: R2 = 0.019 (std: 0.098)
  gb-tuned-xl: R2 = 0.018 (std: 0.099)
  xgb-xl: R2 = -0.011 (std: 0.091)
  xgb-l: R2 = -0.011 (std: 0.091)
  mlp-adaptive-xl: R2 = -0.259 (std: 0.153)
  mlp-l: R2 = -0.191 (std: 0.096)
  svr-rbf-xl: R2 = 0.068 (std: 0.122)
  svr-poly-l: R2 = 0.068 (std: 0.122)
  knn-tuned-sqrt: R2 = -0.060 (std: 0.140)
  knn-tuned-l: R2 = -0.060 (std: 0.140)
  ridge: R2 = -0.136 (std: 0.171)

Model-based training with 12 models
Best R2: 0.075, Mean R2: -0.031
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.152 gb-tuned-l:0.087 gb-tuned-xl:0.086 xgb-xl:0.065 xgb-l:0.065 mlp-l:0.011 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.039 knn-tuned-l:0.039 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.8607, kl_div=0.0000
    Epoch 1: policy_loss=-0.0150, value_loss=0.9693, entropy=0.8569, kl_div=0.0409
  Round 1/3: Mean predicted reward = 9.067
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.152 gb-tuned-l:0.087 gb-tuned-xl:0.086 xgb-xl:0.065 xgb-l:0.065 mlp-l:0.011 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.039 knn-tuned-l:0.039 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9667, entropy=0.8518, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1389
  Round 2/3: Mean predicted reward = 9.056
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.152 gb-tuned-l:0.087 gb-tuned-xl:0.086 xgb-xl:0.065 xgb-l:0.065 mlp-l:0.011 svr-rbf-xl:0.142 svr-poly-l:0.142 knn-tuned-sqrt:0.039 knn-tuned-l:0.039 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9680, entropy=0.8449, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1729
  Round 3/3: Mean predicted reward = 8.947

  === Progress Analysis ===
  Status: NORMAL

--- Round 16 Results ---
  Mean Oracle Reward: 9.110
  Min Oracle Reward: 5.285
  Max Oracle Reward: 11.680
  Std Oracle Reward: 1.088
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: -0.031, Max: 0.075, Count: 13
  Total Sequences Evaluated: 562
    Oracle Count: 512 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}]

======================================================================
EXPERIMENT ROUND 17/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 562
  Performance plateaued, reducing LR to 0.000100

--- Round 17 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CAACCGTTGG
  ATATGACCGG
  CGGCAATATG
  AGTCGGACAT
  TTCGAAAGGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.791
  Max reward: 10.352
  With intrinsic bonuses: 8.724

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9672, entropy=0.8459, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1602

=== Surrogate Model Training ===
Total samples: 594

Training on 563 samples (removed 31 outliers)
Reward range: [6.31, 11.30], mean: 9.00
  Created 13 candidate models for data size 563
Current R2 threshold: -0.22999999999999998
  rf-tuned-l: R2 = 0.087 (std: 0.078)
  rf-tuned-xl: R2 = 0.099 (std: 0.059)
  gb-tuned-l: R2 = 0.019 (std: 0.095)
  gb-tuned-xl: R2 = 0.019 (std: 0.095)
  xgb-xl: R2 = -0.023 (std: 0.045)
  xgb-l: R2 = -0.023 (std: 0.045)
  mlp-adaptive-xl: R2 = -0.163 (std: 0.091)
  mlp-l: R2 = -0.155 (std: 0.226)
  svr-rbf-xl: R2 = 0.104 (std: 0.073)
  svr-poly-l: R2 = 0.104 (std: 0.073)
  knn-tuned-sqrt: R2 = -0.070 (std: 0.099)
  knn-tuned-l: R2 = -0.070 (std: 0.099)
  ridge: R2 = -0.126 (std: 0.136)

Model-based training with 13 models
Best R2: 0.104, Mean R2: -0.015
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.164 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.048 xgb-l:0.048 mlp-adaptive-xl:0.012 mlp-l:0.013 svr-rbf-xl:0.173 svr-poly-l:0.173 knn-tuned-sqrt:0.030 knn-tuned-l:0.030 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.8391, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0898
  Round 1/3: Mean predicted reward = 8.708
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.164 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.048 xgb-l:0.048 mlp-adaptive-xl:0.012 mlp-l:0.013 svr-rbf-xl:0.173 svr-poly-l:0.173 knn-tuned-sqrt:0.030 knn-tuned-l:0.030 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9662, entropy=0.8493, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1091
  Round 2/3: Mean predicted reward = 9.071
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.145 rf-tuned-xl:0.164 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.048 xgb-l:0.048 mlp-adaptive-xl:0.012 mlp-l:0.013 svr-rbf-xl:0.173 svr-poly-l:0.173 knn-tuned-sqrt:0.030 knn-tuned-l:0.030 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.8196, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1137
  Round 3/3: Mean predicted reward = 9.131

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 17 Results ---
  Mean Oracle Reward: 8.765
  Min Oracle Reward: 5.400
  Max Oracle Reward: 10.590
  Std Oracle Reward: 0.940
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.015, Max: 0.104, Count: 13
  Total Sequences Evaluated: 594
    Oracle Count: 544 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}]

======================================================================
EXPERIMENT ROUND 18/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 594

--- Round 18 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CTACGGGTAC
  GCCTAGGCAG
  CACCTGTGGA
  ACGGATCCGT
  CGGCGAATTC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.336
  Max reward: 13.359
  With intrinsic bonuses: 9.342

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.8166, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1649

=== Surrogate Model Training ===
Total samples: 626

Training on 593 samples (removed 33 outliers)
Reward range: [6.31, 11.45], mean: 9.00
  Created 13 candidate models for data size 593
Current R2 threshold: -0.21999999999999997
  rf-tuned-l: R2 = 0.089 (std: 0.075)
  rf-tuned-xl: R2 = 0.091 (std: 0.077)
  gb-tuned-l: R2 = 0.024 (std: 0.131)
  gb-tuned-xl: R2 = 0.024 (std: 0.131)
  xgb-xl: R2 = 0.015 (std: 0.073)
  xgb-l: R2 = 0.015 (std: 0.073)
  mlp-adaptive-xl: R2 = -0.171 (std: 0.155)
  mlp-l: R2 = -0.120 (std: 0.173)
  svr-rbf-xl: R2 = 0.106 (std: 0.070)
  svr-poly-l: R2 = 0.106 (std: 0.070)
  knn-tuned-sqrt: R2 = -0.053 (std: 0.128)
  knn-tuned-l: R2 = -0.053 (std: 0.128)
  ridge: R2 = -0.121 (std: 0.138)

Model-based training with 13 models
Best R2: 0.106, Mean R2: -0.004
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.138 rf-tuned-xl:0.142 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.010 mlp-l:0.017 svr-rbf-xl:0.165 svr-poly-l:0.165 knn-tuned-sqrt:0.033 knn-tuned-l:0.033 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.8163, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1372
  Round 1/3: Mean predicted reward = 8.959
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.138 rf-tuned-xl:0.142 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.010 mlp-l:0.017 svr-rbf-xl:0.165 svr-poly-l:0.165 knn-tuned-sqrt:0.033 knn-tuned-l:0.033 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9663, entropy=0.7927, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1719
  Round 2/3: Mean predicted reward = 9.104
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.138 rf-tuned-xl:0.142 gb-tuned-l:0.073 gb-tuned-xl:0.073 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.010 mlp-l:0.017 svr-rbf-xl:0.165 svr-poly-l:0.165 knn-tuned-sqrt:0.033 knn-tuned-l:0.033 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.8184, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1534
  Round 3/3: Mean predicted reward = 9.022

  === Progress Analysis ===
  Status: NORMAL

--- Round 18 Results ---
  Mean Oracle Reward: 9.331
  Min Oracle Reward: 6.200
  Max Oracle Reward: 13.276
  Std Oracle Reward: 1.436
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.004, Max: 0.106, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 626
    Oracle Count: 576 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 19/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 626

--- Round 19 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GTAACGGCTC
  CATGCTAGTA
  GCAAGCGCTT
  CAGGGAATCT
  GCTCATGCAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.526
  Max reward: 11.078
  With intrinsic bonuses: 9.489

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9705, entropy=0.8116, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0639

=== Surrogate Model Training ===
Total samples: 658

Training on 624 samples (removed 34 outliers)
Reward range: [6.31, 11.45], mean: 9.03
  Created 13 candidate models for data size 624
Current R2 threshold: -0.21
  rf-tuned-l: R2 = 0.111 (std: 0.046)
  rf-tuned-xl: R2 = 0.092 (std: 0.077)
  gb-tuned-l: R2 = 0.042 (std: 0.083)
  gb-tuned-xl: R2 = 0.042 (std: 0.083)
  xgb-xl: R2 = -0.039 (std: 0.067)
  xgb-l: R2 = -0.039 (std: 0.067)
  mlp-adaptive-xl: R2 = -0.107 (std: 0.109)
  mlp-l: R2 = -0.127 (std: 0.137)
  svr-rbf-xl: R2 = 0.096 (std: 0.049)
  svr-poly-l: R2 = 0.096 (std: 0.049)
  knn-tuned-sqrt: R2 = -0.042 (std: 0.101)
  knn-tuned-l: R2 = -0.042 (std: 0.101)
  ridge: R2 = -0.097 (std: 0.096)

Model-based training with 13 models
Best R2: 0.111, Mean R2: -0.001
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.174 rf-tuned-xl:0.144 gb-tuned-l:0.087 gb-tuned-xl:0.087 xgb-xl:0.039 xgb-l:0.039 mlp-adaptive-xl:0.020 mlp-l:0.016 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.038 knn-tuned-l:0.038 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9694, entropy=0.7880, kl_div=0.0000
    Epoch 1: policy_loss=0.0044, value_loss=0.9694, entropy=0.7859, kl_div=0.0496
  Round 1/3: Mean predicted reward = 9.040
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.174 rf-tuned-xl:0.144 gb-tuned-l:0.087 gb-tuned-xl:0.087 xgb-xl:0.039 xgb-l:0.039 mlp-adaptive-xl:0.020 mlp-l:0.016 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.038 knn-tuned-l:0.038 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9673, entropy=0.8171, kl_div=0.0000
    Epoch 1: policy_loss=-0.0187, value_loss=0.9673, entropy=0.8154, kl_div=0.0311
  Round 2/3: Mean predicted reward = 9.026
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.174 rf-tuned-xl:0.144 gb-tuned-l:0.087 gb-tuned-xl:0.087 xgb-xl:0.039 xgb-l:0.039 mlp-adaptive-xl:0.020 mlp-l:0.016 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.038 knn-tuned-l:0.038 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9674, entropy=0.8033, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0657
  Round 3/3: Mean predicted reward = 8.864

  === Progress Analysis ===
  Status: NORMAL

--- Round 19 Results ---
  Mean Oracle Reward: 9.505
  Min Oracle Reward: 6.482
  Max Oracle Reward: 11.261
  Std Oracle Reward: 0.929
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: -0.001, Max: 0.111, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 658
    Oracle Count: 608 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 20/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 658
  Consistent improvement, increasing LR to 0.000360

--- Round 20 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GTCAACGGAT
  ATTAACGGGC
  GACGCTCATG
  CATGCGTGCA
  ACACGGATTG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.384
  Max reward: 11.136
  With intrinsic bonuses: 9.329

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9694, entropy=0.7862, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5768

=== Surrogate Model Training ===
Total samples: 690

Training on 655 samples (removed 35 outliers)
Reward range: [6.34, 11.45], mean: 9.06
  Created 13 candidate models for data size 655
Current R2 threshold: -0.19999999999999998
  rf-tuned-l: R2 = 0.084 (std: 0.054)
  rf-tuned-xl: R2 = 0.082 (std: 0.045)
  gb-tuned-l: R2 = 0.044 (std: 0.051)
  gb-tuned-xl: R2 = 0.044 (std: 0.051)
  xgb-xl: R2 = 0.056 (std: 0.081)
  xgb-l: R2 = 0.056 (std: 0.081)
  mlp-adaptive-xl: R2 = -0.111 (std: 0.146)
  mlp-l: R2 = -0.148 (std: 0.079)
  svr-rbf-xl: R2 = 0.109 (std: 0.072)
  svr-poly-l: R2 = 0.109 (std: 0.072)
  knn-tuned-sqrt: R2 = -0.069 (std: 0.103)
  knn-tuned-l: R2 = -0.069 (std: 0.103)
  ridge: R2 = -0.094 (std: 0.101)

Model-based training with 13 models
Best R2: 0.109, Mean R2: 0.007
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.092 xgb-l:0.092 mlp-adaptive-xl:0.017 mlp-l:0.012 svr-rbf-xl:0.155 svr-poly-l:0.155 knn-tuned-sqrt:0.026 knn-tuned-l:0.026 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9664, entropy=0.7652, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4431
  Round 1/3: Mean predicted reward = 8.881
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.092 xgb-l:0.092 mlp-adaptive-xl:0.017 mlp-l:0.012 svr-rbf-xl:0.155 svr-poly-l:0.155 knn-tuned-sqrt:0.026 knn-tuned-l:0.026 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9694, entropy=0.7466, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6256
  Round 2/3: Mean predicted reward = 9.102
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.121 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.092 xgb-l:0.092 mlp-adaptive-xl:0.017 mlp-l:0.012 svr-rbf-xl:0.155 svr-poly-l:0.155 knn-tuned-sqrt:0.026 knn-tuned-l:0.026 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9672, entropy=0.7252, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7967
  Round 3/3: Mean predicted reward = 9.003

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 20 Results ---
  Mean Oracle Reward: 9.364
  Min Oracle Reward: 7.089
  Max Oracle Reward: 10.971
  Std Oracle Reward: 0.854
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.007, Max: 0.109, Count: 13
  Total Sequences Evaluated: 690
    Oracle Count: 640 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 21/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 690
  Performance plateaued, reducing LR to 0.000136

--- Round 21 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TGAAGACGTC
  TTCAAGGACG
  GGACAGTACT
  CTCCGGAAGT
  CGCGTAAGGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.835
  Max reward: 10.572
  With intrinsic bonuses: 8.848

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.7192, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4291

=== Surrogate Model Training ===
Total samples: 722

Training on 680 samples (removed 42 outliers)
Reward range: [6.41, 11.45], mean: 9.08
  Created 13 candidate models for data size 680
Current R2 threshold: -0.19
  rf-tuned-l: R2 = 0.073 (std: 0.048)
  rf-tuned-xl: R2 = 0.071 (std: 0.062)
  gb-tuned-l: R2 = 0.064 (std: 0.049)
  gb-tuned-xl: R2 = 0.064 (std: 0.049)
  xgb-xl: R2 = -0.014 (std: 0.132)
  xgb-l: R2 = -0.014 (std: 0.132)
  mlp-adaptive-xl: R2 = -0.128 (std: 0.092)
  mlp-l: R2 = -0.068 (std: 0.152)
  svr-rbf-xl: R2 = 0.098 (std: 0.062)
  svr-poly-l: R2 = 0.098 (std: 0.062)
  knn-tuned-sqrt: R2 = -0.061 (std: 0.111)
  knn-tuned-l: R2 = -0.061 (std: 0.111)
  ridge: R2 = -0.071 (std: 0.087)

Model-based training with 13 models
Best R2: 0.098, Mean R2: 0.004
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.120 rf-tuned-xl:0.117 gb-tuned-l:0.109 gb-tuned-xl:0.109 xgb-xl:0.050 xgb-l:0.050 mlp-adaptive-xl:0.016 mlp-l:0.029 svr-rbf-xl:0.154 svr-poly-l:0.154 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.028 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.6982, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4439
  Round 1/3: Mean predicted reward = 8.714
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.120 rf-tuned-xl:0.117 gb-tuned-l:0.109 gb-tuned-xl:0.109 xgb-xl:0.050 xgb-l:0.050 mlp-adaptive-xl:0.016 mlp-l:0.029 svr-rbf-xl:0.154 svr-poly-l:0.154 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.028 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.6842, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4891
  Round 2/3: Mean predicted reward = 9.027
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.120 rf-tuned-xl:0.117 gb-tuned-l:0.109 gb-tuned-xl:0.109 xgb-xl:0.050 xgb-l:0.050 mlp-adaptive-xl:0.016 mlp-l:0.029 svr-rbf-xl:0.154 svr-poly-l:0.154 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.028 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9710, entropy=0.6726, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6115
  Round 3/3: Mean predicted reward = 9.087

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 21 Results ---
  Mean Oracle Reward: 8.882
  Min Oracle Reward: 4.757
  Max Oracle Reward: 10.520
  Std Oracle Reward: 1.257
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.004, Max: 0.098, Count: 13
  Total Sequences Evaluated: 722
    Oracle Count: 672 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 22/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 722

--- Round 22 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CCCGGTGTAA
  GCAGTACATG
  ATGTCGACCG
  CGCCGATTAG
  AGTGCGCCAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.060
  Max reward: 10.989
  With intrinsic bonuses: 9.079

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9679, entropy=0.6571, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6541

=== Surrogate Model Training ===
Total samples: 754

Training on 716 samples (removed 38 outliers)
Reward range: [6.37, 11.45], mean: 9.06
  Created 13 candidate models for data size 716
Current R2 threshold: -0.18
  rf-tuned-l: R2 = 0.095 (std: 0.047)
  rf-tuned-xl: R2 = 0.092 (std: 0.044)
  gb-tuned-l: R2 = 0.052 (std: 0.052)
  gb-tuned-xl: R2 = 0.052 (std: 0.052)
  xgb-xl: R2 = 0.077 (std: 0.098)
  xgb-l: R2 = 0.077 (std: 0.098)
  mlp-adaptive-xl: R2 = -0.122 (std: 0.152)
  mlp-l: R2 = -0.082 (std: 0.134)
  svr-rbf-xl: R2 = 0.120 (std: 0.041)
  svr-poly-l: R2 = 0.120 (std: 0.041)
  knn-tuned-sqrt: R2 = -0.008 (std: 0.089)
  knn-tuned-l: R2 = -0.008 (std: 0.089)
  ridge: R2 = -0.072 (std: 0.078)

Model-based training with 13 models
Best R2: 0.120, Mean R2: 0.030
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.112 gb-tuned-l:0.075 gb-tuned-xl:0.075 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.013 mlp-l:0.020 svr-rbf-xl:0.148 svr-poly-l:0.148 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9695, entropy=0.6530, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4250
  Round 1/3: Mean predicted reward = 8.697
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.112 gb-tuned-l:0.075 gb-tuned-xl:0.075 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.013 mlp-l:0.020 svr-rbf-xl:0.148 svr-poly-l:0.148 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.6087, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6218
  Round 2/3: Mean predicted reward = 8.985
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.112 gb-tuned-l:0.075 gb-tuned-xl:0.075 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.013 mlp-l:0.020 svr-rbf-xl:0.148 svr-poly-l:0.148 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.6090, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5920
  Round 3/3: Mean predicted reward = 8.998

  === Progress Analysis ===
  Status: NORMAL

--- Round 22 Results ---
  Mean Oracle Reward: 9.060
  Min Oracle Reward: 7.299
  Max Oracle Reward: 11.132
  Std Oracle Reward: 0.874
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.030, Max: 0.120, Count: 13
  Total Sequences Evaluated: 754
    Oracle Count: 704 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 23/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 754

--- Round 23 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ACCGAGTCTG
  ACGAATCGGT
  CGCCGTGAAT
  GTAACCTAGG
  TGCAGACGAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.030
  Max reward: 11.338
  With intrinsic bonuses: 9.007

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9711, entropy=0.5770, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4057

=== Surrogate Model Training ===
Total samples: 786

Training on 747 samples (removed 39 outliers)
Reward range: [6.37, 11.45], mean: 9.07
  Created 13 candidate models for data size 747
Current R2 threshold: -0.16999999999999998
  rf-tuned-l: R2 = 0.104 (std: 0.040)
  rf-tuned-xl: R2 = 0.104 (std: 0.042)
  gb-tuned-l: R2 = 0.067 (std: 0.059)
  gb-tuned-xl: R2 = 0.067 (std: 0.059)
  xgb-xl: R2 = 0.031 (std: 0.045)
  xgb-l: R2 = 0.031 (std: 0.045)
  mlp-adaptive-xl: R2 = -0.094 (std: 0.102)
  mlp-l: R2 = -0.027 (std: 0.073)
  svr-rbf-xl: R2 = 0.140 (std: 0.046)
  svr-poly-l: R2 = 0.140 (std: 0.046)
  knn-tuned-sqrt: R2 = 0.012 (std: 0.093)
  knn-tuned-l: R2 = 0.012 (std: 0.093)
  ridge: R2 = -0.071 (std: 0.083)

Model-based training with 13 models
Best R2: 0.140, Mean R2: 0.040
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.116 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.056 xgb-l:0.056 mlp-adaptive-xl:0.016 mlp-l:0.031 svr-rbf-xl:0.167 svr-poly-l:0.167 knn-tuned-sqrt:0.046 knn-tuned-l:0.046 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.5669, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2469
  Round 1/3: Mean predicted reward = 8.613
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.116 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.056 xgb-l:0.056 mlp-adaptive-xl:0.016 mlp-l:0.031 svr-rbf-xl:0.167 svr-poly-l:0.167 knn-tuned-sqrt:0.046 knn-tuned-l:0.046 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9665, entropy=0.5706, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2815
  Round 2/3: Mean predicted reward = 9.117
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.116 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.056 xgb-l:0.056 mlp-adaptive-xl:0.016 mlp-l:0.031 svr-rbf-xl:0.167 svr-poly-l:0.167 knn-tuned-sqrt:0.046 knn-tuned-l:0.046 ridge:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9659, entropy=0.6066, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3044
  Round 3/3: Mean predicted reward = 8.912

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 23 Results ---
  Mean Oracle Reward: 9.029
  Min Oracle Reward: 5.469
  Max Oracle Reward: 11.268
  Std Oracle Reward: 1.210
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.040, Max: 0.140, Count: 13
  Total Sequences Evaluated: 786
    Oracle Count: 736 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 24/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 786
  Performance plateaued, reducing LR to 0.000019

--- Round 24 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  ATATGCGCGC
  AGTCAACGTG
  CCGGATTGAA
  GCGATCGAAT
  CTGTGACGAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.019
  Max reward: 11.625
  With intrinsic bonuses: 9.046

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9701, entropy=0.5833, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=0.9701, entropy=0.5831, kl_div=0.0389

=== Surrogate Model Training ===
Total samples: 818

Training on 774 samples (removed 44 outliers)
Reward range: [6.41, 11.65], mean: 9.08
  Created 13 candidate models for data size 774
Current R2 threshold: -0.15999999999999998
  rf-tuned-l: R2 = 0.087 (std: 0.037)
  rf-tuned-xl: R2 = 0.108 (std: 0.037)
  gb-tuned-l: R2 = 0.070 (std: 0.051)
  gb-tuned-xl: R2 = 0.070 (std: 0.051)
  xgb-xl: R2 = -0.002 (std: 0.037)
  xgb-l: R2 = -0.002 (std: 0.037)
  mlp-adaptive-xl: R2 = -0.020 (std: 0.079)
  mlp-l: R2 = -0.016 (std: 0.072)
  svr-rbf-xl: R2 = 0.152 (std: 0.021)
  svr-poly-l: R2 = 0.152 (std: 0.021)
  knn-tuned-sqrt: R2 = 0.001 (std: 0.091)
  knn-tuned-l: R2 = 0.001 (std: 0.091)
  ridge: R2 = -0.052 (std: 0.068)

Model-based training with 13 models
Best R2: 0.152, Mean R2: 0.042
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.040 xgb-l:0.040 mlp-adaptive-xl:0.033 mlp-l:0.034 svr-rbf-xl:0.185 svr-poly-l:0.185 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9717, entropy=0.5786, kl_div=0.0000
    Epoch 1: policy_loss=-0.0067, value_loss=0.9717, entropy=0.5796, kl_div=-0.0048
  Round 1/3: Mean predicted reward = 8.780
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.040 xgb-l:0.040 mlp-adaptive-xl:0.033 mlp-l:0.034 svr-rbf-xl:0.185 svr-poly-l:0.185 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9699, entropy=0.5821, kl_div=0.0000
    Epoch 1: policy_loss=-0.0033, value_loss=0.9699, entropy=0.5831, kl_div=-0.0133
  Round 2/3: Mean predicted reward = 9.127
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.119 gb-tuned-l:0.081 gb-tuned-xl:0.081 xgb-xl:0.040 xgb-l:0.040 mlp-adaptive-xl:0.033 mlp-l:0.034 svr-rbf-xl:0.185 svr-poly-l:0.185 knn-tuned-sqrt:0.041 knn-tuned-l:0.041 ridge:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.5919, kl_div=0.0000
    Epoch 1: policy_loss=-0.0091, value_loss=0.9688, entropy=0.5916, kl_div=0.0190
  Round 3/3: Mean predicted reward = 9.155

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 24 Results ---
  Mean Oracle Reward: 9.085
  Min Oracle Reward: 5.225
  Max Oracle Reward: 11.502
  Std Oracle Reward: 1.084
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.042, Max: 0.152, Count: 13
  Total Sequences Evaluated: 818
    Oracle Count: 768 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 25/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 818
  Performance plateaued, reducing LR to 0.000150

--- Round 25 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TGGCTACGCA
  ACCGAGTGTC
  TGACAGAGCT
  CAGGTCATAG
  ACTCTCGGGA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.145
  Max reward: 11.475
  With intrinsic bonuses: 9.127

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.5556, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4117

=== Surrogate Model Training ===
Total samples: 850

Training on 804 samples (removed 46 outliers)
Reward range: [6.38, 11.65], mean: 9.09
  Created 13 candidate models for data size 804
Current R2 threshold: -0.15
  rf-tuned-l: R2 = 0.112 (std: 0.045)
  rf-tuned-xl: R2 = 0.113 (std: 0.036)
  gb-tuned-l: R2 = 0.080 (std: 0.038)
  gb-tuned-xl: R2 = 0.080 (std: 0.038)
  xgb-xl: R2 = 0.022 (std: 0.058)
  xgb-l: R2 = 0.022 (std: 0.058)
  mlp-adaptive-xl: R2 = 0.022 (std: 0.083)
  mlp-l: R2 = -0.009 (std: 0.060)
  svr-rbf-xl: R2 = 0.167 (std: 0.045)
  svr-poly-l: R2 = 0.167 (std: 0.045)
  knn-tuned-sqrt: R2 = 0.026 (std: 0.097)
  knn-tuned-l: R2 = 0.026 (std: 0.097)
  ridge: R2 = -0.048 (std: 0.067)

Model-based training with 13 models
Best R2: 0.167, Mean R2: 0.060
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.076 gb-tuned-xl:0.076 xgb-xl:0.043 xgb-l:0.043 mlp-adaptive-xl:0.043 mlp-l:0.031 svr-rbf-xl:0.183 svr-poly-l:0.183 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9703, entropy=0.5502, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2330
  Round 1/3: Mean predicted reward = 8.618
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.076 gb-tuned-xl:0.076 xgb-xl:0.043 xgb-l:0.043 mlp-adaptive-xl:0.043 mlp-l:0.031 svr-rbf-xl:0.183 svr-poly-l:0.183 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9697, entropy=0.5821, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3107
  Round 2/3: Mean predicted reward = 9.110
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.076 gb-tuned-xl:0.076 xgb-xl:0.043 xgb-l:0.043 mlp-adaptive-xl:0.043 mlp-l:0.031 svr-rbf-xl:0.183 svr-poly-l:0.183 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9669, entropy=0.5734, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1457
  Round 3/3: Mean predicted reward = 9.033

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 25 Results ---
  Mean Oracle Reward: 9.135
  Min Oracle Reward: 6.230
  Max Oracle Reward: 11.467
  Std Oracle Reward: 1.293
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.060, Max: 0.167, Count: 13
  Total Sequences Evaluated: 850
    Oracle Count: 800 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 26/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 850
  Performance plateaued, reducing LR to 0.000136

--- Round 26 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CTGACCGTAG
  CCAGTTAAGG
  TGACGGAATC
  CATGCGGCAT
  CGTAGCTACG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.056
  Max reward: 11.478
  With intrinsic bonuses: 9.058

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.5916, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1687

=== Surrogate Model Training ===
Total samples: 882

Training on 835 samples (removed 47 outliers)
Reward range: [6.38, 11.65], mean: 9.10
  Created 13 candidate models for data size 835
Current R2 threshold: -0.13999999999999999
  rf-tuned-l: R2 = 0.120 (std: 0.052)
  rf-tuned-xl: R2 = 0.114 (std: 0.065)
  gb-tuned-l: R2 = 0.083 (std: 0.030)
  gb-tuned-xl: R2 = 0.083 (std: 0.030)
  xgb-xl: R2 = 0.053 (std: 0.075)
  xgb-l: R2 = 0.053 (std: 0.075)
  mlp-adaptive-xl: R2 = 0.030 (std: 0.077)
  mlp-l: R2 = 0.037 (std: 0.062)
  svr-rbf-xl: R2 = 0.177 (std: 0.055)
  svr-poly-l: R2 = 0.177 (std: 0.055)
  knn-tuned-sqrt: R2 = 0.044 (std: 0.107)
  knn-tuned-l: R2 = 0.044 (std: 0.107)
  ridge: R2 = -0.049 (std: 0.051)

Model-based training with 13 models
Best R2: 0.177, Mean R2: 0.074
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.096 gb-tuned-l:0.070 gb-tuned-xl:0.070 xgb-xl:0.052 xgb-l:0.052 mlp-adaptive-xl:0.042 mlp-l:0.044 svr-rbf-xl:0.179 svr-poly-l:0.179 knn-tuned-sqrt:0.048 knn-tuned-l:0.048 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9698, entropy=0.6114, kl_div=0.0000
    Epoch 1: policy_loss=-0.0545, value_loss=0.9698, entropy=0.6164, kl_div=0.0353
  Round 1/3: Mean predicted reward = 8.557
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.096 gb-tuned-l:0.070 gb-tuned-xl:0.070 xgb-xl:0.052 xgb-l:0.052 mlp-adaptive-xl:0.042 mlp-l:0.044 svr-rbf-xl:0.179 svr-poly-l:0.179 knn-tuned-sqrt:0.048 knn-tuned-l:0.048 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9685, entropy=0.5985, kl_div=0.0000
    Epoch 1: policy_loss=-0.0454, value_loss=0.9685, entropy=0.6101, kl_div=-0.2018
  Round 2/3: Mean predicted reward = 8.970
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.096 gb-tuned-l:0.070 gb-tuned-xl:0.070 xgb-xl:0.052 xgb-l:0.052 mlp-adaptive-xl:0.042 mlp-l:0.044 svr-rbf-xl:0.179 svr-poly-l:0.179 knn-tuned-sqrt:0.048 knn-tuned-l:0.048 ridge:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.6136, kl_div=0.0000
    Epoch 1: policy_loss=-0.0956, value_loss=0.9697, entropy=0.6184, kl_div=0.0229
  Round 3/3: Mean predicted reward = 9.132

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 26 Results ---
  Mean Oracle Reward: 9.072
  Min Oracle Reward: 5.427
  Max Oracle Reward: 11.214
  Std Oracle Reward: 1.075
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.074, Max: 0.177, Count: 13
  Total Sequences Evaluated: 882
    Oracle Count: 832 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 27/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 882
  Performance plateaued, reducing LR to 0.000100

--- Round 27 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GCGTCACATG
  CAATCGTGAG
  CGCACATTGG
  ATCTAAGGCG
  TTACCGGAGA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.264
  Max reward: 10.592
  With intrinsic bonuses: 9.327

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9708, entropy=0.6346, kl_div=0.0000
    Epoch 1: policy_loss=-0.0469, value_loss=0.9708, entropy=0.6386, kl_div=-0.0477

=== Surrogate Model Training ===
Total samples: 914

Training on 866 samples (removed 48 outliers)
Reward range: [6.41, 11.65], mean: 9.10
  Created 13 candidate models for data size 866
Current R2 threshold: -0.12999999999999998
  rf-tuned-l: R2 = 0.118 (std: 0.067)
  rf-tuned-xl: R2 = 0.121 (std: 0.075)
  gb-tuned-l: R2 = 0.076 (std: 0.040)
  gb-tuned-xl: R2 = 0.076 (std: 0.040)
  xgb-xl: R2 = 0.100 (std: 0.040)
  xgb-l: R2 = 0.100 (std: 0.040)
  mlp-adaptive-xl: R2 = -0.011 (std: 0.100)
  mlp-l: R2 = 0.022 (std: 0.060)
  svr-rbf-xl: R2 = 0.192 (std: 0.062)
  svr-poly-l: R2 = 0.192 (std: 0.062)
  knn-tuned-sqrt: R2 = 0.047 (std: 0.093)
  knn-tuned-l: R2 = 0.047 (std: 0.093)
  ridge: R2 = -0.048 (std: 0.050)

Model-based training with 13 models
Best R2: 0.192, Mean R2: 0.079
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.090 rf-tuned-xl:0.094 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.025 mlp-l:0.035 svr-rbf-xl:0.190 svr-poly-l:0.190 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.6393, kl_div=0.0000
    Epoch 1: policy_loss=-0.0508, value_loss=0.9695, entropy=0.6468, kl_div=-0.2392
  Round 1/3: Mean predicted reward = 8.793
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.090 rf-tuned-xl:0.094 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.025 mlp-l:0.035 svr-rbf-xl:0.190 svr-poly-l:0.190 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9670, entropy=0.6370, kl_div=0.0000
    Epoch 1: policy_loss=-0.0221, value_loss=0.9670, entropy=0.6391, kl_div=-0.0406
  Round 2/3: Mean predicted reward = 9.164
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.090 rf-tuned-xl:0.094 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.076 xgb-l:0.076 mlp-adaptive-xl:0.025 mlp-l:0.035 svr-rbf-xl:0.190 svr-poly-l:0.190 knn-tuned-sqrt:0.045 knn-tuned-l:0.045 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9660, entropy=0.6233, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1911
  Round 3/3: Mean predicted reward = 9.109

  === Progress Analysis ===
  Status: NORMAL

--- Round 27 Results ---
  Mean Oracle Reward: 9.294
  Min Oracle Reward: 7.006
  Max Oracle Reward: 10.539
  Std Oracle Reward: 0.838
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.079, Max: 0.192, Count: 13
  Total Sequences Evaluated: 914
    Oracle Count: 864 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 28/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 914
  Performance plateaued, reducing LR to 0.000055

--- Round 28 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GCTTACGGAC
  GCAATCATGG
  AGGCAGTATC
  TCAGAGACGT
  CTAGCGAATG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.101
  Max reward: 11.039
  With intrinsic bonuses: 9.080

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9678, entropy=0.6054, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1451

=== Surrogate Model Training ===
Total samples: 946

Training on 896 samples (removed 50 outliers)
Reward range: [6.41, 11.65], mean: 9.12
  Created 13 candidate models for data size 896
Current R2 threshold: -0.12
  rf-tuned-l: R2 = 0.131 (std: 0.079)
  rf-tuned-xl: R2 = 0.129 (std: 0.082)
  gb-tuned-l: R2 = 0.079 (std: 0.033)
  gb-tuned-xl: R2 = 0.079 (std: 0.033)
  xgb-xl: R2 = 0.122 (std: 0.109)
  xgb-l: R2 = 0.122 (std: 0.109)
  mlp-adaptive-xl: R2 = -0.019 (std: 0.093)
  mlp-l: R2 = -0.017 (std: 0.056)
  svr-rbf-xl: R2 = 0.193 (std: 0.073)
  svr-poly-l: R2 = 0.193 (std: 0.073)
  knn-tuned-sqrt: R2 = 0.052 (std: 0.107)
  knn-tuned-l: R2 = 0.052 (std: 0.107)
  ridge: R2 = -0.045 (std: 0.048)

Model-based training with 13 models
Best R2: 0.193, Mean R2: 0.082
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.096 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.089 xgb-l:0.089 mlp-adaptive-xl:0.022 mlp-l:0.022 svr-rbf-xl:0.181 svr-poly-l:0.181 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.6469, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0562
  Round 1/3: Mean predicted reward = 8.690
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.096 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.089 xgb-l:0.089 mlp-adaptive-xl:0.022 mlp-l:0.022 svr-rbf-xl:0.181 svr-poly-l:0.181 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.6193, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1152
  Round 2/3: Mean predicted reward = 9.403
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.096 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.089 xgb-l:0.089 mlp-adaptive-xl:0.022 mlp-l:0.022 svr-rbf-xl:0.181 svr-poly-l:0.181 knn-tuned-sqrt:0.044 knn-tuned-l:0.044 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9698, entropy=0.6131, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1723
  Round 3/3: Mean predicted reward = 9.226

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 28 Results ---
  Mean Oracle Reward: 9.118
  Min Oracle Reward: 0.000
  Max Oracle Reward: 11.390
  Std Oracle Reward: 1.943
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.082, Max: 0.193, Count: 13
  Total Sequences Evaluated: 946
    Oracle Count: 896 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 29/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 946
  Performance plateaued, reducing LR to 0.000019

--- Round 29 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CCGTAGCTGA
  AATCCGGTGA
  ATCCGTGCAG
  ACGTTAAGCG
  CTCGGAATGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.260
  Max reward: 10.624
  With intrinsic bonuses: 9.149

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.6024, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0619

=== Surrogate Model Training ===
Total samples: 978

Training on 926 samples (removed 52 outliers)
Reward range: [6.45, 11.65], mean: 9.13
  Created 13 candidate models for data size 926
Current R2 threshold: -0.10999999999999999
  rf-tuned-l: R2 = 0.137 (std: 0.075)
  rf-tuned-xl: R2 = 0.132 (std: 0.085)
  gb-tuned-l: R2 = 0.059 (std: 0.040)
  gb-tuned-xl: R2 = 0.059 (std: 0.040)
  xgb-xl: R2 = 0.073 (std: 0.078)
  xgb-l: R2 = 0.073 (std: 0.078)
  mlp-adaptive-xl: R2 = -0.035 (std: 0.037)
  mlp-l: R2 = -0.053 (std: 0.066)
  svr-rbf-xl: R2 = 0.198 (std: 0.064)
  svr-poly-l: R2 = 0.198 (std: 0.064)
  knn-tuned-sqrt: R2 = 0.017 (std: 0.100)
  knn-tuned-l: R2 = 0.017 (std: 0.100)
  ridge: R2 = -0.059 (std: 0.061)

Model-based training with 13 models
Best R2: 0.198, Mean R2: 0.063
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.110 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.061 xgb-l:0.061 mlp-adaptive-xl:0.021 mlp-l:0.017 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.035 knn-tuned-l:0.035 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.6547, kl_div=0.0000
    Epoch 1: policy_loss=0.0163, value_loss=0.9695, entropy=0.6541, kl_div=0.0295
  Round 1/3: Mean predicted reward = 8.847
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.110 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.061 xgb-l:0.061 mlp-adaptive-xl:0.021 mlp-l:0.017 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.035 knn-tuned-l:0.035 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.6402, kl_div=0.0000
    Epoch 1: policy_loss=-0.0180, value_loss=0.9690, entropy=0.6396, kl_div=0.0299
  Round 2/3: Mean predicted reward = 9.193
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.116 rf-tuned-xl:0.110 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.061 xgb-l:0.061 mlp-adaptive-xl:0.021 mlp-l:0.017 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.035 knn-tuned-l:0.035 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.6296, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0557
  Round 3/3: Mean predicted reward = 9.326

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 29 Results ---
  Mean Oracle Reward: 9.172
  Min Oracle Reward: 5.028
  Max Oracle Reward: 11.291
  Std Oracle Reward: 1.107
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.063, Max: 0.198, Count: 13
  Total Sequences Evaluated: 978
    Oracle Count: 928 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 30/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 978
  Performance plateaued, reducing LR to 0.000150

--- Round 30 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGGCCAATTC
  GCAGACAGTT
  CGACGCGTGA
  GCAATCTAGG
  AGTCTGGCAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.421
  Max reward: 11.580
  With intrinsic bonuses: 9.386

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.5916, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5857

=== Surrogate Model Training ===
Total samples: 1010

Training on 958 samples (removed 52 outliers)
Reward range: [6.45, 11.65], mean: 9.14
  Created 13 candidate models for data size 958
Current R2 threshold: -0.09999999999999998
  rf-tuned-l: R2 = 0.142 (std: 0.071)
  rf-tuned-xl: R2 = 0.156 (std: 0.066)
  gb-tuned-l: R2 = 0.084 (std: 0.048)
  gb-tuned-xl: R2 = 0.084 (std: 0.048)
  xgb-xl: R2 = 0.104 (std: 0.051)
  xgb-l: R2 = 0.104 (std: 0.051)
  mlp-adaptive-xl: R2 = -0.027 (std: 0.100)
  mlp-l: R2 = 0.021 (std: 0.057)
  svr-rbf-xl: R2 = 0.210 (std: 0.056)
  svr-poly-l: R2 = 0.210 (std: 0.056)
  knn-tuned-sqrt: R2 = 0.023 (std: 0.071)
  knn-tuned-l: R2 = 0.023 (std: 0.071)
  ridge: R2 = -0.058 (std: 0.054)

Model-based training with 13 models
Best R2: 0.210, Mean R2: 0.083
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.118 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.019 mlp-l:0.030 svr-rbf-xl:0.201 svr-poly-l:0.201 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9702, entropy=0.6090, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3204
  Round 1/3: Mean predicted reward = 8.863
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.118 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.019 mlp-l:0.030 svr-rbf-xl:0.201 svr-poly-l:0.201 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9699, entropy=0.6234, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4164
  Round 2/3: Mean predicted reward = 9.285
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.102 rf-tuned-xl:0.118 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.070 xgb-l:0.070 mlp-adaptive-xl:0.019 mlp-l:0.030 svr-rbf-xl:0.201 svr-poly-l:0.201 knn-tuned-sqrt:0.031 knn-tuned-l:0.031 ridge:0.014 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.6016, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4624
  Round 3/3: Mean predicted reward = 9.320

  === Progress Analysis ===
  Status: NORMAL

--- Round 30 Results ---
  Mean Oracle Reward: 9.438
  Min Oracle Reward: 6.976
  Max Oracle Reward: 11.454
  Std Oracle Reward: 0.799
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.083, Max: 0.210, Count: 13
  Total Sequences Evaluated: 1010
    Oracle Count: 960 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 31/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1010
  Consistent improvement, increasing LR to 0.000327

--- Round 31 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AGGTCCATGC
  GCACTTAGGC
  ACGATCTGCG
  GCGTACGATA
  GACGACGCTT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.145
  Max reward: 10.781
  With intrinsic bonuses: 9.173

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9682, entropy=0.6001, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9615

=== Surrogate Model Training ===
Total samples: 1042

Training on 990 samples (removed 52 outliers)
Reward range: [6.45, 11.65], mean: 9.14
  Created 13 candidate models for data size 990
Current R2 threshold: -0.09
  rf-tuned-l: R2 = 0.152 (std: 0.043)
  rf-tuned-xl: R2 = 0.148 (std: 0.052)
  gb-tuned-l: R2 = 0.102 (std: 0.040)
  gb-tuned-xl: R2 = 0.102 (std: 0.040)
  xgb-xl: R2 = 0.092 (std: 0.052)
  xgb-l: R2 = 0.092 (std: 0.052)
  mlp-adaptive-xl: R2 = 0.028 (std: 0.051)
  mlp-l: R2 = 0.036 (std: 0.104)
  svr-rbf-xl: R2 = 0.223 (std: 0.036)
  svr-poly-l: R2 = 0.223 (std: 0.036)
  knn-tuned-sqrt: R2 = 0.017 (std: 0.055)
  knn-tuned-l: R2 = 0.017 (std: 0.055)
  ridge: R2 = -0.035 (std: 0.055)

Model-based training with 13 models
Best R2: 0.223, Mean R2: 0.092
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.100 gb-tuned-l:0.063 gb-tuned-xl:0.063 xgb-xl:0.057 xgb-l:0.057 mlp-adaptive-xl:0.030 mlp-l:0.033 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.5608, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6407
  Round 1/3: Mean predicted reward = 8.592
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.100 gb-tuned-l:0.063 gb-tuned-xl:0.063 xgb-xl:0.057 xgb-l:0.057 mlp-adaptive-xl:0.030 mlp-l:0.033 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9699, entropy=0.5372, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0543
  Round 2/3: Mean predicted reward = 9.231
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.100 gb-tuned-l:0.063 gb-tuned-xl:0.063 xgb-xl:0.057 xgb-l:0.057 mlp-adaptive-xl:0.030 mlp-l:0.033 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9666, entropy=0.5626, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1612
  Round 3/3: Mean predicted reward = 9.122

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 31 Results ---
  Mean Oracle Reward: 9.158
  Min Oracle Reward: 6.723
  Max Oracle Reward: 10.649
  Std Oracle Reward: 0.963
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.092, Max: 0.223, Count: 13
  Total Sequences Evaluated: 1042
    Oracle Count: 992 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 32/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1042

--- Round 32 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TAGTACCCGG
  TTGACGCGAA
  TGAGACTCGC
  GCTGAACGTC
  GACCTAGGTC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.057
  Max reward: 10.833
  With intrinsic bonuses: 9.095

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.5104, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1113

=== Surrogate Model Training ===
Total samples: 1074

Training on 1021 samples (removed 53 outliers)
Reward range: [6.45, 11.65], mean: 9.14
  Created 13 candidate models for data size 1021
Current R2 threshold: -0.07999999999999999
  rf-tuned-l: R2 = 0.163 (std: 0.050)
  rf-tuned-xl: R2 = 0.163 (std: 0.055)
  gb-tuned-l: R2 = 0.107 (std: 0.025)
  gb-tuned-xl: R2 = 0.107 (std: 0.025)
  xgb-xl: R2 = 0.098 (std: 0.046)
  xgb-l: R2 = 0.098 (std: 0.046)
  mlp-adaptive-xl: R2 = 0.051 (std: 0.063)
  mlp-l: R2 = 0.028 (std: 0.047)
  svr-rbf-xl: R2 = 0.219 (std: 0.026)
  svr-poly-l: R2 = 0.219 (std: 0.026)
  knn-tuned-sqrt: R2 = 0.011 (std: 0.072)
  knn-tuned-l: R2 = 0.011 (std: 0.072)
  ridge: R2 = -0.025 (std: 0.050)

Model-based training with 13 models
Best R2: 0.219, Mean R2: 0.096
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.112 gb-tuned-l:0.064 gb-tuned-xl:0.064 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.037 mlp-l:0.029 svr-rbf-xl:0.198 svr-poly-l:0.198 knn-tuned-sqrt:0.025 knn-tuned-l:0.025 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=0.5431, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5816
  Round 1/3: Mean predicted reward = 8.826
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.112 gb-tuned-l:0.064 gb-tuned-xl:0.064 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.037 mlp-l:0.029 svr-rbf-xl:0.198 svr-poly-l:0.198 knn-tuned-sqrt:0.025 knn-tuned-l:0.025 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9711, entropy=0.5143, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8627
  Round 2/3: Mean predicted reward = 9.222
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.112 gb-tuned-l:0.064 gb-tuned-xl:0.064 xgb-xl:0.059 xgb-l:0.059 mlp-adaptive-xl:0.037 mlp-l:0.029 svr-rbf-xl:0.198 svr-poly-l:0.198 knn-tuned-sqrt:0.025 knn-tuned-l:0.025 ridge:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9670, entropy=0.5222, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9995
  Round 3/3: Mean predicted reward = 9.399

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 32 Results ---
  Mean Oracle Reward: 9.059
  Min Oracle Reward: 5.355
  Max Oracle Reward: 10.627
  Std Oracle Reward: 1.100
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.096, Max: 0.219, Count: 13
  Total Sequences Evaluated: 1074
    Oracle Count: 1024 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 33/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1074

--- Round 33 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  AGCAGTAGCT
  ACTCGGGTAA
  CTGGGAATCC
  AGCGCTTGAC
  CCTAGTAGGA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.893
  Max reward: 10.452
  With intrinsic bonuses: 8.908

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9671, entropy=0.4566, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6504

=== Surrogate Model Training ===
Total samples: 1106

Training on 1051 samples (removed 55 outliers)
Reward range: [6.45, 11.65], mean: 9.14
  Created 13 candidate models for data size 1051
Current R2 threshold: -0.06999999999999998
  rf-tuned-l: R2 = 0.145 (std: 0.048)
  rf-tuned-xl: R2 = 0.166 (std: 0.049)
  gb-tuned-l: R2 = 0.105 (std: 0.043)
  gb-tuned-xl: R2 = 0.105 (std: 0.043)
  xgb-xl: R2 = 0.116 (std: 0.060)
  xgb-l: R2 = 0.116 (std: 0.060)
  mlp-adaptive-xl: R2 = 0.042 (std: 0.123)
  mlp-l: R2 = 0.031 (std: 0.069)
  svr-rbf-xl: R2 = 0.233 (std: 0.048)
  svr-poly-l: R2 = 0.233 (std: 0.048)
  knn-tuned-sqrt: R2 = 0.028 (std: 0.080)
  knn-tuned-l: R2 = 0.028 (std: 0.080)
  ridge: R2 = -0.026 (std: 0.049)

Model-based training with 13 models
Best R2: 0.233, Mean R2: 0.102
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.108 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.031 mlp-l:0.028 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9707, entropy=0.5041, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2841
  Round 1/3: Mean predicted reward = 8.813
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.108 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.031 mlp-l:0.028 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=0.4986, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4050
  Round 2/3: Mean predicted reward = 9.233
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.108 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.066 xgb-l:0.066 mlp-adaptive-xl:0.031 mlp-l:0.028 svr-rbf-xl:0.212 svr-poly-l:0.212 knn-tuned-sqrt:0.027 knn-tuned-l:0.027 ridge:0.016 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9678, entropy=0.4695, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5519
  Round 3/3: Mean predicted reward = 9.415

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 33 Results ---
  Mean Oracle Reward: 8.915
  Min Oracle Reward: 4.396
  Max Oracle Reward: 10.578
  Std Oracle Reward: 1.445
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.102, Max: 0.233, Count: 13
  Total Sequences Evaluated: 1106
    Oracle Count: 1056 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 34/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1106

--- Round 34 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TGCACCTGGA
  TGAGCGATCC
  TCCGTCAGGA
  TAAGGCACGT
  TCCGAGGCAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.145
  Max reward: 11.001
  With intrinsic bonuses: 9.174

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9714, entropy=0.4878, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2077

=== Surrogate Model Training ===
Total samples: 1138

Training on 1081 samples (removed 57 outliers)
Reward range: [6.45, 11.65], mean: 9.15
  Created 13 candidate models for data size 1081
Current R2 threshold: -0.06
  rf-tuned-l: R2 = 0.173 (std: 0.078)
  rf-tuned-xl: R2 = 0.178 (std: 0.070)
  gb-tuned-l: R2 = 0.120 (std: 0.052)
  gb-tuned-xl: R2 = 0.120 (std: 0.052)
  xgb-xl: R2 = 0.189 (std: 0.088)
  xgb-l: R2 = 0.189 (std: 0.088)
  mlp-adaptive-xl: R2 = 0.061 (std: 0.053)
  mlp-l: R2 = 0.047 (std: 0.081)
  svr-rbf-xl: R2 = 0.250 (std: 0.054)
  svr-poly-l: R2 = 0.250 (std: 0.054)
  knn-tuned-sqrt: R2 = 0.059 (std: 0.105)
  knn-tuned-l: R2 = 0.059 (std: 0.105)
  ridge: R2 = -0.025 (std: 0.053)

Model-based training with 13 models
Best R2: 0.250, Mean R2: 0.129
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.089 rf-tuned-xl:0.093 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.029 mlp-l:0.025 svr-rbf-xl:0.192 svr-poly-l:0.192 knn-tuned-sqrt:0.028 knn-tuned-l:0.028 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.4782, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1426
  Round 1/3: Mean predicted reward = 8.546
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.089 rf-tuned-xl:0.093 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.029 mlp-l:0.025 svr-rbf-xl:0.192 svr-poly-l:0.192 knn-tuned-sqrt:0.028 knn-tuned-l:0.028 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.4760, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1644
  Round 2/3: Mean predicted reward = 9.249
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.089 rf-tuned-xl:0.093 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.029 mlp-l:0.025 svr-rbf-xl:0.192 svr-poly-l:0.192 knn-tuned-sqrt:0.028 knn-tuned-l:0.028 ridge:0.012 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.4820, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1856
  Round 3/3: Mean predicted reward = 9.295

  === Progress Analysis ===
  Status: NORMAL

--- Round 34 Results ---
  Mean Oracle Reward: 9.166
  Min Oracle Reward: 5.656
  Max Oracle Reward: 10.676
  Std Oracle Reward: 1.102
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.129, Max: 0.250, Count: 13
  Total Sequences Evaluated: 1138
    Oracle Count: 1088 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 35/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1138

--- Round 35 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  CACGTACGGT
  AGACCGGATT
  GGCTGAACAT
  AGCATGGCTA
  TGAAGTCCGA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.328
  Max reward: 11.482
  With intrinsic bonuses: 9.338

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9676, entropy=0.4688, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.6804

=== Surrogate Model Training ===
Total samples: 1170

Training on 1113 samples (removed 57 outliers)
Reward range: [6.45, 11.65], mean: 9.15
  Created 13 candidate models for data size 1113
Current R2 threshold: -0.04999999999999999
  rf-tuned-l: R2 = 0.206 (std: 0.077)
  rf-tuned-xl: R2 = 0.201 (std: 0.070)
  gb-tuned-l: R2 = 0.146 (std: 0.061)
  gb-tuned-xl: R2 = 0.146 (std: 0.061)
  xgb-xl: R2 = 0.210 (std: 0.056)
  xgb-l: R2 = 0.210 (std: 0.056)
  mlp-adaptive-xl: R2 = 0.162 (std: 0.061)
  mlp-l: R2 = 0.116 (std: 0.060)
  svr-rbf-xl: R2 = 0.270 (std: 0.060)
  svr-poly-l: R2 = 0.270 (std: 0.060)
  knn-tuned-sqrt: R2 = 0.062 (std: 0.110)
  knn-tuned-l: R2 = 0.062 (std: 0.110)
  ridge: R2 = -0.010 (std: 0.053)

Model-based training with 13 models
Best R2: 0.270, Mean R2: 0.158
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.090 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.098 xgb-l:0.098 mlp-adaptive-xl:0.061 mlp-l:0.039 svr-rbf-xl:0.180 svr-poly-l:0.180 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 ridge:0.011 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9704, entropy=0.4452, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.0507
  Round 1/3: Mean predicted reward = 8.243
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.090 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.098 xgb-l:0.098 mlp-adaptive-xl:0.061 mlp-l:0.039 svr-rbf-xl:0.180 svr-poly-l:0.180 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 ridge:0.011 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9676, entropy=0.4360, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.3803
  Round 2/3: Mean predicted reward = 9.167
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.090 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.098 xgb-l:0.098 mlp-adaptive-xl:0.061 mlp-l:0.039 svr-rbf-xl:0.180 svr-poly-l:0.180 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 ridge:0.011 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9668, entropy=0.4251, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.4708
  Round 3/3: Mean predicted reward = 9.260

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 35 Results ---
  Mean Oracle Reward: 9.290
  Min Oracle Reward: 7.143
  Max Oracle Reward: 11.523
  Std Oracle Reward: 0.898
  Sequence Diversity: 0.969
  Models Used: 13
  Model R2 - Mean: 0.158, Max: 0.270, Count: 13
  Total Sequences Evaluated: 1170
    Oracle Count: 1120 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 36/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1170
  Consistent improvement, increasing LR to 0.000327

--- Round 36 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GTTGCGAACC
  TGCGGCCTAA
  GCTGCAATGA
  TAGCCATGAG
  GTGCAAAGCT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.917
  Max reward: 11.758
  With intrinsic bonuses: 8.918

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.4476, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.1363

=== Surrogate Model Training ===
Total samples: 1202

Training on 1143 samples (removed 59 outliers)
Reward range: [6.45, 11.68], mean: 9.15
  Created 13 candidate models for data size 1143
Current R2 threshold: -0.03999999999999998
  rf-tuned-l: R2 = 0.216 (std: 0.077)
  rf-tuned-xl: R2 = 0.220 (std: 0.080)
  gb-tuned-l: R2 = 0.156 (std: 0.051)
  gb-tuned-xl: R2 = 0.156 (std: 0.051)
  xgb-xl: R2 = 0.250 (std: 0.054)
  xgb-l: R2 = 0.250 (std: 0.054)
  mlp-adaptive-xl: R2 = 0.140 (std: 0.083)
  mlp-l: R2 = 0.121 (std: 0.052)
  svr-rbf-xl: R2 = 0.287 (std: 0.066)
  svr-poly-l: R2 = 0.287 (std: 0.066)
  knn-tuned-sqrt: R2 = 0.064 (std: 0.097)
  knn-tuned-l: R2 = 0.064 (std: 0.097)
  ridge: R2 = -0.011 (std: 0.039)

Model-based training with 13 models
Best R2: 0.287, Mean R2: 0.169
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.091 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.041 mlp-l:0.034 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 ridge:0.009 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9680, entropy=0.4252, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4704
  Round 1/3: Mean predicted reward = 8.294
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.091 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.041 mlp-l:0.034 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 ridge:0.009 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9703, entropy=0.3868, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8012
  Round 2/3: Mean predicted reward = 9.104
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.088 rf-tuned-xl:0.091 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.041 mlp-l:0.034 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 ridge:0.009 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9696, entropy=0.4370, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8013
  Round 3/3: Mean predicted reward = 9.332

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 36 Results ---
  Mean Oracle Reward: 8.837
  Min Oracle Reward: 5.800
  Max Oracle Reward: 11.590
  Std Oracle Reward: 1.245
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.169, Max: 0.287, Count: 13
  Total Sequences Evaluated: 1202
    Oracle Count: 1152 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 37/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1202

--- Round 37 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GTCAGCAGAT
  AATCTGCGGA
  AATGTCAGGC
  CAGCTTGCAG
  GCGACTGCTA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.125
  Max reward: 10.546
  With intrinsic bonuses: 9.145

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.3939, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.7161

=== Surrogate Model Training ===
Total samples: 1234

Training on 1175 samples (removed 59 outliers)
Reward range: [6.45, 11.68], mean: 9.15
  Created 13 candidate models for data size 1175
Current R2 threshold: -0.02999999999999997
  rf-tuned-l: R2 = 0.216 (std: 0.072)
  rf-tuned-xl: R2 = 0.222 (std: 0.075)
  gb-tuned-l: R2 = 0.154 (std: 0.046)
  gb-tuned-xl: R2 = 0.154 (std: 0.046)
  xgb-xl: R2 = 0.226 (std: 0.087)
  xgb-l: R2 = 0.226 (std: 0.087)
  mlp-adaptive-xl: R2 = 0.141 (std: 0.054)
  mlp-l: R2 = 0.153 (std: 0.060)
  svr-rbf-xl: R2 = 0.279 (std: 0.064)
  svr-poly-l: R2 = 0.279 (std: 0.064)
  knn-tuned-sqrt: R2 = 0.039 (std: 0.091)
  knn-tuned-l: R2 = 0.039 (std: 0.091)
  ridge: R2 = -0.012 (std: 0.032)

Model-based training with 13 models
Best R2: 0.279, Mean R2: 0.163
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.101 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.045 mlp-l:0.051 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.016 knn-tuned-l:0.016 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9702, entropy=0.4077, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2663
  Round 1/3: Mean predicted reward = 8.447
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.101 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.045 mlp-l:0.051 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.016 knn-tuned-l:0.016 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9697, entropy=0.3864, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3232
  Round 2/3: Mean predicted reward = 9.378
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.101 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.045 mlp-l:0.051 svr-rbf-xl:0.178 svr-poly-l:0.178 knn-tuned-sqrt:0.016 knn-tuned-l:0.016 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9702, entropy=0.3696, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5412
  Round 3/3: Mean predicted reward = 9.270

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 37 Results ---
  Mean Oracle Reward: 9.113
  Min Oracle Reward: 6.503
  Max Oracle Reward: 10.830
  Std Oracle Reward: 1.110
  Sequence Diversity: 1.000
  Models Used: 13
  Model R2 - Mean: 0.163, Max: 0.279, Count: 13
  Total Sequences Evaluated: 1234
    Oracle Count: 1184 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 38/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1234

--- Round 38 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  TCAGGTACGA
  GCATATATCG
  CTCCGAGAGT
  CAGCTTAGGC
  GATAGTCGCC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.290
  Max reward: 11.515
  With intrinsic bonuses: 9.357

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.3639, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2130

=== Surrogate Model Training ===
Total samples: 1266

Training on 1207 samples (removed 59 outliers)
Reward range: [6.45, 11.68], mean: 9.16
  Created 13 candidate models for data size 1207
Current R2 threshold: -0.019999999999999962
  rf-tuned-l: R2 = 0.217 (std: 0.059)
  rf-tuned-xl: R2 = 0.205 (std: 0.053)
  gb-tuned-l: R2 = 0.141 (std: 0.043)
  gb-tuned-xl: R2 = 0.141 (std: 0.043)
  xgb-xl: R2 = 0.223 (std: 0.090)
  xgb-l: R2 = 0.223 (std: 0.090)
  mlp-adaptive-xl: R2 = 0.126 (std: 0.052)
  mlp-l: R2 = 0.144 (std: 0.056)
  svr-rbf-xl: R2 = 0.264 (std: 0.054)
  svr-poly-l: R2 = 0.264 (std: 0.054)
  knn-tuned-sqrt: R2 = 0.021 (std: 0.064)
  knn-tuned-l: R2 = 0.021 (std: 0.064)
  ridge: R2 = -0.016 (std: 0.029)

Model-based training with 13 models
Best R2: 0.264, Mean R2: 0.152
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.094 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.113 xgb-l:0.113 mlp-adaptive-xl:0.043 mlp-l:0.051 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.015 knn-tuned-l:0.015 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9679, entropy=0.3452, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0604
  Round 1/3: Mean predicted reward = 7.771
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.094 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.113 xgb-l:0.113 mlp-adaptive-xl:0.043 mlp-l:0.051 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.015 knn-tuned-l:0.015 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.3848, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1556
  Round 2/3: Mean predicted reward = 9.404
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.094 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.113 xgb-l:0.113 mlp-adaptive-xl:0.043 mlp-l:0.051 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.015 knn-tuned-l:0.015 ridge:0.010 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.3292, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2075
  Round 3/3: Mean predicted reward = 9.196

  === Progress Analysis ===
  Status: NORMAL

--- Round 38 Results ---
  Mean Oracle Reward: 9.343
  Min Oracle Reward: 6.988
  Max Oracle Reward: 11.533
  Std Oracle Reward: 0.935
  Sequence Diversity: 0.969
  Models Used: 13
  Model R2 - Mean: 0.152, Max: 0.264, Count: 13
  Total Sequences Evaluated: 1266
    Oracle Count: 1216 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 39/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1266
  Consistent improvement, increasing LR to 0.000045

--- Round 39 Configuration ---
Learning rate: 0.000045
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGCACTGAGC
  TTGGGCCAAA
  AGCCGCTTAG
  TGCGTCAGAC
  GCCGTCTAAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.245
  Max reward: 11.592
  With intrinsic bonuses: 9.227

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.3633, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1222

=== Surrogate Model Training ===
Total samples: 1298

Training on 1238 samples (removed 60 outliers)
Reward range: [6.45, 11.68], mean: 9.16
  Created 13 candidate models for data size 1238
Current R2 threshold: -0.010000000000000009
  rf-tuned-l: R2 = 0.233 (std: 0.066)
  rf-tuned-xl: R2 = 0.227 (std: 0.074)
  gb-tuned-l: R2 = 0.165 (std: 0.040)
  gb-tuned-xl: R2 = 0.165 (std: 0.040)
  xgb-xl: R2 = 0.238 (std: 0.102)
  xgb-l: R2 = 0.238 (std: 0.102)
  mlp-adaptive-xl: R2 = 0.175 (std: 0.045)
  mlp-l: R2 = 0.080 (std: 0.051)
  svr-rbf-xl: R2 = 0.271 (std: 0.054)
  svr-poly-l: R2 = 0.271 (std: 0.054)
  knn-tuned-sqrt: R2 = 0.062 (std: 0.073)
  knn-tuned-l: R2 = 0.062 (std: 0.073)
  ridge: R2 = -0.014 (std: 0.036)

Model-based training with 12 models
Best R2: 0.271, Mean R2: 0.167
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.061 mlp-l:0.024 svr-rbf-xl:0.160 svr-poly-l:0.160 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9679, entropy=0.3110, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0543
  Round 1/3: Mean predicted reward = 8.160
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.061 mlp-l:0.024 svr-rbf-xl:0.160 svr-poly-l:0.160 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9677, entropy=0.3536, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0794
  Round 2/3: Mean predicted reward = 9.341
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.061 mlp-l:0.024 svr-rbf-xl:0.160 svr-poly-l:0.160 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9677, entropy=0.3176, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1219
  Round 3/3: Mean predicted reward = 9.089

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 39 Results ---
  Mean Oracle Reward: 9.239
  Min Oracle Reward: 4.305
  Max Oracle Reward: 11.827
  Std Oracle Reward: 1.317
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: 0.167, Max: 0.271, Count: 13
  Total Sequences Evaluated: 1298
    Oracle Count: 1248 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 40/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1298
  Performance plateaued, reducing LR to 0.000150

--- Round 40 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  TGGCTAAAGC
  GTCCGAGTAC
  GCAGTCAGTC
  GCTGGCTACA
  CAGGCTACGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.301
  Max reward: 11.251
  With intrinsic bonuses: 9.307

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9696, entropy=0.3848, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1884

=== Surrogate Model Training ===
Total samples: 1330

Training on 1266 samples (removed 64 outliers)
Reward range: [6.50, 11.68], mean: 9.17
  Created 13 candidate models for data size 1266
Current R2 threshold: 0.0
  rf-tuned-l: R2 = 0.241 (std: 0.076)
  rf-tuned-xl: R2 = 0.235 (std: 0.069)
  gb-tuned-l: R2 = 0.174 (std: 0.047)
  gb-tuned-xl: R2 = 0.174 (std: 0.047)
  xgb-xl: R2 = 0.261 (std: 0.077)
  xgb-l: R2 = 0.261 (std: 0.077)
  mlp-adaptive-xl: R2 = 0.158 (std: 0.037)
  mlp-l: R2 = 0.071 (std: 0.047)
  svr-rbf-xl: R2 = 0.271 (std: 0.056)
  svr-poly-l: R2 = 0.271 (std: 0.056)
  knn-tuned-sqrt: R2 = 0.078 (std: 0.077)
  knn-tuned-l: R2 = 0.078 (std: 0.077)
  ridge: R2 = -0.012 (std: 0.033)

Model-based training with 12 models
Best R2: 0.271, Mean R2: 0.174
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.134 xgb-l:0.134 mlp-adaptive-xl:0.048 mlp-l:0.020 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9681, entropy=0.3299, kl_div=0.0000
    Epoch 1: policy_loss=-0.0454, value_loss=0.9681, entropy=0.3294, kl_div=0.0008
  Round 1/3: Mean predicted reward = 7.861
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.134 xgb-l:0.134 mlp-adaptive-xl:0.048 mlp-l:0.020 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9681, entropy=0.3512, kl_div=0.0000
    Epoch 1: policy_loss=-0.0368, value_loss=0.9681, entropy=0.3515, kl_div=-0.0603
  Round 2/3: Mean predicted reward = 9.115
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.103 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.134 xgb-l:0.134 mlp-adaptive-xl:0.048 mlp-l:0.020 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9665, entropy=0.3026, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2179
  Round 3/3: Mean predicted reward = 9.265

  === Progress Analysis ===
  Status: NORMAL

--- Round 40 Results ---
  Mean Oracle Reward: 9.270
  Min Oracle Reward: 7.316
  Max Oracle Reward: 11.221
  Std Oracle Reward: 0.798
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: 0.174, Max: 0.271, Count: 13
  Total Sequences Evaluated: 1330
    Oracle Count: 1280 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 41/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1330
  Performance plateaued, reducing LR to 0.000136

--- Round 41 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  CGTCAGTGAC
  GGACTCGAAT
  GTCACGCGTA
  GCAGCTACTG
  TCCCGAGATG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.131
  Max reward: 10.997
  With intrinsic bonuses: 9.149

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9715, entropy=0.3514, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2249

=== Surrogate Model Training ===
Total samples: 1362

Training on 1296 samples (removed 66 outliers)
Reward range: [6.50, 11.68], mean: 9.18
  Created 13 candidate models for data size 1296
Current R2 threshold: 0.010000000000000009
  rf-tuned-l: R2 = 0.254 (std: 0.078)
  rf-tuned-xl: R2 = 0.251 (std: 0.073)
  gb-tuned-l: R2 = 0.183 (std: 0.048)
  gb-tuned-xl: R2 = 0.183 (std: 0.048)
  xgb-xl: R2 = 0.274 (std: 0.083)
  xgb-l: R2 = 0.274 (std: 0.083)
  mlp-adaptive-xl: R2 = 0.183 (std: 0.053)
  mlp-l: R2 = 0.153 (std: 0.041)
  svr-rbf-xl: R2 = 0.278 (std: 0.050)
  svr-poly-l: R2 = 0.278 (std: 0.050)
  knn-tuned-sqrt: R2 = 0.090 (std: 0.078)
  knn-tuned-l: R2 = 0.090 (std: 0.078)
  ridge: R2 = -0.002 (std: 0.030)

Model-based training with 12 models
Best R2: 0.278, Mean R2: 0.191
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.106 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.133 xgb-l:0.133 mlp-adaptive-xl:0.054 mlp-l:0.040 svr-rbf-xl:0.138 svr-poly-l:0.138 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.3089, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0759
  Round 1/3: Mean predicted reward = 7.541
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.106 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.133 xgb-l:0.133 mlp-adaptive-xl:0.054 mlp-l:0.040 svr-rbf-xl:0.138 svr-poly-l:0.138 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.3288, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2012
  Round 2/3: Mean predicted reward = 9.318
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.106 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.133 xgb-l:0.133 mlp-adaptive-xl:0.054 mlp-l:0.040 svr-rbf-xl:0.138 svr-poly-l:0.138 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9671, entropy=0.3286, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2559
  Round 3/3: Mean predicted reward = 9.286

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 41 Results ---
  Mean Oracle Reward: 9.179
  Min Oracle Reward: 4.420
  Max Oracle Reward: 11.071
  Std Oracle Reward: 1.315
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: 0.191, Max: 0.278, Count: 13
  Total Sequences Evaluated: 1362
    Oracle Count: 1312 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 42/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1362
  Performance plateaued, reducing LR to 0.000100

--- Round 42 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 1.000) ---
  GGGCAATCAT
  AGGGTGCCCA
  TCGGTCGCAA
  AATTCCGGGC
  GCCGTTAGCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.349
  Max reward: 11.102
  With intrinsic bonuses: 9.252

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9705, entropy=0.3342, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3188

=== Surrogate Model Training ===
Total samples: 1394

Training on 1325 samples (removed 69 outliers)
Reward range: [6.53, 11.68], mean: 9.19
  Created 13 candidate models for data size 1325
Current R2 threshold: 0.020000000000000018
  rf-tuned-l: R2 = 0.250 (std: 0.082)
  rf-tuned-xl: R2 = 0.254 (std: 0.083)
  gb-tuned-l: R2 = 0.188 (std: 0.039)
  gb-tuned-xl: R2 = 0.188 (std: 0.039)
  xgb-xl: R2 = 0.259 (std: 0.084)
  xgb-l: R2 = 0.259 (std: 0.084)
  mlp-adaptive-xl: R2 = 0.205 (std: 0.014)
  mlp-l: R2 = 0.150 (std: 0.080)
  svr-rbf-xl: R2 = 0.279 (std: 0.061)
  svr-poly-l: R2 = 0.279 (std: 0.061)
  knn-tuned-sqrt: R2 = 0.080 (std: 0.080)
  knn-tuned-l: R2 = 0.080 (std: 0.080)
  ridge: R2 = 0.000 (std: 0.030)

Model-based training with 12 models
Best R2: 0.279, Mean R2: 0.190
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.112 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.068 mlp-l:0.039 svr-rbf-xl:0.143 svr-poly-l:0.143 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.2972, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1300
  Round 1/3: Mean predicted reward = 7.105
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.112 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.068 mlp-l:0.039 svr-rbf-xl:0.143 svr-poly-l:0.143 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9700, entropy=0.3213, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1449
  Round 2/3: Mean predicted reward = 9.447
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.112 gb-tuned-l:0.058 gb-tuned-xl:0.058 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.068 mlp-l:0.039 svr-rbf-xl:0.143 svr-poly-l:0.143 knn-tuned-sqrt:0.020 knn-tuned-l:0.020 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.3377, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1265
  Round 3/3: Mean predicted reward = 9.246

  === Progress Analysis ===
  Status: NORMAL

--- Round 42 Results ---
  Mean Oracle Reward: 9.288
  Min Oracle Reward: 6.272
  Max Oracle Reward: 11.186
  Std Oracle Reward: 0.976
  Sequence Diversity: 1.000
  Models Used: 12
  Model R2 - Mean: 0.190, Max: 0.279, Count: 13
  Total Sequences Evaluated: 1394
    Oracle Count: 1344 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 43/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1394
  Performance plateaued, reducing LR to 0.000055

--- Round 43 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  CCGATACGGG
  GCAGCGCTAT
  CGATACCGGG
  GGCATATCCG
  CAGGTAGCCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.137
  Max reward: 10.742
  With intrinsic bonuses: 9.123

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.3193, kl_div=0.0000
    Epoch 1: policy_loss=-0.0257, value_loss=0.9686, entropy=0.3189, kl_div=0.0359

=== Surrogate Model Training ===
Total samples: 1426

Training on 1358 samples (removed 68 outliers)
Reward range: [6.50, 11.68], mean: 9.19
  Created 13 candidate models for data size 1358
Current R2 threshold: 0.030000000000000027
  rf-tuned-l: R2 = 0.254 (std: 0.072)
  rf-tuned-xl: R2 = 0.254 (std: 0.074)
  gb-tuned-l: R2 = 0.186 (std: 0.049)
  gb-tuned-xl: R2 = 0.186 (std: 0.049)
  xgb-xl: R2 = 0.300 (std: 0.057)
  xgb-l: R2 = 0.300 (std: 0.057)
  mlp-adaptive-xl: R2 = 0.206 (std: 0.071)
  mlp-l: R2 = 0.170 (std: 0.080)
  svr-rbf-xl: R2 = 0.282 (std: 0.064)
  svr-poly-l: R2 = 0.282 (std: 0.064)
  knn-tuned-sqrt: R2 = 0.077 (std: 0.071)
  knn-tuned-l: R2 = 0.077 (std: 0.071)
  ridge: R2 = -0.002 (std: 0.034)

Model-based training with 12 models
Best R2: 0.300, Mean R2: 0.198
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.155 xgb-l:0.155 mlp-adaptive-xl:0.061 mlp-l:0.042 svr-rbf-xl:0.129 svr-poly-l:0.129 knn-tuned-sqrt:0.017 knn-tuned-l:0.017 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9685, entropy=0.3160, kl_div=0.0000
    Epoch 1: policy_loss=-0.0289, value_loss=0.9685, entropy=0.3174, kl_div=-0.0854
  Round 1/5: Mean predicted reward = 7.133
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.155 xgb-l:0.155 mlp-adaptive-xl:0.061 mlp-l:0.042 svr-rbf-xl:0.129 svr-poly-l:0.129 knn-tuned-sqrt:0.017 knn-tuned-l:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.2941, kl_div=0.0000
    Epoch 1: policy_loss=0.0267, value_loss=0.9686, entropy=0.2950, kl_div=-0.0716
  Round 2/5: Mean predicted reward = 9.253
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.155 xgb-l:0.155 mlp-adaptive-xl:0.061 mlp-l:0.042 svr-rbf-xl:0.129 svr-poly-l:0.129 knn-tuned-sqrt:0.017 knn-tuned-l:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9728, entropy=0.3194, kl_div=0.0000
    Epoch 1: policy_loss=-0.0230, value_loss=0.9728, entropy=0.3191, kl_div=0.0269
  Round 3/5: Mean predicted reward = 9.460
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.155 xgb-l:0.155 mlp-adaptive-xl:0.061 mlp-l:0.042 svr-rbf-xl:0.129 svr-poly-l:0.129 knn-tuned-sqrt:0.017 knn-tuned-l:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.3248, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1145
  Round 4/5: Mean predicted reward = 9.295
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.098 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.155 xgb-l:0.155 mlp-adaptive-xl:0.061 mlp-l:0.042 svr-rbf-xl:0.129 svr-poly-l:0.129 knn-tuned-sqrt:0.017 knn-tuned-l:0.017 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9704, entropy=0.3014, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1406
  Round 5/5: Mean predicted reward = 9.265

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 43 Results ---
  Mean Oracle Reward: 9.111
  Min Oracle Reward: 6.148
  Max Oracle Reward: 11.105
  Std Oracle Reward: 1.147
  Sequence Diversity: 0.969
  Models Used: 12
  Model R2 - Mean: 0.198, Max: 0.300, Count: 13
  Total Sequences Evaluated: 1426
    Oracle Count: 1376 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 44/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1426

--- Round 44 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  GTCCAATGGC
  TGACGGAATC
  ACTGGTCAGC
  ACTCAATGTG
  GACCTGGAGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.183
  Max reward: 11.375
  With intrinsic bonuses: 9.165

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9666, entropy=0.2851, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0839

=== Surrogate Model Training ===
Total samples: 1458

Training on 1388 samples (removed 70 outliers)
Reward range: [6.51, 11.68], mean: 9.19
  Created 13 candidate models for data size 1388
Current R2 threshold: 0.040000000000000036
  rf-tuned-l: R2 = 0.258 (std: 0.073)
  rf-tuned-xl: R2 = 0.254 (std: 0.072)
  gb-tuned-l: R2 = 0.186 (std: 0.061)
  gb-tuned-xl: R2 = 0.186 (std: 0.061)
  xgb-xl: R2 = 0.294 (std: 0.063)
  xgb-l: R2 = 0.294 (std: 0.063)
  mlp-adaptive-xl: R2 = 0.189 (std: 0.050)
  mlp-l: R2 = 0.176 (std: 0.052)
  svr-rbf-xl: R2 = 0.296 (std: 0.069)
  svr-poly-l: R2 = 0.296 (std: 0.069)
  knn-tuned-sqrt: R2 = 0.095 (std: 0.078)
  knn-tuned-l: R2 = 0.095 (std: 0.078)
  ridge: R2 = -0.003 (std: 0.033)

Model-based training with 12 models
Best R2: 0.296, Mean R2: 0.201
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.096 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.142 xgb-l:0.142 mlp-adaptive-xl:0.050 mlp-l:0.044 svr-rbf-xl:0.146 svr-poly-l:0.146 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=34.6416, entropy=0.3362, kl_div=0.0000
    Epoch 1: policy_loss=0.0169, value_loss=34.6415, entropy=0.3359, kl_div=0.0196
  Round 1/3: Mean predicted reward = 6.982
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.096 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.142 xgb-l:0.142 mlp-adaptive-xl:0.050 mlp-l:0.044 svr-rbf-xl:0.146 svr-poly-l:0.146 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9707, entropy=0.3084, kl_div=0.0000
    Epoch 1: policy_loss=-0.0178, value_loss=0.9707, entropy=0.3083, kl_div=0.0197
  Round 2/3: Mean predicted reward = 9.354
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.096 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.142 xgb-l:0.142 mlp-adaptive-xl:0.050 mlp-l:0.044 svr-rbf-xl:0.146 svr-poly-l:0.146 knn-tuned-sqrt:0.019 knn-tuned-l:0.019 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.3302, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0793
  Round 3/3: Mean predicted reward = 9.179

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 44 Results ---
  Mean Oracle Reward: 9.184
  Min Oracle Reward: 5.425
  Max Oracle Reward: 11.609
  Std Oracle Reward: 1.172
  Sequence Diversity: 0.969
  Models Used: 12
  Model R2 - Mean: 0.201, Max: 0.296, Count: 13
  Total Sequences Evaluated: 1458
    Oracle Count: 1408 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 45/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1458
  Performance plateaued, reducing LR to 0.000150

--- Round 45 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  AGTACGCGTC
  AGCGAGTCTC
  GGACCAGGCT
  CATGCAGGAT
  CGGGACTACT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.132
  Max reward: 10.645
  With intrinsic bonuses: 9.163

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9695, entropy=0.3416, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3565

=== Surrogate Model Training ===
Total samples: 1490

Training on 1417 samples (removed 73 outliers)
Reward range: [6.56, 11.68], mean: 9.20
  Created 13 candidate models for data size 1417
Current R2 threshold: 0.050000000000000044
  rf-tuned-l: R2 = 0.271 (std: 0.062)
  rf-tuned-xl: R2 = 0.260 (std: 0.066)
  gb-tuned-l: R2 = 0.187 (std: 0.053)
  gb-tuned-xl: R2 = 0.187 (std: 0.053)
  xgb-xl: R2 = 0.278 (std: 0.072)
  xgb-l: R2 = 0.278 (std: 0.072)
  mlp-adaptive-xl: R2 = 0.201 (std: 0.075)
  mlp-l: R2 = 0.163 (std: 0.052)
  svr-rbf-xl: R2 = 0.316 (std: 0.070)
  svr-poly-l: R2 = 0.316 (std: 0.070)
  knn-tuned-sqrt: R2 = 0.119 (std: 0.069)
  knn-tuned-l: R2 = 0.119 (std: 0.069)
  ridge: R2 = 0.003 (std: 0.031)

Model-based training with 12 models
Best R2: 0.316, Mean R2: 0.208
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.096 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.053 mlp-l:0.036 svr-rbf-xl:0.168 svr-poly-l:0.168 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=18.2704, entropy=0.2879, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1683
  Round 1/5: Mean predicted reward = 7.140
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.096 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.053 mlp-l:0.036 svr-rbf-xl:0.168 svr-poly-l:0.168 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9700, entropy=0.3128, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2689
  Round 2/5: Mean predicted reward = 9.438
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.096 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.053 mlp-l:0.036 svr-rbf-xl:0.168 svr-poly-l:0.168 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9673, entropy=0.3113, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3667
  Round 3/5: Mean predicted reward = 9.342
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.096 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.053 mlp-l:0.036 svr-rbf-xl:0.168 svr-poly-l:0.168 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.3105, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4556
  Round 4/5: Mean predicted reward = 9.438
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.096 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.053 mlp-l:0.036 svr-rbf-xl:0.168 svr-poly-l:0.168 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.2640, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4588
  Round 5/5: Mean predicted reward = 9.432

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 45 Results ---
  Mean Oracle Reward: 9.159
  Min Oracle Reward: 6.502
  Max Oracle Reward: 10.494
  Std Oracle Reward: 0.952
  Sequence Diversity: 0.969
  Models Used: 12
  Model R2 - Mean: 0.208, Max: 0.316, Count: 13
  Total Sequences Evaluated: 1490
    Oracle Count: 1440 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 46/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1490
  Performance plateaued, reducing LR to 0.000136

--- Round 46 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  GTAATCCGAG
  AGCGGCCATG
  GATGCTGCAC
  GCGGCATGCA
  CCGAGTAGCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.303
  Max reward: 11.208
  With intrinsic bonuses: 9.240

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9707, entropy=0.2707, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3644

=== Surrogate Model Training ===
Total samples: 1522

Training on 1445 samples (removed 77 outliers)
Reward range: [6.57, 11.68], mean: 9.21
  Created 13 candidate models for data size 1445
Current R2 threshold: 0.06
  rf-tuned-l: R2 = 0.272 (std: 0.065)
  rf-tuned-xl: R2 = 0.265 (std: 0.055)
  gb-tuned-l: R2 = 0.180 (std: 0.047)
  gb-tuned-xl: R2 = 0.180 (std: 0.047)
  xgb-xl: R2 = 0.294 (std: 0.070)
  xgb-l: R2 = 0.294 (std: 0.070)
  mlp-adaptive-xl: R2 = 0.179 (std: 0.059)
  mlp-l: R2 = 0.204 (std: 0.065)
  svr-rbf-xl: R2 = 0.311 (std: 0.064)
  svr-poly-l: R2 = 0.311 (std: 0.064)
  knn-tuned-sqrt: R2 = 0.116 (std: 0.069)
  knn-tuned-l: R2 = 0.116 (std: 0.069)
  ridge: R2 = -0.001 (std: 0.027)

Model-based training with 12 models
Best R2: 0.311, Mean R2: 0.209
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.098 gb-tuned-l:0.042 gb-tuned-xl:0.042 xgb-xl:0.131 xgb-l:0.131 mlp-adaptive-xl:0.041 mlp-l:0.053 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=112.3834, entropy=0.2604, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2552
  Round 1/5: Mean predicted reward = 5.311
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.098 gb-tuned-l:0.042 gb-tuned-xl:0.042 xgb-xl:0.131 xgb-l:0.131 mlp-adaptive-xl:0.041 mlp-l:0.053 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.2310, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4041
  Round 2/5: Mean predicted reward = 9.391
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.098 gb-tuned-l:0.042 gb-tuned-xl:0.042 xgb-xl:0.131 xgb-l:0.131 mlp-adaptive-xl:0.041 mlp-l:0.053 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.2409, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4376
  Round 3/5: Mean predicted reward = 9.305
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.098 gb-tuned-l:0.042 gb-tuned-xl:0.042 xgb-xl:0.131 xgb-l:0.131 mlp-adaptive-xl:0.041 mlp-l:0.053 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.2505, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5241
  Round 4/5: Mean predicted reward = 9.368
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.098 gb-tuned-l:0.042 gb-tuned-xl:0.042 xgb-xl:0.131 xgb-l:0.131 mlp-adaptive-xl:0.041 mlp-l:0.053 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.2464, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5948
  Round 5/5: Mean predicted reward = 9.309

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 46 Results ---
  Mean Oracle Reward: 9.293
  Min Oracle Reward: 5.477
  Max Oracle Reward: 11.329
  Std Oracle Reward: 1.152
  Sequence Diversity: 0.906
  Models Used: 12
  Model R2 - Mean: 0.209, Max: 0.311, Count: 13
  Total Sequences Evaluated: 1522
    Oracle Count: 1472 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 47/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1522
  Performance plateaued, reducing LR to 0.000100

--- Round 47 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  TCGCCAAGTG
  TGCATAGGCC
  CAGACTGCTG
  GAGCCCTATG
  GTAGACGCTC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.598
  Max reward: 11.750
  With intrinsic bonuses: 9.579

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.2351, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4091

=== Surrogate Model Training ===
Total samples: 1554

Training on 1473 samples (removed 81 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1473
Current R2 threshold: 0.07
  rf-tuned-l: R2 = 0.281 (std: 0.068)
  rf-tuned-xl: R2 = 0.274 (std: 0.075)
  gb-tuned-l: R2 = 0.198 (std: 0.065)
  gb-tuned-xl: R2 = 0.198 (std: 0.065)
  xgb-xl: R2 = 0.299 (std: 0.084)
  xgb-l: R2 = 0.299 (std: 0.084)
  mlp-adaptive-xl: R2 = 0.187 (std: 0.025)
  mlp-l: R2 = 0.181 (std: 0.056)
  svr-rbf-xl: R2 = 0.320 (std: 0.064)
  svr-poly-l: R2 = 0.320 (std: 0.064)
  knn-tuned-sqrt: R2 = 0.121 (std: 0.078)
  knn-tuned-l: R2 = 0.121 (std: 0.078)
  ridge: R2 = 0.007 (std: 0.034)

Model-based training with 12 models
Best R2: 0.320, Mean R2: 0.216
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.100 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.042 mlp-l:0.039 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=64.3477, entropy=0.2249, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2827
  Round 1/5: Mean predicted reward = 5.545
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.100 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.042 mlp-l:0.039 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9671, entropy=0.1992, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3747
  Round 2/5: Mean predicted reward = 9.364
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.100 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.042 mlp-l:0.039 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9662, entropy=0.2238, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4131
  Round 3/5: Mean predicted reward = 9.246
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.100 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.042 mlp-l:0.039 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.2203, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5105
  Round 4/5: Mean predicted reward = 9.282
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.100 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.042 mlp-l:0.039 svr-rbf-xl:0.158 svr-poly-l:0.158 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9713, entropy=0.2174, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4989
  Round 5/5: Mean predicted reward = 9.298

  === Progress Analysis ===
  Status: NORMAL

--- Round 47 Results ---
  Mean Oracle Reward: 9.550
  Min Oracle Reward: 6.731
  Max Oracle Reward: 11.734
  Std Oracle Reward: 1.005
  Sequence Diversity: 0.906
  Models Used: 12
  Model R2 - Mean: 0.216, Max: 0.320, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 1554
    Oracle Count: 1504 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 48/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1554
  Consistent improvement, increasing LR to 0.000132

--- Round 48 Configuration ---
Learning rate: 0.000132
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  TCCGTGCAAG
  GCTCGAGCAT
  CTACGAGCGT
  CTAGCGGAGC
  CGACACGGGT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.078
  Max reward: 10.705
  With intrinsic bonuses: 9.077

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9716, entropy=0.2057, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5462

=== Surrogate Model Training ===
Total samples: 1586

Training on 1505 samples (removed 81 outliers)
Reward range: [6.57, 11.68], mean: 9.21
  Created 13 candidate models for data size 1505
Current R2 threshold: 0.08000000000000002
  rf-tuned-l: R2 = 0.275 (std: 0.053)
  rf-tuned-xl: R2 = 0.282 (std: 0.050)
  gb-tuned-l: R2 = 0.203 (std: 0.062)
  gb-tuned-xl: R2 = 0.203 (std: 0.062)
  xgb-xl: R2 = 0.291 (std: 0.061)
  xgb-l: R2 = 0.291 (std: 0.061)
  mlp-adaptive-xl: R2 = 0.165 (std: 0.064)
  mlp-l: R2 = 0.156 (std: 0.048)
  svr-rbf-xl: R2 = 0.315 (std: 0.063)
  svr-poly-l: R2 = 0.315 (std: 0.063)
  knn-tuned-sqrt: R2 = 0.113 (std: 0.074)
  knn-tuned-l: R2 = 0.113 (std: 0.074)
  ridge: R2 = 0.007 (std: 0.032)

Model-based training with 12 models
Best R2: 0.315, Mean R2: 0.210
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.115 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.035 mlp-l:0.032 svr-rbf-xl:0.159 svr-poly-l:0.159 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=305.0821, entropy=0.1642, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3121
  Round 1/5: Mean predicted reward = 4.931
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.115 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.035 mlp-l:0.032 svr-rbf-xl:0.159 svr-poly-l:0.159 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9666, entropy=0.2050, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4060
  Round 2/5: Mean predicted reward = 9.334
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.115 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.035 mlp-l:0.032 svr-rbf-xl:0.159 svr-poly-l:0.159 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9673, entropy=0.1902, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.5446
  Round 3/5: Mean predicted reward = 9.273
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.115 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.035 mlp-l:0.032 svr-rbf-xl:0.159 svr-poly-l:0.159 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9702, entropy=0.1846, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4719
  Round 4/5: Mean predicted reward = 9.306
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.115 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.035 mlp-l:0.032 svr-rbf-xl:0.159 svr-poly-l:0.159 knn-tuned-sqrt:0.021 knn-tuned-l:0.021 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9676, entropy=0.2147, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.6696
  Round 5/5: Mean predicted reward = 9.349

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 48 Results ---
  Mean Oracle Reward: 9.062
  Min Oracle Reward: 5.410
  Max Oracle Reward: 10.569
  Std Oracle Reward: 1.150
  Sequence Diversity: 0.938
  Models Used: 12
  Model R2 - Mean: 0.210, Max: 0.315, Count: 13
  Total Sequences Evaluated: 1586
    Oracle Count: 1536 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 49/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1586

--- Round 49 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  GCGGTAGCAC
  GAAGTCCCGG
  TTAGAGGCCA
  CTCGGCATAG
  AGCTCCAGGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.002
  Max reward: 11.023
  With intrinsic bonuses: 9.039

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9706, entropy=0.1694, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1211

=== Surrogate Model Training ===
Total samples: 1618

Training on 1532 samples (removed 86 outliers)
Reward range: [6.60, 11.68], mean: 9.22
  Created 13 candidate models for data size 1532
Current R2 threshold: 0.09000000000000002
  rf-tuned-l: R2 = 0.261 (std: 0.053)
  rf-tuned-xl: R2 = 0.268 (std: 0.045)
  gb-tuned-l: R2 = 0.196 (std: 0.040)
  gb-tuned-xl: R2 = 0.196 (std: 0.040)
  xgb-xl: R2 = 0.247 (std: 0.050)
  xgb-l: R2 = 0.247 (std: 0.050)
  mlp-adaptive-xl: R2 = 0.194 (std: 0.062)
  mlp-l: R2 = 0.159 (std: 0.078)
  svr-rbf-xl: R2 = 0.307 (std: 0.056)
  svr-poly-l: R2 = 0.307 (std: 0.056)
  knn-tuned-sqrt: R2 = 0.105 (std: 0.065)
  knn-tuned-l: R2 = 0.105 (std: 0.065)
  ridge: R2 = 0.008 (std: 0.037)

Model-based training with 12 models
Best R2: 0.307, Mean R2: 0.200
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.115 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.094 xgb-l:0.094 mlp-adaptive-xl:0.055 mlp-l:0.039 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=55.9767, entropy=0.2048, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0555
  Round 1/5: Mean predicted reward = 6.042
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.115 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.094 xgb-l:0.094 mlp-adaptive-xl:0.055 mlp-l:0.039 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1908, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0813
  Round 2/5: Mean predicted reward = 9.264
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.115 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.094 xgb-l:0.094 mlp-adaptive-xl:0.055 mlp-l:0.039 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9685, entropy=0.1967, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1170
  Round 3/5: Mean predicted reward = 9.307
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.115 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.094 xgb-l:0.094 mlp-adaptive-xl:0.055 mlp-l:0.039 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9673, entropy=0.2031, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1241
  Round 4/5: Mean predicted reward = 9.281
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.115 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.094 xgb-l:0.094 mlp-adaptive-xl:0.055 mlp-l:0.039 svr-rbf-xl:0.170 svr-poly-l:0.170 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1908, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1447
  Round 5/5: Mean predicted reward = 9.401

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 49 Results ---
  Mean Oracle Reward: 8.978
  Min Oracle Reward: 4.133
  Max Oracle Reward: 11.144
  Std Oracle Reward: 1.277
  Sequence Diversity: 0.969
  Models Used: 12
  Model R2 - Mean: 0.200, Max: 0.307, Count: 13
  Total Sequences Evaluated: 1618
    Oracle Count: 1568 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 50/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1618

--- Round 50 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  ATCCGGCGAT
  CGAGACTTGC
  GTCTGGCCAA
  GGTACACCGG
  GGTGTACCCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.939
  Max reward: 11.086
  With intrinsic bonuses: 8.948

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9734, entropy=0.1861, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9754

=== Surrogate Model Training ===
Total samples: 1650

Training on 1562 samples (removed 88 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1562
Current R2 threshold: 0.10000000000000003
  rf-tuned-l: R2 = 0.278 (std: 0.050)
  rf-tuned-xl: R2 = 0.276 (std: 0.039)
  gb-tuned-l: R2 = 0.207 (std: 0.055)
  gb-tuned-xl: R2 = 0.207 (std: 0.055)
  xgb-xl: R2 = 0.271 (std: 0.044)
  xgb-l: R2 = 0.271 (std: 0.044)
  mlp-adaptive-xl: R2 = 0.196 (std: 0.034)
  mlp-l: R2 = 0.205 (std: 0.058)
  svr-rbf-xl: R2 = 0.312 (std: 0.041)
  svr-poly-l: R2 = 0.312 (std: 0.041)
  knn-tuned-sqrt: R2 = 0.126 (std: 0.053)
  knn-tuned-l: R2 = 0.126 (std: 0.053)
  ridge: R2 = 0.014 (std: 0.035)

Model-based training with 12 models
Best R2: 0.312, Mean R2: 0.215
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.109 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.049 mlp-l:0.054 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=65.3973, entropy=0.1810, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4092
  Round 1/5: Mean predicted reward = 3.799
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.109 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.049 mlp-l:0.054 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1893, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.8544
  Round 2/5: Mean predicted reward = 9.339
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.109 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.049 mlp-l:0.054 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9685, entropy=0.1699, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.9394
  Round 3/5: Mean predicted reward = 9.405
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.109 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.049 mlp-l:0.054 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9673, entropy=0.1832, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2178
  Round 4/5: Mean predicted reward = 9.497
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.109 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.049 mlp-l:0.054 svr-rbf-xl:0.156 svr-poly-l:0.156 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9670, entropy=0.1716, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 1.2242
  Round 5/5: Mean predicted reward = 9.254

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 50 Results ---
  Mean Oracle Reward: 8.945
  Min Oracle Reward: 4.659
  Max Oracle Reward: 11.216
  Std Oracle Reward: 1.402
  Sequence Diversity: 0.875
  Models Used: 12
  Model R2 - Mean: 0.215, Max: 0.312, Count: 13
  Total Sequences Evaluated: 1650
    Oracle Count: 1600 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 51/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1650
  Performance plateaued, reducing LR to 0.000136

--- Round 51 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  CTTCGGCGAA
  CTGCGGAACT
  GGTGACCCGA
  ATCGCGAGTA
  ATAGCGTATC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.101
  Max reward: 11.104
  With intrinsic bonuses: 9.072

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9731, entropy=0.1701, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1767

=== Surrogate Model Training ===
Total samples: 1682

Training on 1595 samples (removed 87 outliers)
Reward range: [6.57, 11.68], mean: 9.22
  Created 13 candidate models for data size 1595
Current R2 threshold: 0.11000000000000004
  rf-tuned-l: R2 = 0.256 (std: 0.035)
  rf-tuned-xl: R2 = 0.268 (std: 0.034)
  gb-tuned-l: R2 = 0.193 (std: 0.037)
  gb-tuned-xl: R2 = 0.193 (std: 0.037)
  xgb-xl: R2 = 0.266 (std: 0.052)
  xgb-l: R2 = 0.266 (std: 0.052)
  mlp-adaptive-xl: R2 = 0.225 (std: 0.057)
  mlp-l: R2 = 0.206 (std: 0.056)
  svr-rbf-xl: R2 = 0.302 (std: 0.037)
  svr-poly-l: R2 = 0.302 (std: 0.037)
  knn-tuned-sqrt: R2 = 0.111 (std: 0.047)
  knn-tuned-l: R2 = 0.111 (std: 0.047)
  ridge: R2 = 0.011 (std: 0.028)

Model-based training with 12 models
Best R2: 0.302, Mean R2: 0.209
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.109 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.071 mlp-l:0.059 svr-rbf-xl:0.153 svr-poly-l:0.153 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=64.0517, entropy=0.1606, kl_div=0.0000
    Epoch 1: policy_loss=0.0288, value_loss=64.0512, entropy=0.1599, kl_div=0.0497
  Round 1/5: Mean predicted reward = 5.802
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.109 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.071 mlp-l:0.059 svr-rbf-xl:0.153 svr-poly-l:0.153 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.1709, kl_div=0.0000
    Epoch 1: policy_loss=-0.0481, value_loss=0.9683, entropy=0.1705, kl_div=-0.0450
  Round 2/5: Mean predicted reward = 9.227
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.109 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.071 mlp-l:0.059 svr-rbf-xl:0.153 svr-poly-l:0.153 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.1502, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1148
  Round 3/5: Mean predicted reward = 9.167
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.109 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.071 mlp-l:0.059 svr-rbf-xl:0.153 svr-poly-l:0.153 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9682, entropy=0.1470, kl_div=0.0000
    Epoch 1: policy_loss=-0.0641, value_loss=0.9682, entropy=0.1446, kl_div=0.0108
  Round 4/5: Mean predicted reward = 9.431
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.109 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.071 mlp-l:0.059 svr-rbf-xl:0.153 svr-poly-l:0.153 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9674, entropy=0.1539, kl_div=0.0000
    Epoch 1: policy_loss=-0.0482, value_loss=0.9674, entropy=0.1531, kl_div=-0.0689
  Round 5/5: Mean predicted reward = 9.242

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 51 Results ---
  Mean Oracle Reward: 9.112
  Min Oracle Reward: 5.573
  Max Oracle Reward: 11.094
  Std Oracle Reward: 1.171
  Sequence Diversity: 0.969
  Models Used: 12
  Model R2 - Mean: 0.209, Max: 0.302, Count: 13
  Total Sequences Evaluated: 1682
    Oracle Count: 1632 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 52/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1682
  Performance plateaued, reducing LR to 0.000100

--- Round 52 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  ATGCACCGTG
  CTCGACGGAG
  CCATCGGGGA
  AGCTATCAGG
  GCAGAGATCT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.027
  Max reward: 10.445
  With intrinsic bonuses: 9.063

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9717, entropy=0.1586, kl_div=0.0000
    Epoch 1: policy_loss=-0.0322, value_loss=0.9717, entropy=0.1576, kl_div=-0.0627

=== Surrogate Model Training ===
Total samples: 1714

Training on 1623 samples (removed 91 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1623
Current R2 threshold: 0.12
  rf-tuned-l: R2 = 0.240 (std: 0.055)
  rf-tuned-xl: R2 = 0.246 (std: 0.048)
  gb-tuned-l: R2 = 0.186 (std: 0.040)
  gb-tuned-xl: R2 = 0.186 (std: 0.040)
  xgb-xl: R2 = 0.259 (std: 0.082)
  xgb-l: R2 = 0.259 (std: 0.082)
  mlp-adaptive-xl: R2 = 0.187 (std: 0.061)
  mlp-l: R2 = 0.154 (std: 0.079)
  svr-rbf-xl: R2 = 0.287 (std: 0.048)
  svr-poly-l: R2 = 0.287 (std: 0.048)
  knn-tuned-sqrt: R2 = 0.103 (std: 0.042)
  knn-tuned-l: R2 = 0.103 (std: 0.042)
  ridge: R2 = 0.009 (std: 0.028)

Model-based training with 10 models
Best R2: 0.287, Mean R2: 0.193
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.107 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.060 mlp-l:0.043 svr-rbf-xl:0.163 svr-poly-l:0.163 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=233.8374, entropy=0.1464, kl_div=0.0000
    Epoch 1: policy_loss=-0.0100, value_loss=233.8362, entropy=0.1461, kl_div=-0.1912
  Round 1/3: Mean predicted reward = 4.671
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.107 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.060 mlp-l:0.043 svr-rbf-xl:0.163 svr-poly-l:0.163 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9670, entropy=0.1518, kl_div=0.0000
    Epoch 1: policy_loss=-0.0346, value_loss=0.9670, entropy=0.1502, kl_div=0.0121
  Round 2/3: Mean predicted reward = 9.195
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.107 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.060 mlp-l:0.043 svr-rbf-xl:0.163 svr-poly-l:0.163 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.1336, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1228
  Round 3/3: Mean predicted reward = 9.364

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 52 Results ---
  Mean Oracle Reward: 9.055
  Min Oracle Reward: 3.879
  Max Oracle Reward: 10.517
  Std Oracle Reward: 1.230
  Sequence Diversity: 0.906
  Models Used: 10
  Model R2 - Mean: 0.193, Max: 0.287, Count: 13
  Total Sequences Evaluated: 1714
    Oracle Count: 1664 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 53/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1714
  Performance plateaued, reducing LR to 0.000055

--- Round 53 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  CAGGCGTTCA
  GCTGCCAAGG
  GCGCAGTCTA
  CGGCGACATT
  CACGGTACTG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.022
  Max reward: 10.526
  With intrinsic bonuses: 8.988

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9703, entropy=0.1432, kl_div=0.0000
    Epoch 1: policy_loss=-0.0218, value_loss=0.9703, entropy=0.1430, kl_div=-0.0144

=== Surrogate Model Training ===
Total samples: 1746

Training on 1653 samples (removed 93 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1653
Current R2 threshold: 0.13
  rf-tuned-l: R2 = 0.224 (std: 0.088)
  rf-tuned-xl: R2 = 0.234 (std: 0.070)
  gb-tuned-l: R2 = 0.179 (std: 0.043)
  gb-tuned-xl: R2 = 0.179 (std: 0.043)
  xgb-xl: R2 = 0.251 (std: 0.083)
  xgb-l: R2 = 0.251 (std: 0.083)
  mlp-adaptive-xl: R2 = 0.147 (std: 0.079)
  mlp-l: R2 = 0.144 (std: 0.054)
  svr-rbf-xl: R2 = 0.283 (std: 0.059)
  svr-poly-l: R2 = 0.283 (std: 0.059)
  knn-tuned-sqrt: R2 = 0.096 (std: 0.063)
  knn-tuned-l: R2 = 0.096 (std: 0.063)
  ridge: R2 = 0.013 (std: 0.030)

Model-based training with 10 models
Best R2: 0.283, Mean R2: 0.183
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.105 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.125 xgb-l:0.125 mlp-adaptive-xl:0.044 mlp-l:0.043 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=171.4171, entropy=0.1381, kl_div=0.0000
    Epoch 1: policy_loss=-0.0307, value_loss=171.4165, entropy=0.1395, kl_div=-0.1274
  Round 1/3: Mean predicted reward = 2.375
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.105 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.125 xgb-l:0.125 mlp-adaptive-xl:0.044 mlp-l:0.043 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.1361, kl_div=0.0000
    Epoch 1: policy_loss=-0.0085, value_loss=0.9695, entropy=0.1383, kl_div=-0.1200
  Round 2/3: Mean predicted reward = 9.375
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.095 rf-tuned-xl:0.105 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.125 xgb-l:0.125 mlp-adaptive-xl:0.044 mlp-l:0.043 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.1361, kl_div=0.0000
    Epoch 1: policy_loss=-0.0099, value_loss=0.9686, entropy=0.1373, kl_div=-0.0223
  Round 3/3: Mean predicted reward = 9.278

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 53 Results ---
  Mean Oracle Reward: 9.000
  Min Oracle Reward: 4.947
  Max Oracle Reward: 10.764
  Std Oracle Reward: 1.193
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.183, Max: 0.283, Count: 13
  Total Sequences Evaluated: 1746
    Oracle Count: 1696 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 54/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1746
  Performance plateaued, reducing LR to 0.000019

--- Round 54 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.969) ---
  CATGTCGAAG
  GGGTCAAGCC
  TATACCCGGG
  GTCGCGGCAA
  TTAGGCAGCC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 8.638
  Max reward: 10.173
  With intrinsic bonuses: 8.679

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9701, entropy=0.1478, kl_div=0.0000
    Epoch 1: policy_loss=-0.0050, value_loss=0.9701, entropy=0.1479, kl_div=-0.0082

=== Surrogate Model Training ===
Total samples: 1778

Training on 1682 samples (removed 96 outliers)
Reward range: [6.58, 11.68], mean: 9.22
  Created 13 candidate models for data size 1682
Current R2 threshold: 0.14
  rf-tuned-l: R2 = 0.210 (std: 0.123)
  rf-tuned-xl: R2 = 0.210 (std: 0.116)
  gb-tuned-l: R2 = 0.174 (std: 0.037)
  gb-tuned-xl: R2 = 0.174 (std: 0.037)
  xgb-xl: R2 = 0.205 (std: 0.135)
  xgb-l: R2 = 0.205 (std: 0.135)
  mlp-adaptive-xl: R2 = 0.141 (std: 0.074)
  mlp-l: R2 = 0.125 (std: 0.123)
  svr-rbf-xl: R2 = 0.268 (std: 0.084)
  svr-poly-l: R2 = 0.268 (std: 0.084)
  knn-tuned-sqrt: R2 = 0.071 (std: 0.074)
  knn-tuned-l: R2 = 0.071 (std: 0.074)
  ridge: R2 = 0.012 (std: 0.028)

Model-based training with 9 models
Best R2: 0.268, Mean R2: 0.164
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.107 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.053 svr-rbf-xl:0.190 svr-poly-l:0.190 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=181.6546, entropy=0.1531, kl_div=0.0000
    Epoch 1: policy_loss=-0.0235, value_loss=181.6545, entropy=0.1537, kl_div=-0.0377
  Round 1/3: Mean predicted reward = 4.113
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.107 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.053 svr-rbf-xl:0.190 svr-poly-l:0.190 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9671, entropy=0.1267, kl_div=0.0000
    Epoch 1: policy_loss=0.0042, value_loss=0.9671, entropy=0.1271, kl_div=-0.0259
  Round 2/3: Mean predicted reward = 9.299
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.107 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.053 svr-rbf-xl:0.190 svr-poly-l:0.190 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.1451, kl_div=0.0000
    Epoch 1: policy_loss=-0.0103, value_loss=0.9687, entropy=0.1452, kl_div=-0.0090
  Round 3/3: Mean predicted reward = 9.401

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 54 Results ---
  Mean Oracle Reward: 8.652
  Min Oracle Reward: 4.951
  Max Oracle Reward: 10.336
  Std Oracle Reward: 1.410
  Sequence Diversity: 0.969
  Models Used: 9
  Model R2 - Mean: 0.164, Max: 0.268, Count: 13
  Total Sequences Evaluated: 1778
    Oracle Count: 1728 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 55/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1778

--- Round 55 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GTAACCGGCT
  TCTGGCAAAG
  AGCGTACGCG
  CTACAAGTGG
  GACTGGTCAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.127
  Max reward: 12.436
  With intrinsic bonuses: 9.107

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=0.1422, kl_div=0.0000
    Epoch 1: policy_loss=0.1588, value_loss=0.9694, entropy=0.1424, kl_div=-0.4038

=== Surrogate Model Training ===
Total samples: 1810

Training on 1712 samples (removed 98 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1712
Current R2 threshold: 0.15000000000000002
  rf-tuned-l: R2 = 0.204 (std: 0.134)
  rf-tuned-xl: R2 = 0.198 (std: 0.141)
  gb-tuned-l: R2 = 0.168 (std: 0.054)
  gb-tuned-xl: R2 = 0.168 (std: 0.054)
  xgb-xl: R2 = 0.216 (std: 0.150)
  xgb-l: R2 = 0.216 (std: 0.150)
  mlp-adaptive-xl: R2 = 0.117 (std: 0.123)
  mlp-l: R2 = 0.155 (std: 0.103)
  svr-rbf-xl: R2 = 0.252 (std: 0.109)
  svr-poly-l: R2 = 0.252 (std: 0.109)
  knn-tuned-sqrt: R2 = 0.066 (std: 0.084)
  knn-tuned-l: R2 = 0.066 (std: 0.084)
  ridge: R2 = 0.014 (std: 0.033)

Model-based training with 9 models
Best R2: 0.252, Mean R2: 0.161
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.100 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.119 xgb-l:0.119 mlp-l:0.065 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=554.0131, entropy=0.1498, kl_div=0.0000
    Epoch 1: policy_loss=0.0363, value_loss=554.0093, entropy=0.1544, kl_div=-0.3518
  Round 1/3: Mean predicted reward = 1.397
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.100 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.119 xgb-l:0.119 mlp-l:0.065 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.1424, kl_div=0.0000
    Epoch 1: policy_loss=-0.0074, value_loss=0.9683, entropy=0.1465, kl_div=-0.1799
  Round 2/3: Mean predicted reward = 9.425
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.100 gb-tuned-l:0.074 gb-tuned-xl:0.074 xgb-xl:0.119 xgb-l:0.119 mlp-l:0.065 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9722, entropy=0.1454, kl_div=0.0000
    Epoch 1: policy_loss=-0.0272, value_loss=0.9722, entropy=0.1515, kl_div=-0.1511
  Round 3/3: Mean predicted reward = 9.298

  === Progress Analysis ===
  Status: NORMAL

--- Round 55 Results ---
  Mean Oracle Reward: 9.102
  Min Oracle Reward: 6.851
  Max Oracle Reward: 12.286
  Std Oracle Reward: 1.063
  Sequence Diversity: 0.844
  Models Used: 9
  Model R2 - Mean: 0.161, Max: 0.252, Count: 13
  Total Sequences Evaluated: 1810
    Oracle Count: 1760 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 56/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1810

--- Round 56 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GATTGAGCAC
  CGATGCGATA
  AAGGTGCCCT
  GCGAATCCTG
  CGATGTACGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.053
  Max reward: 11.265
  With intrinsic bonuses: 9.115

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9715, entropy=0.1513, kl_div=0.0000
    Epoch 1: policy_loss=-0.0231, value_loss=0.9715, entropy=0.1535, kl_div=-0.1072

=== Surrogate Model Training ===
Total samples: 1842

Training on 1744 samples (removed 98 outliers)
Reward range: [6.59, 11.68], mean: 9.21
  Created 13 candidate models for data size 1744
Current R2 threshold: 0.16000000000000003
  rf-tuned-l: R2 = 0.200 (std: 0.146)
  rf-tuned-xl: R2 = 0.189 (std: 0.146)
  gb-tuned-l: R2 = 0.157 (std: 0.082)
  gb-tuned-xl: R2 = 0.157 (std: 0.082)
  xgb-xl: R2 = 0.192 (std: 0.198)
  xgb-l: R2 = 0.192 (std: 0.198)
  mlp-adaptive-xl: R2 = 0.101 (std: 0.139)
  mlp-l: R2 = 0.132 (std: 0.087)
  svr-rbf-xl: R2 = 0.239 (std: 0.125)
  svr-poly-l: R2 = 0.239 (std: 0.125)
  knn-tuned-sqrt: R2 = 0.061 (std: 0.097)
  knn-tuned-l: R2 = 0.061 (std: 0.097)
  ridge: R2 = 0.015 (std: 0.027)

Model-based training with 6 models
Best R2: 0.239, Mean R2: 0.149
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.134 xgb-xl:0.138 xgb-l:0.138 svr-rbf-xl:0.220 svr-poly-l:0.220 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=453.3382, entropy=0.1718, kl_div=0.0000
    Epoch 1: policy_loss=-0.0345, value_loss=453.3353, entropy=0.1731, kl_div=-0.2087
  Round 1/3: Mean predicted reward = 0.334
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.134 xgb-xl:0.138 xgb-l:0.138 svr-rbf-xl:0.220 svr-poly-l:0.220 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.1576, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.4402
  Round 2/3: Mean predicted reward = 9.405
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.134 xgb-xl:0.138 xgb-l:0.138 svr-rbf-xl:0.220 svr-poly-l:0.220 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9683, entropy=0.1795, kl_div=0.0000
    Epoch 1: policy_loss=-0.0718, value_loss=0.9683, entropy=0.1811, kl_div=0.0284
  Round 3/3: Mean predicted reward = 9.280

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 56 Results ---
  Mean Oracle Reward: 9.098
  Min Oracle Reward: 7.449
  Max Oracle Reward: 11.242
  Std Oracle Reward: 0.906
  Sequence Diversity: 0.844
  Models Used: 6
  Model R2 - Mean: 0.149, Max: 0.239, Count: 13
  Total Sequences Evaluated: 1842
    Oracle Count: 1792 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 57/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 1842

--- Round 57 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  ATCACGTGCG
  GACCTAGGCG
  GCATCCAGGT
  GGCCAGAGTC
  CTGGAGGCCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.327
  Max reward: 11.230
  With intrinsic bonuses: 9.310

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9707, entropy=0.1580, kl_div=0.0000
    Epoch 1: policy_loss=0.0144, value_loss=0.9707, entropy=0.1611, kl_div=-0.2034

=== Surrogate Model Training ===
Total samples: 1874

Training on 1774 samples (removed 100 outliers)
Reward range: [6.59, 11.68], mean: 9.22
  Created 13 candidate models for data size 1774
Current R2 threshold: 0.17000000000000004
  rf-tuned-l: R2 = 0.196 (std: 0.160)
  rf-tuned-xl: R2 = 0.186 (std: 0.169)
  gb-tuned-l: R2 = 0.164 (std: 0.088)
  gb-tuned-xl: R2 = 0.164 (std: 0.088)
  xgb-xl: R2 = 0.193 (std: 0.206)
  xgb-l: R2 = 0.193 (std: 0.206)
  mlp-adaptive-xl: R2 = 0.115 (std: 0.139)
  mlp-l: R2 = 0.137 (std: 0.160)
  svr-rbf-xl: R2 = 0.232 (std: 0.138)
  svr-poly-l: R2 = 0.232 (std: 0.138)
  knn-tuned-sqrt: R2 = 0.059 (std: 0.092)
  knn-tuned-l: R2 = 0.059 (std: 0.092)
  ridge: R2 = 0.018 (std: 0.026)

Model-based training with 6 models
Best R2: 0.232, Mean R2: 0.150
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.135 xgb-xl:0.144 xgb-l:0.144 svr-rbf-xl:0.213 svr-poly-l:0.213 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=766.1489, entropy=0.1489, kl_div=0.0000
    Epoch 1: policy_loss=0.0013, value_loss=766.1458, entropy=0.1523, kl_div=-0.2166
  Round 1/3: Mean predicted reward = 0.170
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.135 xgb-xl:0.144 xgb-l:0.144 svr-rbf-xl:0.213 svr-poly-l:0.213 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9709, entropy=0.1647, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1214
  Round 2/3: Mean predicted reward = 9.474
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.149 rf-tuned-xl:0.135 xgb-xl:0.144 xgb-l:0.144 svr-rbf-xl:0.213 svr-poly-l:0.213 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9701, entropy=0.1509, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0973
  Round 3/3: Mean predicted reward = 9.456

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 57 Results ---
  Mean Oracle Reward: 9.283
  Min Oracle Reward: 5.793
  Max Oracle Reward: 11.105
  Std Oracle Reward: 1.268
  Sequence Diversity: 0.938
  Models Used: 6
  Model R2 - Mean: 0.150, Max: 0.232, Count: 13
  Total Sequences Evaluated: 1874
    Oracle Count: 1824 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 58/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 1874

--- Round 58 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GCCGATGTCA
  GCACCTGGAT
  GTCCGAGCAG
  AGACCATTGT
  GCGACTACGT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.168
  Max reward: 10.645
  With intrinsic bonuses: 9.229

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9708, entropy=0.1384, kl_div=0.0000
    Epoch 1: policy_loss=-0.0404, value_loss=0.9708, entropy=0.1386, kl_div=0.0070

=== Surrogate Model Training ===
Total samples: 1906

Training on 1806 samples (removed 100 outliers)
Reward range: [6.57, 11.68], mean: 9.22
  Created 13 candidate models for data size 1806
Current R2 threshold: 0.18
  rf-tuned-l: R2 = 0.204 (std: 0.144)
  rf-tuned-xl: R2 = 0.194 (std: 0.147)
  gb-tuned-l: R2 = 0.174 (std: 0.072)
  gb-tuned-xl: R2 = 0.174 (std: 0.072)
  xgb-xl: R2 = 0.185 (std: 0.176)
  xgb-l: R2 = 0.185 (std: 0.176)
  mlp-adaptive-xl: R2 = 0.151 (std: 0.102)
  mlp-l: R2 = 0.127 (std: 0.128)
  svr-rbf-xl: R2 = 0.241 (std: 0.124)
  svr-poly-l: R2 = 0.241 (std: 0.124)
  knn-tuned-sqrt: R2 = 0.062 (std: 0.073)
  knn-tuned-l: R2 = 0.062 (std: 0.073)
  ridge: R2 = 0.018 (std: 0.028)

Model-based training with 6 models
Best R2: 0.241, Mean R2: 0.155
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.155 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 svr-rbf-xl:0.224 svr-poly-l:0.224 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=759.5947, entropy=0.1714, kl_div=0.0000
    Epoch 1: policy_loss=-0.0257, value_loss=759.5929, entropy=0.1744, kl_div=-0.2075
  Round 1/3: Mean predicted reward = 0.502
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.155 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 svr-rbf-xl:0.224 svr-poly-l:0.224 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9702, entropy=0.1604, kl_div=0.0000
    Epoch 1: policy_loss=-0.0280, value_loss=0.9702, entropy=0.1610, kl_div=-0.0107
  Round 2/3: Mean predicted reward = 9.383
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.155 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 svr-rbf-xl:0.224 svr-poly-l:0.224 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1628, kl_div=0.0000
    Epoch 1: policy_loss=-0.0387, value_loss=0.9691, entropy=0.1634, kl_div=-0.0658
  Round 3/3: Mean predicted reward = 9.235

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 58 Results ---
  Mean Oracle Reward: 9.183
  Min Oracle Reward: 5.644
  Max Oracle Reward: 10.603
  Std Oracle Reward: 1.307
  Sequence Diversity: 0.844
  Models Used: 6
  Model R2 - Mean: 0.155, Max: 0.241, Count: 13
  Total Sequences Evaluated: 1906
    Oracle Count: 1856 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 59/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 1906

--- Round 59 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  AGTCCGCATG
  CTCAGCGGTA
  GACCGCTGGA
  ACAGGGTCCG
  CGTGCGGACA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.149
  Max reward: 10.670
  With intrinsic bonuses: 9.164

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9732, entropy=0.1745, kl_div=0.0000
    Epoch 1: policy_loss=-0.0230, value_loss=0.9732, entropy=0.1755, kl_div=-0.0549

=== Surrogate Model Training ===
Total samples: 1938

Training on 1833 samples (removed 105 outliers)
Reward range: [6.59, 11.68], mean: 9.23
  Created 13 candidate models for data size 1833
Current R2 threshold: 0.19
  rf-tuned-l: R2 = 0.214 (std: 0.156)
  rf-tuned-xl: R2 = 0.210 (std: 0.179)
  gb-tuned-l: R2 = 0.166 (std: 0.096)
  gb-tuned-xl: R2 = 0.166 (std: 0.096)
  xgb-xl: R2 = 0.196 (std: 0.209)
  xgb-l: R2 = 0.196 (std: 0.209)
  mlp-adaptive-xl: R2 = 0.161 (std: 0.137)
  mlp-l: R2 = 0.144 (std: 0.110)
  svr-rbf-xl: R2 = 0.240 (std: 0.138)
  svr-poly-l: R2 = 0.240 (std: 0.138)
  knn-tuned-sqrt: R2 = 0.063 (std: 0.094)
  knn-tuned-l: R2 = 0.063 (std: 0.094)
  ridge: R2 = 0.018 (std: 0.025)

Model-based training with 6 models
Best R2: 0.240, Mean R2: 0.160
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.161 rf-tuned-xl:0.155 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.208 svr-poly-l:0.208 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=670.9899, entropy=0.1745, kl_div=0.0000
    Epoch 1: policy_loss=-0.0472, value_loss=670.9890, entropy=0.1757, kl_div=-0.1317
  Round 1/3: Mean predicted reward = -1.014
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.161 rf-tuned-xl:0.155 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.208 svr-poly-l:0.208 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9674, entropy=0.1554, kl_div=0.0000
    Epoch 1: policy_loss=0.0159, value_loss=0.9674, entropy=0.1563, kl_div=-0.1075
  Round 2/3: Mean predicted reward = 9.339
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.161 rf-tuned-xl:0.155 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.208 svr-poly-l:0.208 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9677, entropy=0.1674, kl_div=0.0000
    Epoch 1: policy_loss=-0.0099, value_loss=0.9677, entropy=0.1678, kl_div=-0.0051
  Round 3/3: Mean predicted reward = 9.331

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 59 Results ---
  Mean Oracle Reward: 9.123
  Min Oracle Reward: 5.273
  Max Oracle Reward: 10.611
  Std Oracle Reward: 1.215
  Sequence Diversity: 0.938
  Models Used: 6
  Model R2 - Mean: 0.160, Max: 0.240, Count: 13
  Total Sequences Evaluated: 1938
    Oracle Count: 1888 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 60/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 1938
  Performance plateaued, reducing LR to 0.000150

--- Round 60 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  GAGGACTGCC
  GGAAGCTCTA
  ATATGGCCGC
  GCGGCAATCT
  GCGTTCAAAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.243
  Max reward: 11.641
  With intrinsic bonuses: 9.230

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9713, entropy=0.1644, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1715

=== Surrogate Model Training ===
Total samples: 1970

Training on 1862 samples (removed 108 outliers)
Reward range: [6.64, 11.68], mean: 9.23
  Created 13 candidate models for data size 1862
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.223 (std: 0.131)
  rf-tuned-xl: R2 = 0.219 (std: 0.132)
  gb-tuned-l: R2 = 0.174 (std: 0.073)
  gb-tuned-xl: R2 = 0.174 (std: 0.073)
  xgb-xl: R2 = 0.214 (std: 0.136)
  xgb-l: R2 = 0.214 (std: 0.136)
  mlp-adaptive-xl: R2 = 0.151 (std: 0.085)
  mlp-l: R2 = 0.155 (std: 0.118)
  svr-rbf-xl: R2 = 0.255 (std: 0.114)
  svr-poly-l: R2 = 0.255 (std: 0.114)
  knn-tuned-sqrt: R2 = 0.076 (std: 0.086)
  knn-tuned-l: R2 = 0.076 (std: 0.086)
  ridge: R2 = 0.015 (std: 0.029)

Model-based training with 6 models
Best R2: 0.255, Mean R2: 0.169
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.147 xgb-xl:0.139 xgb-l:0.139 svr-rbf-xl:0.211 svr-poly-l:0.211 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=585.3860, entropy=0.1628, kl_div=0.0000
    Epoch 1: policy_loss=-0.0190, value_loss=585.3835, entropy=0.1630, kl_div=-0.0518
  Round 1/3: Mean predicted reward = -2.011
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.147 xgb-xl:0.139 xgb-l:0.139 svr-rbf-xl:0.211 svr-poly-l:0.211 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9696, entropy=0.1536, kl_div=0.0000
    Epoch 1: policy_loss=-0.0147, value_loss=0.9696, entropy=0.1537, kl_div=0.0087
  Round 2/3: Mean predicted reward = 9.114
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.153 rf-tuned-xl:0.147 xgb-xl:0.139 xgb-l:0.139 svr-rbf-xl:0.211 svr-poly-l:0.211 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9699, entropy=0.1614, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3416
  Round 3/3: Mean predicted reward = 9.496

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 60 Results ---
  Mean Oracle Reward: 9.224
  Min Oracle Reward: 6.219
  Max Oracle Reward: 11.652
  Std Oracle Reward: 1.175
  Sequence Diversity: 0.906
  Models Used: 6
  Model R2 - Mean: 0.169, Max: 0.255, Count: 13
  Total Sequences Evaluated: 1970
    Oracle Count: 1920 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 61/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 1970
  Performance plateaued, reducing LR to 0.000136

--- Round 61 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  TTAGGCCAGA
  TCTCAGGCGA
  GTCACGAGTA
  CGAGTCATAG
  TCGCAGTGCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.532
  Max reward: 11.377
  With intrinsic bonuses: 9.519

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9709, entropy=0.1615, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3656

=== Surrogate Model Training ===
Total samples: 2002

Training on 1893 samples (removed 109 outliers)
Reward range: [6.64, 11.68], mean: 9.24
  Created 13 candidate models for data size 1893
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.239 (std: 0.106)
  rf-tuned-xl: R2 = 0.244 (std: 0.105)
  gb-tuned-l: R2 = 0.188 (std: 0.067)
  gb-tuned-xl: R2 = 0.188 (std: 0.067)
  xgb-xl: R2 = 0.252 (std: 0.121)
  xgb-l: R2 = 0.252 (std: 0.121)
  mlp-adaptive-xl: R2 = 0.166 (std: 0.121)
  mlp-l: R2 = 0.160 (std: 0.129)
  svr-rbf-xl: R2 = 0.267 (std: 0.096)
  svr-poly-l: R2 = 0.267 (std: 0.096)
  knn-tuned-sqrt: R2 = 0.086 (std: 0.083)
  knn-tuned-l: R2 = 0.086 (std: 0.083)
  ridge: R2 = 0.016 (std: 0.028)

Model-based training with 6 models
Best R2: 0.267, Mean R2: 0.185
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.144 rf-tuned-xl:0.150 xgb-xl:0.164 xgb-l:0.164 svr-rbf-xl:0.189 svr-poly-l:0.189 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=722.8802, entropy=0.1757, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0938
  Round 1/3: Mean predicted reward = -1.529
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.144 rf-tuned-xl:0.150 xgb-xl:0.164 xgb-l:0.164 svr-rbf-xl:0.189 svr-poly-l:0.189 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9699, entropy=0.1628, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2271
  Round 2/3: Mean predicted reward = 9.405
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.144 rf-tuned-xl:0.150 xgb-xl:0.164 xgb-l:0.164 svr-rbf-xl:0.189 svr-poly-l:0.189 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.1651, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3188
  Round 3/3: Mean predicted reward = 9.431

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 61 Results ---
  Mean Oracle Reward: 9.554
  Min Oracle Reward: 6.586
  Max Oracle Reward: 11.479
  Std Oracle Reward: 0.898
  Sequence Diversity: 0.844
  Models Used: 6
  Model R2 - Mean: 0.185, Max: 0.267, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2002
    Oracle Count: 1952 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 62/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2002
  Consistent improvement, increasing LR to 0.000240

--- Round 62 Configuration ---
Learning rate: 0.000240
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.938) ---
  ATGACTGCGC
  CCGATTACGG
  GAGCTCCGGA
  CAAGGCTGCT
  AGACTCGGTA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.430
  Max reward: 10.479
  With intrinsic bonuses: 9.417

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9672, entropy=0.1737, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3861

=== Surrogate Model Training ===
Total samples: 2034

Training on 1925 samples (removed 109 outliers)
Reward range: [6.64, 11.68], mean: 9.24
  Created 13 candidate models for data size 1925
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.247 (std: 0.079)
  rf-tuned-xl: R2 = 0.256 (std: 0.070)
  gb-tuned-l: R2 = 0.189 (std: 0.058)
  gb-tuned-xl: R2 = 0.189 (std: 0.058)
  xgb-xl: R2 = 0.244 (std: 0.071)
  xgb-l: R2 = 0.244 (std: 0.071)
  mlp-adaptive-xl: R2 = 0.181 (std: 0.108)
  mlp-l: R2 = 0.171 (std: 0.056)
  svr-rbf-xl: R2 = 0.279 (std: 0.075)
  svr-poly-l: R2 = 0.279 (std: 0.075)
  knn-tuned-sqrt: R2 = 0.097 (std: 0.071)
  knn-tuned-l: R2 = 0.097 (std: 0.071)
  ridge: R2 = 0.020 (std: 0.031)

Model-based training with 6 models
Best R2: 0.279, Mean R2: 0.192
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.160 xgb-xl:0.143 xgb-l:0.143 svr-rbf-xl:0.204 svr-poly-l:0.204 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=565.1430, entropy=0.1567, kl_div=0.0000
    Epoch 1: policy_loss=0.0026, value_loss=565.1401, entropy=0.1571, kl_div=0.0248
  Round 1/3: Mean predicted reward = -2.541
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.160 xgb-xl:0.143 xgb-l:0.143 svr-rbf-xl:0.204 svr-poly-l:0.204 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9674, entropy=0.1438, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0880
  Round 2/3: Mean predicted reward = 9.477
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.147 rf-tuned-xl:0.160 xgb-xl:0.143 xgb-l:0.143 svr-rbf-xl:0.204 svr-poly-l:0.204 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9706, entropy=0.1546, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2449
  Round 3/3: Mean predicted reward = 9.351

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 62 Results ---
  Mean Oracle Reward: 9.401
  Min Oracle Reward: 6.638
  Max Oracle Reward: 10.482
  Std Oracle Reward: 0.827
  Sequence Diversity: 0.938
  Models Used: 6
  Model R2 - Mean: 0.192, Max: 0.279, Count: 13
  Total Sequences Evaluated: 2034
    Oracle Count: 1984 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 63/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2034

--- Round 63 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  AATCGGGACT
  GGCTACATCG
  AGTCACGGTA
  GAGCGACTCG
  TACGGCCGTA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.360
  Max reward: 11.838
  With intrinsic bonuses: 9.339

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9708, entropy=0.1749, kl_div=0.0000
    Epoch 1: policy_loss=-0.0260, value_loss=0.9707, entropy=0.1740, kl_div=-0.0381

=== Surrogate Model Training ===
Total samples: 2066

Training on 1956 samples (removed 110 outliers)
Reward range: [6.64, 11.68], mean: 9.25
  Created 13 candidate models for data size 1956
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.277 (std: 0.051)
  rf-tuned-xl: R2 = 0.263 (std: 0.064)
  gb-tuned-l: R2 = 0.192 (std: 0.050)
  gb-tuned-xl: R2 = 0.192 (std: 0.050)
  xgb-xl: R2 = 0.251 (std: 0.059)
  xgb-l: R2 = 0.251 (std: 0.059)
  mlp-adaptive-xl: R2 = 0.214 (std: 0.038)
  mlp-l: R2 = 0.204 (std: 0.030)
  svr-rbf-xl: R2 = 0.297 (std: 0.055)
  svr-poly-l: R2 = 0.297 (std: 0.055)
  knn-tuned-sqrt: R2 = 0.107 (std: 0.071)
  knn-tuned-l: R2 = 0.107 (std: 0.071)
  ridge: R2 = 0.022 (std: 0.034)

Model-based training with 8 models
Best R2: 0.297, Mean R2: 0.205
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.078 mlp-l:0.070 svr-rbf-xl:0.178 svr-poly-l:0.178 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2674.8833, entropy=0.1395, kl_div=0.0000
    Epoch 1: policy_loss=-0.0329, value_loss=2674.8809, entropy=0.1401, kl_div=-0.2126
  Round 1/3: Mean predicted reward = -4.019
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.078 mlp-l:0.070 svr-rbf-xl:0.178 svr-poly-l:0.178 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9704, entropy=0.1704, kl_div=0.0000
    Epoch 1: policy_loss=-0.0118, value_loss=0.9704, entropy=0.1706, kl_div=-0.0635
  Round 2/3: Mean predicted reward = 9.320
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.146 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.078 mlp-l:0.070 svr-rbf-xl:0.178 svr-poly-l:0.178 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=0.1626, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1508
  Round 3/3: Mean predicted reward = 9.280

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 63 Results ---
  Mean Oracle Reward: 9.330
  Min Oracle Reward: 6.116
  Max Oracle Reward: 11.592
  Std Oracle Reward: 1.100
  Sequence Diversity: 0.875
  Models Used: 8
  Model R2 - Mean: 0.205, Max: 0.297, Count: 13
  Total Sequences Evaluated: 2066
    Oracle Count: 2016 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 64/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2066
  Performance plateaued, reducing LR to 0.000019

--- Round 64 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  TCCCGGGAGA
  GATCACGCGT
  AGCGATCCGG
  CTGAGACTGC
  CGTACAGGCT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.470
  Max reward: 10.737
  With intrinsic bonuses: 9.489

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.1627, kl_div=0.0000
    Epoch 1: policy_loss=-0.0079, value_loss=0.9695, entropy=0.1623, kl_div=0.0144

=== Surrogate Model Training ===
Total samples: 2098

Training on 1988 samples (removed 110 outliers)
Reward range: [6.64, 11.80], mean: 9.25
  Created 13 candidate models for data size 1988
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.274 (std: 0.056)
  rf-tuned-xl: R2 = 0.277 (std: 0.051)
  gb-tuned-l: R2 = 0.198 (std: 0.040)
  gb-tuned-xl: R2 = 0.198 (std: 0.040)
  xgb-xl: R2 = 0.304 (std: 0.045)
  xgb-l: R2 = 0.304 (std: 0.045)
  mlp-adaptive-xl: R2 = 0.210 (std: 0.053)
  mlp-l: R2 = 0.210 (std: 0.024)
  svr-rbf-xl: R2 = 0.302 (std: 0.042)
  svr-poly-l: R2 = 0.302 (std: 0.042)
  knn-tuned-sqrt: R2 = 0.118 (std: 0.076)
  knn-tuned-l: R2 = 0.118 (std: 0.076)
  ridge: R2 = 0.024 (std: 0.034)

Model-based training with 8 models
Best R2: 0.304, Mean R2: 0.218
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.119 rf-tuned-xl:0.122 xgb-xl:0.160 xgb-l:0.160 mlp-adaptive-xl:0.063 mlp-l:0.063 svr-rbf-xl:0.157 svr-poly-l:0.157 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=1584.4014, entropy=0.1665, kl_div=0.0000
    Epoch 1: policy_loss=-0.0222, value_loss=1584.4010, entropy=0.1664, kl_div=-0.0373
  Round 1/5: Mean predicted reward = -2.552
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.119 rf-tuned-xl:0.122 xgb-xl:0.160 xgb-l:0.160 mlp-adaptive-xl:0.063 mlp-l:0.063 svr-rbf-xl:0.157 svr-poly-l:0.157 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9694, entropy=0.1645, kl_div=0.0000
    Epoch 1: policy_loss=0.0074, value_loss=0.9694, entropy=0.1644, kl_div=-0.0226
  Round 2/5: Mean predicted reward = 9.413
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.119 rf-tuned-xl:0.122 xgb-xl:0.160 xgb-l:0.160 mlp-adaptive-xl:0.063 mlp-l:0.063 svr-rbf-xl:0.157 svr-poly-l:0.157 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9698, entropy=0.1613, kl_div=0.0000
    Epoch 1: policy_loss=-0.0040, value_loss=0.9698, entropy=0.1611, kl_div=-0.0051
  Round 3/5: Mean predicted reward = 9.433
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.119 rf-tuned-xl:0.122 xgb-xl:0.160 xgb-l:0.160 mlp-adaptive-xl:0.063 mlp-l:0.063 svr-rbf-xl:0.157 svr-poly-l:0.157 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9712, entropy=0.1557, kl_div=0.0000
    Epoch 1: policy_loss=-0.0107, value_loss=0.9712, entropy=0.1554, kl_div=0.0024
  Round 4/5: Mean predicted reward = 9.310
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.119 rf-tuned-xl:0.122 xgb-xl:0.160 xgb-l:0.160 mlp-adaptive-xl:0.063 mlp-l:0.063 svr-rbf-xl:0.157 svr-poly-l:0.157 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9703, entropy=0.1580, kl_div=0.0000
    Epoch 1: policy_loss=-0.0123, value_loss=0.9703, entropy=0.1577, kl_div=0.0093
  Round 5/5: Mean predicted reward = 9.471

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 64 Results ---
  Mean Oracle Reward: 9.465
  Min Oracle Reward: 5.323
  Max Oracle Reward: 10.844
  Std Oracle Reward: 1.017
  Sequence Diversity: 0.844
  Models Used: 8
  Model R2 - Mean: 0.218, Max: 0.304, Count: 13
  Total Sequences Evaluated: 2098
    Oracle Count: 2048 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 65/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2098
  Performance plateaued, reducing LR to 0.000150

--- Round 65 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  CGCCAGTTAG
  AGTCCGTGAA
  CTCCTGGAAG
  TACCCGGGAG
  GGAGTCCAAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.395
  Max reward: 13.086
  With intrinsic bonuses: 9.428

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9682, entropy=0.1754, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3137

=== Surrogate Model Training ===
Total samples: 2130

Training on 2019 samples (removed 111 outliers)
Reward range: [6.60, 11.80], mean: 9.25
  Created 13 candidate models for data size 2019
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.278 (std: 0.058)
  rf-tuned-xl: R2 = 0.279 (std: 0.047)
  gb-tuned-l: R2 = 0.216 (std: 0.038)
  gb-tuned-xl: R2 = 0.216 (std: 0.038)
  xgb-xl: R2 = 0.284 (std: 0.046)
  xgb-l: R2 = 0.284 (std: 0.046)
  mlp-adaptive-xl: R2 = 0.191 (std: 0.063)
  mlp-l: R2 = 0.195 (std: 0.051)
  svr-rbf-xl: R2 = 0.307 (std: 0.033)
  svr-poly-l: R2 = 0.307 (std: 0.033)
  knn-tuned-sqrt: R2 = 0.121 (std: 0.056)
  knn-tuned-l: R2 = 0.121 (std: 0.056)
  ridge: R2 = 0.028 (std: 0.040)

Model-based training with 8 models
Best R2: 0.307, Mean R2: 0.217
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.127 rf-tuned-xl:0.128 gb-tuned-l:0.069 gb-tuned-xl:0.069 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=636.6334, entropy=0.1580, kl_div=0.0000
    Epoch 1: policy_loss=0.0485, value_loss=636.6320, entropy=0.1578, kl_div=-0.0810
  Round 1/5: Mean predicted reward = -2.986
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.127 rf-tuned-xl:0.128 gb-tuned-l:0.069 gb-tuned-xl:0.069 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.1661, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0516
  Round 2/5: Mean predicted reward = 9.352
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.127 rf-tuned-xl:0.128 gb-tuned-l:0.069 gb-tuned-xl:0.069 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9685, entropy=0.1806, kl_div=0.0000
    Epoch 1: policy_loss=0.0478, value_loss=0.9685, entropy=0.1811, kl_div=0.0148
  Round 3/5: Mean predicted reward = 9.392
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.127 rf-tuned-xl:0.128 gb-tuned-l:0.069 gb-tuned-xl:0.069 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9704, entropy=0.1325, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1785
  Round 4/5: Mean predicted reward = 9.272
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.127 rf-tuned-xl:0.128 gb-tuned-l:0.069 gb-tuned-xl:0.069 xgb-xl:0.134 xgb-l:0.134 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9674, entropy=0.1445, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3198
  Round 5/5: Mean predicted reward = 9.409

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 65 Results ---
  Mean Oracle Reward: 9.367
  Min Oracle Reward: 6.757
  Max Oracle Reward: 12.988
  Std Oracle Reward: 1.155
  Sequence Diversity: 0.844
  Models Used: 8
  Model R2 - Mean: 0.217, Max: 0.307, Count: 13
  Total Sequences Evaluated: 2130
    Oracle Count: 2080 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 66/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2130
  Performance plateaued, reducing LR to 0.000136

--- Round 66 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  AAAGTTCGGC
  TTCCGGACGA
  CCGTATAGCG
  ACGATTGCGA
  AATGCCGGAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.483
  Max reward: 11.624
  With intrinsic bonuses: 9.467

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9700, entropy=0.1535, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2414

=== Surrogate Model Training ===
Total samples: 2162

Training on 2049 samples (removed 113 outliers)
Reward range: [6.64, 11.80], mean: 9.26
  Created 13 candidate models for data size 2049
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.273 (std: 0.053)
  rf-tuned-xl: R2 = 0.284 (std: 0.062)
  gb-tuned-l: R2 = 0.205 (std: 0.034)
  gb-tuned-xl: R2 = 0.205 (std: 0.034)
  xgb-xl: R2 = 0.275 (std: 0.053)
  xgb-l: R2 = 0.275 (std: 0.053)
  mlp-adaptive-xl: R2 = 0.201 (std: 0.052)
  mlp-l: R2 = 0.213 (std: 0.025)
  svr-rbf-xl: R2 = 0.301 (std: 0.030)
  svr-poly-l: R2 = 0.301 (std: 0.030)
  knn-tuned-sqrt: R2 = 0.117 (std: 0.053)
  knn-tuned-l: R2 = 0.117 (std: 0.053)
  ridge: R2 = 0.022 (std: 0.037)

Model-based training with 10 models
Best R2: 0.301, Mean R2: 0.215
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.126 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.055 mlp-l:0.062 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1275.2777, entropy=0.1437, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1307
  Round 1/5: Mean predicted reward = -2.534
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.126 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.055 mlp-l:0.062 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9702, entropy=0.1566, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1017
  Round 2/5: Mean predicted reward = 9.294
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.126 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.055 mlp-l:0.062 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9701, entropy=0.1148, kl_div=0.0000
    Epoch 1: policy_loss=-0.0296, value_loss=0.9701, entropy=0.1155, kl_div=0.0294
  Round 3/5: Mean predicted reward = 9.168
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.126 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.055 mlp-l:0.062 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9708, entropy=0.1200, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1294
  Round 4/5: Mean predicted reward = 9.261
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.113 rf-tuned-xl:0.126 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.115 xgb-l:0.115 mlp-adaptive-xl:0.055 mlp-l:0.062 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9703, entropy=0.1212, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1926
  Round 5/5: Mean predicted reward = 9.410

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 66 Results ---
  Mean Oracle Reward: 9.457
  Min Oracle Reward: 6.590
  Max Oracle Reward: 11.416
  Std Oracle Reward: 0.927
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.215, Max: 0.301, Count: 13
  Total Sequences Evaluated: 2162
    Oracle Count: 2112 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 67/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2162
  Performance plateaued, reducing LR to 0.000100

--- Round 67 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  AAGCCTGCGT
  CGAGATGACT
  AGTTGAACGC
  GCGTACTGAA
  TATGCAGGCC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.334
  Max reward: 11.910
  With intrinsic bonuses: 9.258

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9713, entropy=0.1340, kl_div=0.0000
    Epoch 1: policy_loss=-0.0055, value_loss=0.9713, entropy=0.1340, kl_div=0.0215

=== Surrogate Model Training ===
Total samples: 2194

Training on 2077 samples (removed 117 outliers)
Reward range: [6.64, 11.80], mean: 9.26
  Created 13 candidate models for data size 2077
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.274 (std: 0.063)
  rf-tuned-xl: R2 = 0.277 (std: 0.061)
  gb-tuned-l: R2 = 0.210 (std: 0.042)
  gb-tuned-xl: R2 = 0.210 (std: 0.042)
  xgb-xl: R2 = 0.287 (std: 0.038)
  xgb-l: R2 = 0.287 (std: 0.038)
  mlp-adaptive-xl: R2 = 0.210 (std: 0.056)
  mlp-l: R2 = 0.230 (std: 0.033)
  svr-rbf-xl: R2 = 0.302 (std: 0.034)
  svr-poly-l: R2 = 0.302 (std: 0.034)
  knn-tuned-sqrt: R2 = 0.122 (std: 0.062)
  knn-tuned-l: R2 = 0.122 (std: 0.062)
  ridge: R2 = 0.019 (std: 0.037)

Model-based training with 10 models
Best R2: 0.302, Mean R2: 0.220
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.112 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.057 mlp-l:0.070 svr-rbf-xl:0.144 svr-poly-l:0.144 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1643.5586, entropy=0.1494, kl_div=0.0000
    Epoch 1: policy_loss=-0.0458, value_loss=1643.5571, entropy=0.1538, kl_div=-0.1300
  Round 1/5: Mean predicted reward = -6.069
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.112 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.057 mlp-l:0.070 svr-rbf-xl:0.144 svr-poly-l:0.144 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9707, entropy=0.1473, kl_div=0.0000
    Epoch 1: policy_loss=0.0036, value_loss=0.9707, entropy=0.1487, kl_div=-0.0351
  Round 2/5: Mean predicted reward = 9.286
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.112 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.057 mlp-l:0.070 svr-rbf-xl:0.144 svr-poly-l:0.144 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9699, entropy=0.1504, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0818
  Round 3/5: Mean predicted reward = 9.341
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.112 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.057 mlp-l:0.070 svr-rbf-xl:0.144 svr-poly-l:0.144 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9700, entropy=0.1539, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2080
  Round 4/5: Mean predicted reward = 9.506
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.112 gb-tuned-l:0.057 gb-tuned-xl:0.057 xgb-xl:0.124 xgb-l:0.124 mlp-adaptive-xl:0.057 mlp-l:0.070 svr-rbf-xl:0.144 svr-poly-l:0.144 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9704, entropy=0.1287, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1330
  Round 5/5: Mean predicted reward = 9.313

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 67 Results ---
  Mean Oracle Reward: 9.324
  Min Oracle Reward: 5.202
  Max Oracle Reward: 11.902
  Std Oracle Reward: 1.458
  Sequence Diversity: 0.906
  Models Used: 10
  Model R2 - Mean: 0.220, Max: 0.302, Count: 13
  Total Sequences Evaluated: 2194
    Oracle Count: 2144 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 68/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2194
  Performance plateaued, reducing LR to 0.000055

--- Round 68 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  CGGATCCTAG
  CTTGCGGAAA
  TTGGGACACC
  CCATGTGGCA
  AGTGCGCTAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.590
  Max reward: 12.901
  With intrinsic bonuses: 9.569

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.1225, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0960

=== Surrogate Model Training ===
Total samples: 2226

Training on 2109 samples (removed 117 outliers)
Reward range: [6.64, 11.83], mean: 9.27
  Created 13 candidate models for data size 2109
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.281 (std: 0.073)
  rf-tuned-xl: R2 = 0.281 (std: 0.077)
  gb-tuned-l: R2 = 0.203 (std: 0.035)
  gb-tuned-xl: R2 = 0.203 (std: 0.035)
  xgb-xl: R2 = 0.296 (std: 0.043)
  xgb-l: R2 = 0.296 (std: 0.043)
  mlp-adaptive-xl: R2 = 0.222 (std: 0.057)
  mlp-l: R2 = 0.234 (std: 0.072)
  svr-rbf-xl: R2 = 0.311 (std: 0.036)
  svr-poly-l: R2 = 0.311 (std: 0.036)
  knn-tuned-sqrt: R2 = 0.139 (std: 0.063)
  knn-tuned-l: R2 = 0.139 (std: 0.063)
  ridge: R2 = 0.015 (std: 0.039)

Model-based training with 10 models
Best R2: 0.311, Mean R2: 0.225
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.109 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.128 xgb-l:0.128 mlp-adaptive-xl:0.061 mlp-l:0.068 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1760.0730, entropy=0.1435, kl_div=0.0000
    Epoch 1: policy_loss=0.0073, value_loss=1760.0719, entropy=0.1433, kl_div=0.0211
  Round 1/5: Mean predicted reward = -6.648
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.109 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.128 xgb-l:0.128 mlp-adaptive-xl:0.061 mlp-l:0.068 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.1250, kl_div=0.0000
    Epoch 1: policy_loss=-0.0211, value_loss=0.9688, entropy=0.1251, kl_div=0.0244
  Round 2/5: Mean predicted reward = 9.374
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.109 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.128 xgb-l:0.128 mlp-adaptive-xl:0.061 mlp-l:0.068 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.1482, kl_div=0.0000
    Epoch 1: policy_loss=-0.0233, value_loss=0.9693, entropy=0.1478, kl_div=0.0475
  Round 3/5: Mean predicted reward = 9.475
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.109 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.128 xgb-l:0.128 mlp-adaptive-xl:0.061 mlp-l:0.068 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.1229, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0603
  Round 4/5: Mean predicted reward = 9.256
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.109 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.128 xgb-l:0.128 mlp-adaptive-xl:0.061 mlp-l:0.068 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.1352, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0834
  Round 5/5: Mean predicted reward = 9.403

  === Progress Analysis ===
  Status: NORMAL

--- Round 68 Results ---
  Mean Oracle Reward: 9.569
  Min Oracle Reward: 7.971
  Max Oracle Reward: 12.946
  Std Oracle Reward: 0.884
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.225, Max: 0.311, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2226
    Oracle Count: 2176 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 69/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2226

--- Round 69 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  AACGGGTCAT
  AGGATTCCAG
  CGGCAATGGC
  GGTACCAATG
  AAGGGGCCCT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.041
  Max reward: 11.898
  With intrinsic bonuses: 9.007

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9711, entropy=0.1191, kl_div=0.0000
    Epoch 1: policy_loss=0.0058, value_loss=0.9711, entropy=0.1192, kl_div=0.0207

=== Surrogate Model Training ===
Total samples: 2258

Training on 2137 samples (removed 121 outliers)
Reward range: [6.64, 11.83], mean: 9.27
  Created 13 candidate models for data size 2137
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.279 (std: 0.075)
  rf-tuned-xl: R2 = 0.272 (std: 0.080)
  gb-tuned-l: R2 = 0.198 (std: 0.044)
  gb-tuned-xl: R2 = 0.198 (std: 0.044)
  xgb-xl: R2 = 0.264 (std: 0.064)
  xgb-l: R2 = 0.264 (std: 0.064)
  mlp-adaptive-xl: R2 = 0.224 (std: 0.081)
  mlp-l: R2 = 0.190 (std: 0.083)
  svr-rbf-xl: R2 = 0.299 (std: 0.061)
  svr-poly-l: R2 = 0.299 (std: 0.061)
  knn-tuned-sqrt: R2 = 0.118 (std: 0.069)
  knn-tuned-l: R2 = 0.118 (std: 0.069)
  ridge: R2 = 0.014 (std: 0.034)

Model-based training with 7 models
Best R2: 0.299, Mean R2: 0.211
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.150 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.086 svr-rbf-xl:0.183 svr-poly-l:0.183 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2018.8403, entropy=0.1388, kl_div=0.0000
    Epoch 1: policy_loss=-0.0225, value_loss=2018.8401, entropy=0.1395, kl_div=-0.0364
  Round 1/3: Mean predicted reward = -7.147
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.150 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.086 svr-rbf-xl:0.183 svr-poly-l:0.183 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1354, kl_div=0.0000
    Epoch 1: policy_loss=-0.0042, value_loss=0.9691, entropy=0.1359, kl_div=-0.0259
  Round 2/3: Mean predicted reward = 9.328
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.150 rf-tuned-xl:0.140 xgb-xl:0.129 xgb-l:0.129 mlp-adaptive-xl:0.086 svr-rbf-xl:0.183 svr-poly-l:0.183 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9699, entropy=0.1270, kl_div=0.0000
    Epoch 1: policy_loss=-0.0229, value_loss=0.9699, entropy=0.1270, kl_div=0.0190
  Round 3/3: Mean predicted reward = 9.419

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 69 Results ---
  Mean Oracle Reward: 9.018
  Min Oracle Reward: 4.403
  Max Oracle Reward: 12.046
  Std Oracle Reward: 1.498
  Sequence Diversity: 0.906
  Models Used: 7
  Model R2 - Mean: 0.211, Max: 0.299, Count: 13
  Total Sequences Evaluated: 2258
    Oracle Count: 2208 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 70/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2258

--- Round 70 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GAGACACGTT
  GCCGAATCTG
  TCGCTGCGAA
  ATGCGACGCG
  GGAATTCGCC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.379
  Max reward: 13.091
  With intrinsic bonuses: 9.374

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9705, entropy=0.1373, kl_div=0.0000
    Epoch 1: policy_loss=-0.0147, value_loss=0.9705, entropy=0.1407, kl_div=-0.0477

=== Surrogate Model Training ===
Total samples: 2290

Training on 2167 samples (removed 123 outliers)
Reward range: [6.64, 11.83], mean: 9.27
  Created 13 candidate models for data size 2167
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.253 (std: 0.105)
  rf-tuned-xl: R2 = 0.253 (std: 0.111)
  gb-tuned-l: R2 = 0.193 (std: 0.063)
  gb-tuned-xl: R2 = 0.193 (std: 0.063)
  xgb-xl: R2 = 0.241 (std: 0.093)
  xgb-l: R2 = 0.241 (std: 0.093)
  mlp-adaptive-xl: R2 = 0.208 (std: 0.105)
  mlp-l: R2 = 0.210 (std: 0.108)
  svr-rbf-xl: R2 = 0.289 (std: 0.084)
  svr-poly-l: R2 = 0.289 (std: 0.084)
  knn-tuned-sqrt: R2 = 0.110 (std: 0.094)
  knn-tuned-l: R2 = 0.110 (std: 0.094)
  ridge: R2 = 0.014 (std: 0.034)

Model-based training with 8 models
Best R2: 0.289, Mean R2: 0.200
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.126 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.081 mlp-l:0.082 svr-rbf-xl:0.180 svr-poly-l:0.180 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1525.0747, entropy=0.1150, kl_div=0.0000
    Epoch 1: policy_loss=0.0483, value_loss=1525.0701, entropy=0.1160, kl_div=-0.0652
  Round 1/3: Mean predicted reward = -9.008
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.126 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.081 mlp-l:0.082 svr-rbf-xl:0.180 svr-poly-l:0.180 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9698, entropy=0.1220, kl_div=0.0000
    Epoch 1: policy_loss=0.0166, value_loss=0.9698, entropy=0.1276, kl_div=-0.1197
  Round 2/3: Mean predicted reward = 9.329
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.126 rf-tuned-xl:0.126 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.081 mlp-l:0.082 svr-rbf-xl:0.180 svr-poly-l:0.180 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.1369, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1594
  Round 3/3: Mean predicted reward = 9.450

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 70 Results ---
  Mean Oracle Reward: 9.415
  Min Oracle Reward: 6.239
  Max Oracle Reward: 13.063
  Std Oracle Reward: 1.304
  Sequence Diversity: 0.844
  Models Used: 8
  Model R2 - Mean: 0.200, Max: 0.289, Count: 13
  Total Sequences Evaluated: 2290
    Oracle Count: 2240 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 71/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2290

--- Round 71 Configuration ---
Learning rate: 0.000272
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  CGGTACCTAG
  TCGGCCAGAT
  AGCACTGCGT
  GAGCCGGACT
  GACATCCGGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.547
  Max reward: 11.580
  With intrinsic bonuses: 9.546

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.1532, kl_div=0.0000
    Epoch 1: policy_loss=-0.0334, value_loss=0.9687, entropy=0.1550, kl_div=-0.0310

=== Surrogate Model Training ===
Total samples: 2322

Training on 2198 samples (removed 124 outliers)
Reward range: [6.64, 11.83], mean: 9.28
  Created 13 candidate models for data size 2198
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.260 (std: 0.128)
  rf-tuned-xl: R2 = 0.259 (std: 0.128)
  gb-tuned-l: R2 = 0.204 (std: 0.081)
  gb-tuned-xl: R2 = 0.204 (std: 0.081)
  xgb-xl: R2 = 0.268 (std: 0.094)
  xgb-l: R2 = 0.268 (std: 0.094)
  mlp-adaptive-xl: R2 = 0.216 (std: 0.103)
  mlp-l: R2 = 0.208 (std: 0.110)
  svr-rbf-xl: R2 = 0.304 (std: 0.096)
  svr-poly-l: R2 = 0.304 (std: 0.096)
  knn-tuned-sqrt: R2 = 0.117 (std: 0.099)
  knn-tuned-l: R2 = 0.117 (std: 0.099)
  ridge: R2 = 0.020 (std: 0.036)

Model-based training with 10 models
Best R2: 0.304, Mean R2: 0.211
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.103 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.067 mlp-l:0.062 svr-rbf-xl:0.161 svr-poly-l:0.161 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=1215.9697, entropy=0.1389, kl_div=0.0000
    Epoch 1: policy_loss=-0.0584, value_loss=1215.9664, entropy=0.1423, kl_div=-0.1857
  Round 1/5: Mean predicted reward = -7.823
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.103 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.067 mlp-l:0.062 svr-rbf-xl:0.161 svr-poly-l:0.161 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1454, kl_div=0.0000
    Epoch 1: policy_loss=-0.0031, value_loss=0.9689, entropy=0.1499, kl_div=-0.1049
  Round 2/5: Mean predicted reward = 9.482
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.103 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.067 mlp-l:0.062 svr-rbf-xl:0.161 svr-poly-l:0.161 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9698, entropy=0.1356, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2429
  Round 3/5: Mean predicted reward = 9.496
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.103 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.067 mlp-l:0.062 svr-rbf-xl:0.161 svr-poly-l:0.161 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9696, entropy=0.1574, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2193
  Round 4/5: Mean predicted reward = 9.501
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.103 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.112 xgb-l:0.112 mlp-adaptive-xl:0.067 mlp-l:0.062 svr-rbf-xl:0.161 svr-poly-l:0.161 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.1189, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2496
  Round 5/5: Mean predicted reward = 9.394

  === Progress Analysis ===
  Status: NORMAL

--- Round 71 Results ---
  Mean Oracle Reward: 9.590
  Min Oracle Reward: 7.300
  Max Oracle Reward: 11.933
  Std Oracle Reward: 1.081
  Sequence Diversity: 0.875
  Models Used: 10
  Model R2 - Mean: 0.211, Max: 0.304, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2322
    Oracle Count: 2272 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 72/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2322
  Consistent improvement, increasing LR to 0.000240

--- Round 72 Configuration ---
Learning rate: 0.000240
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  TGTAGCAGCA
  CACGGGACTT
  CGCAATTGGC
  GGCTCGCAAT
  CTGAAGCCTG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.450
  Max reward: 10.358
  With intrinsic bonuses: 9.416

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9705, entropy=0.1242, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2323

=== Surrogate Model Training ===
Total samples: 2354

Training on 2230 samples (removed 124 outliers)
Reward range: [6.64, 11.88], mean: 9.28
  Created 13 candidate models for data size 2230
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.245 (std: 0.148)
  rf-tuned-xl: R2 = 0.250 (std: 0.140)
  gb-tuned-l: R2 = 0.202 (std: 0.076)
  gb-tuned-xl: R2 = 0.202 (std: 0.076)
  xgb-xl: R2 = 0.252 (std: 0.139)
  xgb-l: R2 = 0.252 (std: 0.139)
  mlp-adaptive-xl: R2 = 0.208 (std: 0.124)
  mlp-l: R2 = 0.224 (std: 0.100)
  svr-rbf-xl: R2 = 0.299 (std: 0.113)
  svr-poly-l: R2 = 0.299 (std: 0.113)
  knn-tuned-sqrt: R2 = 0.103 (std: 0.120)
  knn-tuned-l: R2 = 0.103 (std: 0.120)
  ridge: R2 = 0.020 (std: 0.035)

Model-based training with 10 models
Best R2: 0.299, Mean R2: 0.204
Running 3 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.066 mlp-l:0.078 svr-rbf-xl:0.165 svr-poly-l:0.165 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2198.2302, entropy=0.1316, kl_div=0.0000
    Epoch 1: policy_loss=0.0444, value_loss=2198.2263, entropy=0.1325, kl_div=0.0113
  Round 1/3: Mean predicted reward = -10.227
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.066 mlp-l:0.078 svr-rbf-xl:0.165 svr-poly-l:0.165 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1383, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0778
  Round 2/3: Mean predicted reward = 9.392
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.096 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.103 xgb-l:0.103 mlp-adaptive-xl:0.066 mlp-l:0.078 svr-rbf-xl:0.165 svr-poly-l:0.165 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9706, entropy=0.1421, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2072
  Round 3/3: Mean predicted reward = 9.579

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 72 Results ---
  Mean Oracle Reward: 9.435
  Min Oracle Reward: 5.287
  Max Oracle Reward: 10.517
  Std Oracle Reward: 1.002
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.204, Max: 0.299, Count: 13
  Total Sequences Evaluated: 2354
    Oracle Count: 2304 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 73/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2354
  Performance plateaued, reducing LR to 0.000055

--- Round 73 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  GCTAGCAATG
  AGCAGCTATG
  CTCGGATACG
  AGACCCGGTT
  CTACAGGGCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.508
  Max reward: 11.184
  With intrinsic bonuses: 9.487

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.1378, kl_div=0.0000
    Epoch 1: policy_loss=-0.0155, value_loss=0.9688, entropy=0.1374, kl_div=0.0490

=== Surrogate Model Training ===
Total samples: 2386

Training on 2262 samples (removed 124 outliers)
Reward range: [6.64, 11.90], mean: 9.29
  Created 13 candidate models for data size 2262
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.257 (std: 0.125)
  rf-tuned-xl: R2 = 0.252 (std: 0.122)
  gb-tuned-l: R2 = 0.210 (std: 0.076)
  gb-tuned-xl: R2 = 0.210 (std: 0.076)
  xgb-xl: R2 = 0.244 (std: 0.148)
  xgb-l: R2 = 0.244 (std: 0.148)
  mlp-adaptive-xl: R2 = 0.224 (std: 0.116)
  mlp-l: R2 = 0.223 (std: 0.101)
  svr-rbf-xl: R2 = 0.306 (std: 0.111)
  svr-poly-l: R2 = 0.306 (std: 0.111)
  knn-tuned-sqrt: R2 = 0.106 (std: 0.113)
  knn-tuned-l: R2 = 0.106 (std: 0.113)
  ridge: R2 = 0.023 (std: 0.034)

Model-based training with 10 models
Best R2: 0.306, Mean R2: 0.209
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.098 gb-tuned-l:0.065 gb-tuned-xl:0.065 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.075 mlp-l:0.074 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1287.8175, entropy=0.1254, kl_div=0.0000
    Epoch 1: policy_loss=-0.0237, value_loss=1287.8168, entropy=0.1264, kl_div=-0.0326
  Round 1/5: Mean predicted reward = -11.930
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.098 gb-tuned-l:0.065 gb-tuned-xl:0.065 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.075 mlp-l:0.074 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9685, entropy=0.1285, kl_div=0.0000
    Epoch 1: policy_loss=-0.0118, value_loss=0.9685, entropy=0.1294, kl_div=-0.0249
  Round 2/5: Mean predicted reward = 9.379
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.098 gb-tuned-l:0.065 gb-tuned-xl:0.065 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.075 mlp-l:0.074 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9694, entropy=0.1209, kl_div=0.0000
    Epoch 1: policy_loss=-0.0155, value_loss=0.9694, entropy=0.1211, kl_div=-0.0001
  Round 3/5: Mean predicted reward = 9.476
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.098 gb-tuned-l:0.065 gb-tuned-xl:0.065 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.075 mlp-l:0.074 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.1310, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0509
  Round 4/5: Mean predicted reward = 9.467
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.104 rf-tuned-xl:0.098 gb-tuned-l:0.065 gb-tuned-xl:0.065 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.075 mlp-l:0.074 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1405, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0664
  Round 5/5: Mean predicted reward = 9.351

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 73 Results ---
  Mean Oracle Reward: 9.559
  Min Oracle Reward: 5.181
  Max Oracle Reward: 11.096
  Std Oracle Reward: 1.133
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.209, Max: 0.306, Count: 13
  Total Sequences Evaluated: 2386
    Oracle Count: 2336 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 74/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2386
  Performance plateaued, reducing LR to 0.000019

--- Round 74 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  TGGCCGAGAC
  CAGTATCGGC
  TGCGGCAACT
  CAAGGGCCGT
  CTAGGGTACC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.329
  Max reward: 10.818
  With intrinsic bonuses: 9.356

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9680, entropy=0.1338, kl_div=0.0000
    Epoch 1: policy_loss=-0.0028, value_loss=0.9680, entropy=0.1337, kl_div=0.0201

=== Surrogate Model Training ===
Total samples: 2418

Training on 2292 samples (removed 126 outliers)
Reward range: [6.64, 11.90], mean: 9.29
  Created 13 candidate models for data size 2292
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.246 (std: 0.122)
  rf-tuned-xl: R2 = 0.247 (std: 0.122)
  gb-tuned-l: R2 = 0.197 (std: 0.072)
  gb-tuned-xl: R2 = 0.197 (std: 0.072)
  xgb-xl: R2 = 0.245 (std: 0.140)
  xgb-l: R2 = 0.245 (std: 0.140)
  mlp-adaptive-xl: R2 = 0.229 (std: 0.102)
  mlp-l: R2 = 0.239 (std: 0.091)
  svr-rbf-xl: R2 = 0.301 (std: 0.103)
  svr-poly-l: R2 = 0.301 (std: 0.103)
  knn-tuned-sqrt: R2 = 0.093 (std: 0.109)
  knn-tuned-l: R2 = 0.093 (std: 0.109)
  ridge: R2 = 0.019 (std: 0.035)

Model-based training with 8 models
Best R2: 0.301, Mean R2: 0.204
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.110 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.091 mlp-l:0.101 svr-rbf-xl:0.188 svr-poly-l:0.188 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2436.2759, entropy=0.1201, kl_div=0.0000
    Epoch 1: policy_loss=-0.0030, value_loss=2436.2759, entropy=0.1205, kl_div=-0.0055
  Round 1/5: Mean predicted reward = -10.470
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.110 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.091 mlp-l:0.101 svr-rbf-xl:0.188 svr-poly-l:0.188 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.1298, kl_div=0.0000
    Epoch 1: policy_loss=-0.0051, value_loss=0.9692, entropy=0.1299, kl_div=0.0014
  Round 2/5: Mean predicted reward = 9.325
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.110 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.091 mlp-l:0.101 svr-rbf-xl:0.188 svr-poly-l:0.188 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1275, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=0.9689, entropy=0.1276, kl_div=0.0050
  Round 3/5: Mean predicted reward = 9.539
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.110 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.091 mlp-l:0.101 svr-rbf-xl:0.188 svr-poly-l:0.188 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.1544, kl_div=0.0000
    Epoch 1: policy_loss=-0.0177, value_loss=0.9690, entropy=0.1544, kl_div=0.0243
  Round 4/5: Mean predicted reward = 9.418
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.110 xgb-xl:0.107 xgb-l:0.107 mlp-adaptive-xl:0.091 mlp-l:0.101 svr-rbf-xl:0.188 svr-poly-l:0.188 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9701, entropy=0.1280, kl_div=0.0000
    Epoch 1: policy_loss=-0.0084, value_loss=0.9701, entropy=0.1281, kl_div=0.0129
  Round 5/5: Mean predicted reward = 9.319

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 74 Results ---
  Mean Oracle Reward: 9.353
  Min Oracle Reward: 5.729
  Max Oracle Reward: 10.985
  Std Oracle Reward: 1.220
  Sequence Diversity: 0.875
  Models Used: 8
  Model R2 - Mean: 0.204, Max: 0.301, Count: 13
  Total Sequences Evaluated: 2418
    Oracle Count: 2368 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 75/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2418
  Performance plateaued, reducing LR to 0.000150

--- Round 75 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  AATCGTGGAC
  ACGGGTATCC
  TAAGGCCGTC
  CGACAGCGGT
  GACGGCTAGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.505
  Max reward: 11.309
  With intrinsic bonuses: 9.470

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1220, kl_div=0.0000
    Epoch 1: policy_loss=-0.0055, value_loss=0.9684, entropy=0.1213, kl_div=0.0237

=== Surrogate Model Training ===
Total samples: 2450

Training on 2324 samples (removed 126 outliers)
Reward range: [6.60, 11.90], mean: 9.30
  Created 13 candidate models for data size 2324
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.250 (std: 0.123)
  rf-tuned-xl: R2 = 0.258 (std: 0.114)
  gb-tuned-l: R2 = 0.209 (std: 0.079)
  gb-tuned-xl: R2 = 0.209 (std: 0.079)
  xgb-xl: R2 = 0.244 (std: 0.142)
  xgb-l: R2 = 0.244 (std: 0.142)
  mlp-adaptive-xl: R2 = 0.255 (std: 0.105)
  mlp-l: R2 = 0.236 (std: 0.121)
  svr-rbf-xl: R2 = 0.309 (std: 0.101)
  svr-poly-l: R2 = 0.309 (std: 0.101)
  knn-tuned-sqrt: R2 = 0.101 (std: 0.097)
  knn-tuned-l: R2 = 0.101 (std: 0.097)
  ridge: R2 = 0.023 (std: 0.036)

Model-based training with 10 models
Best R2: 0.309, Mean R2: 0.211
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.087 xgb-l:0.087 mlp-adaptive-xl:0.097 mlp-l:0.080 svr-rbf-xl:0.167 svr-poly-l:0.167 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=4761.5620, entropy=0.1189, kl_div=0.0000
    Epoch 1: policy_loss=0.0012, value_loss=4761.5586, entropy=0.1175, kl_div=0.0235
  Round 1/5: Mean predicted reward = -13.499
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.087 xgb-l:0.087 mlp-adaptive-xl:0.097 mlp-l:0.080 svr-rbf-xl:0.167 svr-poly-l:0.167 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1364, kl_div=0.0000
    Epoch 1: policy_loss=-0.0208, value_loss=0.9689, entropy=0.1348, kl_div=-0.0529
  Round 2/5: Mean predicted reward = 9.512
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.087 xgb-l:0.087 mlp-adaptive-xl:0.097 mlp-l:0.080 svr-rbf-xl:0.167 svr-poly-l:0.167 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9685, entropy=0.1147, kl_div=0.0000
    Epoch 1: policy_loss=-0.0236, value_loss=0.9685, entropy=0.1123, kl_div=-0.0183
  Round 3/5: Mean predicted reward = 9.365
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.087 xgb-l:0.087 mlp-adaptive-xl:0.097 mlp-l:0.080 svr-rbf-xl:0.167 svr-poly-l:0.167 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9695, entropy=0.1231, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1381
  Round 4/5: Mean predicted reward = 9.460
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.092 rf-tuned-xl:0.100 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.087 xgb-l:0.087 mlp-adaptive-xl:0.097 mlp-l:0.080 svr-rbf-xl:0.167 svr-poly-l:0.167 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1225, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1364
  Round 5/5: Mean predicted reward = 9.482

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 75 Results ---
  Mean Oracle Reward: 9.494
  Min Oracle Reward: 6.146
  Max Oracle Reward: 11.307
  Std Oracle Reward: 0.945
  Sequence Diversity: 0.875
  Models Used: 10
  Model R2 - Mean: 0.211, Max: 0.309, Count: 13
  Total Sequences Evaluated: 2450
    Oracle Count: 2400 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 76/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2450
  Performance plateaued, reducing LR to 0.000136

--- Round 76 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.906) ---
  ATTGACGCAG
  ACGGGTCTAC
  CTCGCAAGTG
  CCGTATCGGA
  TGCTGACGAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.378
  Max reward: 10.716
  With intrinsic bonuses: 9.341

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9701, entropy=0.1257, kl_div=0.0000
    Epoch 1: policy_loss=-0.0209, value_loss=0.9701, entropy=0.1260, kl_div=0.0302

=== Surrogate Model Training ===
Total samples: 2482

Training on 2354 samples (removed 128 outliers)
Reward range: [6.64, 11.90], mean: 9.30
  Created 13 candidate models for data size 2354
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.266 (std: 0.118)
  rf-tuned-xl: R2 = 0.256 (std: 0.123)
  gb-tuned-l: R2 = 0.212 (std: 0.075)
  gb-tuned-xl: R2 = 0.212 (std: 0.075)
  xgb-xl: R2 = 0.261 (std: 0.123)
  xgb-l: R2 = 0.261 (std: 0.123)
  mlp-adaptive-xl: R2 = 0.233 (std: 0.107)
  mlp-l: R2 = 0.249 (std: 0.081)
  svr-rbf-xl: R2 = 0.318 (std: 0.103)
  svr-poly-l: R2 = 0.318 (std: 0.103)
  knn-tuned-sqrt: R2 = 0.121 (std: 0.102)
  knn-tuned-l: R2 = 0.121 (std: 0.102)
  ridge: R2 = 0.028 (std: 0.041)

Model-based training with 10 models
Best R2: 0.318, Mean R2: 0.220
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.091 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.072 mlp-l:0.085 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2197.6816, entropy=0.1299, kl_div=0.0000
    Epoch 1: policy_loss=-0.0339, value_loss=2197.6797, entropy=0.1312, kl_div=-0.1097
  Round 1/5: Mean predicted reward = -13.803
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.091 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.072 mlp-l:0.085 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.1121, kl_div=0.0000
    Epoch 1: policy_loss=-0.0152, value_loss=0.9687, entropy=0.1129, kl_div=-0.0169
  Round 2/5: Mean predicted reward = 9.377
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.091 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.072 mlp-l:0.085 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=0.0992, kl_div=0.0000
    Epoch 1: policy_loss=-0.0336, value_loss=0.9693, entropy=0.0985, kl_div=0.0423
  Round 3/5: Mean predicted reward = 9.381
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.091 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.072 mlp-l:0.085 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9702, entropy=0.1321, kl_div=0.0000
    Epoch 1: policy_loss=-0.0323, value_loss=0.9702, entropy=0.1324, kl_div=-0.0021
  Round 4/5: Mean predicted reward = 9.446
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.091 gb-tuned-l:0.059 gb-tuned-xl:0.059 xgb-xl:0.096 xgb-l:0.096 mlp-adaptive-xl:0.072 mlp-l:0.085 svr-rbf-xl:0.170 svr-poly-l:0.170 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.0977, kl_div=0.0000
    Epoch 1: policy_loss=-0.0275, value_loss=0.9689, entropy=0.0989, kl_div=-0.0393
  Round 5/5: Mean predicted reward = 9.373

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 76 Results ---
  Mean Oracle Reward: 9.253
  Min Oracle Reward: 6.026
  Max Oracle Reward: 10.663
  Std Oracle Reward: 1.003
  Sequence Diversity: 0.906
  Models Used: 10
  Model R2 - Mean: 0.220, Max: 0.318, Count: 13
  Total Sequences Evaluated: 2482
    Oracle Count: 2432 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 77/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2482
  Performance plateaued, reducing LR to 0.000100

--- Round 77 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  CCGTGAAGGC
  TGCCTAGAGA
  AGCAGCGCGT
  AGTCGTCCAG
  CAGTGGCATC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.627
  Max reward: 11.063
  With intrinsic bonuses: 9.604

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9696, entropy=0.1395, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1028

=== Surrogate Model Training ===
Total samples: 2514

Training on 2386 samples (removed 128 outliers)
Reward range: [6.64, 11.90], mean: 9.30
  Created 13 candidate models for data size 2386
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.254 (std: 0.123)
  rf-tuned-xl: R2 = 0.261 (std: 0.114)
  gb-tuned-l: R2 = 0.221 (std: 0.075)
  gb-tuned-xl: R2 = 0.221 (std: 0.075)
  xgb-xl: R2 = 0.248 (std: 0.147)
  xgb-l: R2 = 0.248 (std: 0.147)
  mlp-adaptive-xl: R2 = 0.251 (std: 0.113)
  mlp-l: R2 = 0.233 (std: 0.103)
  svr-rbf-xl: R2 = 0.318 (std: 0.109)
  svr-poly-l: R2 = 0.318 (std: 0.109)
  knn-tuned-sqrt: R2 = 0.121 (std: 0.100)
  knn-tuned-l: R2 = 0.121 (std: 0.100)
  ridge: R2 = 0.028 (std: 0.043)

Model-based training with 10 models
Best R2: 0.318, Mean R2: 0.219
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.091 rf-tuned-xl:0.098 gb-tuned-l:0.066 gb-tuned-xl:0.066 xgb-xl:0.086 xgb-l:0.086 mlp-adaptive-xl:0.089 mlp-l:0.074 svr-rbf-xl:0.173 svr-poly-l:0.173 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2204.6504, entropy=0.1170, kl_div=0.0000
    Epoch 1: policy_loss=-0.0099, value_loss=2204.6492, entropy=0.1176, kl_div=-0.0661
  Round 1/5: Mean predicted reward = -15.638
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.091 rf-tuned-xl:0.098 gb-tuned-l:0.066 gb-tuned-xl:0.066 xgb-xl:0.086 xgb-l:0.086 mlp-adaptive-xl:0.089 mlp-l:0.074 svr-rbf-xl:0.173 svr-poly-l:0.173 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9680, entropy=0.1268, kl_div=0.0000
    Epoch 1: policy_loss=-0.0128, value_loss=0.9680, entropy=0.1279, kl_div=0.0042
  Round 2/5: Mean predicted reward = 9.539
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.091 rf-tuned-xl:0.098 gb-tuned-l:0.066 gb-tuned-xl:0.066 xgb-xl:0.086 xgb-l:0.086 mlp-adaptive-xl:0.089 mlp-l:0.074 svr-rbf-xl:0.173 svr-poly-l:0.173 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9697, entropy=0.1298, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0769
  Round 3/5: Mean predicted reward = 9.311
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.091 rf-tuned-xl:0.098 gb-tuned-l:0.066 gb-tuned-xl:0.066 xgb-xl:0.086 xgb-l:0.086 mlp-adaptive-xl:0.089 mlp-l:0.074 svr-rbf-xl:0.173 svr-poly-l:0.173 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.1386, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0781
  Round 4/5: Mean predicted reward = 9.489
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.091 rf-tuned-xl:0.098 gb-tuned-l:0.066 gb-tuned-xl:0.066 xgb-xl:0.086 xgb-l:0.086 mlp-adaptive-xl:0.089 mlp-l:0.074 svr-rbf-xl:0.173 svr-poly-l:0.173 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9695, entropy=0.1103, kl_div=0.0000
    Epoch 1: policy_loss=-0.0096, value_loss=0.9695, entropy=0.1093, kl_div=0.0313
  Round 5/5: Mean predicted reward = 9.515

  === Progress Analysis ===
  Status: NORMAL

--- Round 77 Results ---
  Mean Oracle Reward: 9.637
  Min Oracle Reward: 7.254
  Max Oracle Reward: 11.293
  Std Oracle Reward: 0.810
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.219, Max: 0.318, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2514
    Oracle Count: 2464 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 78/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2514

--- Round 78 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GACTGCTAGA
  ACTGGTCCGA
  GTTCGCAAGA
  ATGCAGTCAG
  AATGCAGTCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.481
  Max reward: 10.302
  With intrinsic bonuses: 9.496

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.1390, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1413

=== Surrogate Model Training ===
Total samples: 2546

Training on 2418 samples (removed 128 outliers)
Reward range: [6.64, 11.90], mean: 9.30
  Created 13 candidate models for data size 2418
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.266 (std: 0.111)
  rf-tuned-xl: R2 = 0.264 (std: 0.108)
  gb-tuned-l: R2 = 0.217 (std: 0.084)
  gb-tuned-xl: R2 = 0.217 (std: 0.084)
  xgb-xl: R2 = 0.248 (std: 0.120)
  xgb-l: R2 = 0.248 (std: 0.120)
  mlp-adaptive-xl: R2 = 0.253 (std: 0.090)
  mlp-l: R2 = 0.258 (std: 0.124)
  svr-rbf-xl: R2 = 0.319 (std: 0.108)
  svr-poly-l: R2 = 0.319 (std: 0.108)
  knn-tuned-sqrt: R2 = 0.130 (std: 0.091)
  knn-tuned-l: R2 = 0.130 (std: 0.091)
  ridge: R2 = 0.026 (std: 0.045)

Model-based training with 10 models
Best R2: 0.319, Mean R2: 0.223
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.083 xgb-l:0.083 mlp-adaptive-xl:0.087 mlp-l:0.092 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=5202.9478, entropy=0.1303, kl_div=0.0000
    Epoch 1: policy_loss=0.0101, value_loss=5202.9448, entropy=0.1294, kl_div=0.0084
  Round 1/5: Mean predicted reward = -16.222
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.083 xgb-l:0.083 mlp-adaptive-xl:0.087 mlp-l:0.092 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.1125, kl_div=0.0000
    Epoch 1: policy_loss=-0.0251, value_loss=0.9684, entropy=0.1133, kl_div=-0.0600
  Round 2/5: Mean predicted reward = 9.374
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.083 xgb-l:0.083 mlp-adaptive-xl:0.087 mlp-l:0.092 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9681, entropy=0.1227, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0692
  Round 3/5: Mean predicted reward = 9.331
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.083 xgb-l:0.083 mlp-adaptive-xl:0.087 mlp-l:0.092 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.1179, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1417
  Round 4/5: Mean predicted reward = 9.334
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.061 gb-tuned-xl:0.061 xgb-xl:0.083 xgb-l:0.083 mlp-adaptive-xl:0.087 mlp-l:0.092 svr-rbf-xl:0.169 svr-poly-l:0.169 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=0.1174, kl_div=0.0000
    Epoch 1: policy_loss=-0.0139, value_loss=0.9690, entropy=0.1173, kl_div=0.0307
  Round 5/5: Mean predicted reward = 9.351

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 78 Results ---
  Mean Oracle Reward: 9.450
  Min Oracle Reward: 8.138
  Max Oracle Reward: 10.534
  Std Oracle Reward: 0.654
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.223, Max: 0.319, Count: 13
  Total Sequences Evaluated: 2546
    Oracle Count: 2496 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 79/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2546

--- Round 79 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  ACCTAGATGG
  GAATCGGTCC
  CATCCATGGG
  GATGGTCAAC
  CAAGGTCGTC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.341
  Max reward: 10.785
  With intrinsic bonuses: 9.323

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1136, kl_div=0.0000
    Epoch 1: policy_loss=-0.0182, value_loss=0.9691, entropy=0.1138, kl_div=-0.0117

=== Surrogate Model Training ===
Total samples: 2578

Training on 2447 samples (removed 131 outliers)
Reward range: [6.65, 11.90], mean: 9.31
  Created 13 candidate models for data size 2447
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.268 (std: 0.124)
  rf-tuned-xl: R2 = 0.267 (std: 0.119)
  gb-tuned-l: R2 = 0.221 (std: 0.077)
  gb-tuned-xl: R2 = 0.221 (std: 0.077)
  xgb-xl: R2 = 0.248 (std: 0.125)
  xgb-l: R2 = 0.248 (std: 0.125)
  mlp-adaptive-xl: R2 = 0.265 (std: 0.110)
  mlp-l: R2 = 0.248 (std: 0.134)
  svr-rbf-xl: R2 = 0.323 (std: 0.111)
  svr-poly-l: R2 = 0.323 (std: 0.111)
  knn-tuned-sqrt: R2 = 0.133 (std: 0.095)
  knn-tuned-l: R2 = 0.133 (std: 0.095)
  ridge: R2 = 0.028 (std: 0.041)

Model-based training with 10 models
Best R2: 0.323, Mean R2: 0.225
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.096 mlp-l:0.081 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=3415.7471, entropy=0.1118, kl_div=0.0000
    Epoch 1: policy_loss=-0.0137, value_loss=3415.7471, entropy=0.1122, kl_div=-0.0348
  Round 1/5: Mean predicted reward = -12.662
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.096 mlp-l:0.081 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.1198, kl_div=0.0000
    Epoch 1: policy_loss=-0.0057, value_loss=0.9686, entropy=0.1206, kl_div=-0.0226
  Round 2/5: Mean predicted reward = 9.506
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.096 mlp-l:0.081 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.1189, kl_div=0.0000
    Epoch 1: policy_loss=-0.0200, value_loss=0.9689, entropy=0.1195, kl_div=-0.0074
  Round 3/5: Mean predicted reward = 9.456
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.096 mlp-l:0.081 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.1270, kl_div=0.0000
    Epoch 1: policy_loss=-0.0062, value_loss=0.9690, entropy=0.1268, kl_div=0.0136
  Round 4/5: Mean predicted reward = 9.414
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.099 rf-tuned-xl:0.098 gb-tuned-l:0.062 gb-tuned-xl:0.062 xgb-xl:0.081 xgb-l:0.081 mlp-adaptive-xl:0.096 mlp-l:0.081 svr-rbf-xl:0.171 svr-poly-l:0.171 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.1070, kl_div=0.0000
    Epoch 1: policy_loss=-0.0321, value_loss=0.9688, entropy=0.1067, kl_div=-0.0022
  Round 5/5: Mean predicted reward = 9.406

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 79 Results ---
  Mean Oracle Reward: 9.307
  Min Oracle Reward: 6.001
  Max Oracle Reward: 10.559
  Std Oracle Reward: 1.099
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.225, Max: 0.323, Count: 13
  Total Sequences Evaluated: 2578
    Oracle Count: 2528 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 80/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2578

--- Round 80 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  CGGTCTAGAC
  GTGCAAGCTA
  TAGGCCGGCA
  AGGCGTCATC
  TCACAGGGCT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.525
  Max reward: 10.977
  With intrinsic bonuses: 9.537

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1260, kl_div=0.0000
    Epoch 1: policy_loss=0.0386, value_loss=0.9689, entropy=0.1282, kl_div=-0.0052

=== Surrogate Model Training ===
Total samples: 2610

Training on 2479 samples (removed 131 outliers)
Reward range: [6.65, 11.90], mean: 9.31
  Created 13 candidate models for data size 2479
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.283 (std: 0.115)
  rf-tuned-xl: R2 = 0.282 (std: 0.102)
  gb-tuned-l: R2 = 0.220 (std: 0.079)
  gb-tuned-xl: R2 = 0.220 (std: 0.079)
  xgb-xl: R2 = 0.268 (std: 0.107)
  xgb-l: R2 = 0.268 (std: 0.107)
  mlp-adaptive-xl: R2 = 0.251 (std: 0.089)
  mlp-l: R2 = 0.240 (std: 0.092)
  svr-rbf-xl: R2 = 0.332 (std: 0.103)
  svr-poly-l: R2 = 0.332 (std: 0.103)
  knn-tuned-sqrt: R2 = 0.142 (std: 0.081)
  knn-tuned-l: R2 = 0.142 (std: 0.081)
  ridge: R2 = 0.033 (std: 0.044)

Model-based training with 10 models
Best R2: 0.332, Mean R2: 0.232
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.105 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.077 mlp-l:0.069 svr-rbf-xl:0.174 svr-poly-l:0.174 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=5044.1953, entropy=0.1292, kl_div=0.0000
    Epoch 1: policy_loss=-0.0401, value_loss=5044.1914, entropy=0.1335, kl_div=-0.1110
  Round 1/5: Mean predicted reward = -17.542
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.105 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.077 mlp-l:0.069 svr-rbf-xl:0.174 svr-poly-l:0.174 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9698, entropy=0.1377, kl_div=0.0000
    Epoch 1: policy_loss=-0.0345, value_loss=0.9698, entropy=0.1424, kl_div=-0.0669
  Round 2/5: Mean predicted reward = 9.476
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.105 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.077 mlp-l:0.069 svr-rbf-xl:0.174 svr-poly-l:0.174 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1675, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2348
  Round 3/5: Mean predicted reward = 9.528
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.105 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.077 mlp-l:0.069 svr-rbf-xl:0.174 svr-poly-l:0.174 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1296, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2503
  Round 4/5: Mean predicted reward = 9.457
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.106 rf-tuned-xl:0.105 gb-tuned-l:0.056 gb-tuned-xl:0.056 xgb-xl:0.091 xgb-l:0.091 mlp-adaptive-xl:0.077 mlp-l:0.069 svr-rbf-xl:0.174 svr-poly-l:0.174 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9685, entropy=0.1477, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3033
  Round 5/5: Mean predicted reward = 9.492

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 80 Results ---
  Mean Oracle Reward: 9.524
  Min Oracle Reward: 7.010
  Max Oracle Reward: 11.009
  Std Oracle Reward: 0.866
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.232, Max: 0.332, Count: 13
  Total Sequences Evaluated: 2610
    Oracle Count: 2560 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 81/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2610
  Performance plateaued, reducing LR to 0.000136

--- Round 81 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  CGGGAAGCTC
  CTGATCACGG
  TGATGACGCA
  TCCCTGAGAG
  CCAGGAGTAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.457
  Max reward: 11.349
  With intrinsic bonuses: 9.466

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.1237, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0773

=== Surrogate Model Training ===
Total samples: 2642

Training on 2510 samples (removed 132 outliers)
Reward range: [6.68, 11.90], mean: 9.31
  Created 13 candidate models for data size 2510
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.306 (std: 0.095)
  rf-tuned-xl: R2 = 0.298 (std: 0.091)
  gb-tuned-l: R2 = 0.234 (std: 0.075)
  gb-tuned-xl: R2 = 0.234 (std: 0.075)
  xgb-xl: R2 = 0.294 (std: 0.086)
  xgb-l: R2 = 0.294 (std: 0.086)
  mlp-adaptive-xl: R2 = 0.268 (std: 0.090)
  mlp-l: R2 = 0.265 (std: 0.088)
  svr-rbf-xl: R2 = 0.346 (std: 0.092)
  svr-poly-l: R2 = 0.346 (std: 0.092)
  knn-tuned-sqrt: R2 = 0.149 (std: 0.083)
  knn-tuned-l: R2 = 0.149 (std: 0.083)
  ridge: R2 = 0.039 (std: 0.048)

Model-based training with 10 models
Best R2: 0.346, Mean R2: 0.248
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.102 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.076 mlp-l:0.074 svr-rbf-xl:0.166 svr-poly-l:0.166 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=2403.0303, entropy=0.1232, kl_div=0.0000
    Epoch 1: policy_loss=-0.0379, value_loss=2403.0293, entropy=0.1242, kl_div=-0.0542
  Round 1/5: Mean predicted reward = -15.986
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.102 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.076 mlp-l:0.074 svr-rbf-xl:0.166 svr-poly-l:0.166 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.1424, kl_div=0.0000
    Epoch 1: policy_loss=-0.0597, value_loss=0.9684, entropy=0.1437, kl_div=-0.0233
  Round 2/5: Mean predicted reward = 9.455
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.102 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.076 mlp-l:0.074 svr-rbf-xl:0.166 svr-poly-l:0.166 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.1459, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1771
  Round 3/5: Mean predicted reward = 9.467
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.102 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.076 mlp-l:0.074 svr-rbf-xl:0.166 svr-poly-l:0.166 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9684, entropy=0.1233, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0835
  Round 4/5: Mean predicted reward = 9.281
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.111 rf-tuned-xl:0.102 gb-tuned-l:0.054 gb-tuned-xl:0.054 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.076 mlp-l:0.074 svr-rbf-xl:0.166 svr-poly-l:0.166 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.1305, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1005
  Round 5/5: Mean predicted reward = 9.467

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 81 Results ---
  Mean Oracle Reward: 9.480
  Min Oracle Reward: 7.181
  Max Oracle Reward: 11.322
  Std Oracle Reward: 0.904
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.248, Max: 0.346, Count: 13
  Total Sequences Evaluated: 2642
    Oracle Count: 2592 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 82/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2642
  Performance plateaued, reducing LR to 0.000100

--- Round 82 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  GCGCCTGAAT
  AGTGAGCCCG
  GCTCTGCAAG
  AACCTGGTGA
  TCCCGAGTAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.674
  Max reward: 11.219
  With intrinsic bonuses: 9.679

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9687, entropy=0.1341, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1560

=== Surrogate Model Training ===
Total samples: 2674

Training on 2542 samples (removed 132 outliers)
Reward range: [6.68, 11.90], mean: 9.32
  Created 13 candidate models for data size 2542
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.325 (std: 0.078)
  rf-tuned-xl: R2 = 0.323 (std: 0.074)
  gb-tuned-l: R2 = 0.241 (std: 0.069)
  gb-tuned-xl: R2 = 0.241 (std: 0.069)
  xgb-xl: R2 = 0.316 (std: 0.101)
  xgb-l: R2 = 0.316 (std: 0.101)
  mlp-adaptive-xl: R2 = 0.274 (std: 0.098)
  mlp-l: R2 = 0.280 (std: 0.090)
  svr-rbf-xl: R2 = 0.357 (std: 0.090)
  svr-poly-l: R2 = 0.357 (std: 0.090)
  knn-tuned-sqrt: R2 = 0.157 (std: 0.085)
  knn-tuned-l: R2 = 0.157 (std: 0.085)
  ridge: R2 = 0.038 (std: 0.049)

Model-based training with 10 models
Best R2: 0.357, Mean R2: 0.260
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.069 mlp-l:0.074 svr-rbf-xl:0.159 svr-poly-l:0.159 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=2662.2573, entropy=0.1454, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0641
  Round 1/5: Mean predicted reward = -3.862
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.069 mlp-l:0.074 svr-rbf-xl:0.159 svr-poly-l:0.159 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9698, entropy=0.1399, kl_div=0.0000
    Epoch 1: policy_loss=-0.0256, value_loss=0.9698, entropy=0.1388, kl_div=0.0226
  Round 2/5: Mean predicted reward = 9.534
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.069 mlp-l:0.074 svr-rbf-xl:0.159 svr-poly-l:0.159 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.1366, kl_div=0.0000
    Epoch 1: policy_loss=-0.0395, value_loss=0.9688, entropy=0.1364, kl_div=0.0428
  Round 3/5: Mean predicted reward = 9.588
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.069 mlp-l:0.074 svr-rbf-xl:0.159 svr-poly-l:0.159 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.1405, kl_div=0.0000
    Epoch 1: policy_loss=-0.0417, value_loss=0.9688, entropy=0.1391, kl_div=0.0235
  Round 4/5: Mean predicted reward = 9.498
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.105 xgb-l:0.105 mlp-adaptive-xl:0.069 mlp-l:0.074 svr-rbf-xl:0.159 svr-poly-l:0.159 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9680, entropy=0.1278, kl_div=0.0000
    Epoch 1: policy_loss=-0.0365, value_loss=0.9680, entropy=0.1244, kl_div=0.0406
  Round 5/5: Mean predicted reward = 9.423

  === Progress Analysis ===
  Status: NORMAL

--- Round 82 Results ---
  Mean Oracle Reward: 9.701
  Min Oracle Reward: 8.265
  Max Oracle Reward: 11.325
  Std Oracle Reward: 0.826
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.260, Max: 0.357, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2674
    Oracle Count: 2624 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 83/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2674
  Performance plateaued, reducing LR to 0.000055

--- Round 83 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.875) ---
  CAGTGATCGC
  TAGCAGGCTC
  CTCAGGGATC
  CTCGAGACGG
  GGCCAGGACT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.643
  Max reward: 10.688
  With intrinsic bonuses: 9.720

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9701, entropy=0.1433, kl_div=0.0000
    Epoch 1: policy_loss=-0.0347, value_loss=0.9701, entropy=0.1419, kl_div=0.0387

=== Surrogate Model Training ===
Total samples: 2706

Training on 2574 samples (removed 132 outliers)
Reward range: [6.68, 11.90], mean: 9.32
  Created 13 candidate models for data size 2574
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.328 (std: 0.082)
  rf-tuned-xl: R2 = 0.331 (std: 0.077)
  gb-tuned-l: R2 = 0.240 (std: 0.070)
  gb-tuned-xl: R2 = 0.240 (std: 0.070)
  xgb-xl: R2 = 0.327 (std: 0.074)
  xgb-l: R2 = 0.327 (std: 0.074)
  mlp-adaptive-xl: R2 = 0.277 (std: 0.108)
  mlp-l: R2 = 0.294 (std: 0.090)
  svr-rbf-xl: R2 = 0.360 (std: 0.089)
  svr-poly-l: R2 = 0.360 (std: 0.089)
  knn-tuned-sqrt: R2 = 0.170 (std: 0.089)
  knn-tuned-l: R2 = 0.170 (std: 0.089)
  ridge: R2 = 0.040 (std: 0.052)

Model-based training with 10 models
Best R2: 0.360, Mean R2: 0.266
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.116 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.111 xgb-l:0.111 mlp-adaptive-xl:0.067 mlp-l:0.080 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=754.2474, entropy=0.1124, kl_div=0.0000
    Epoch 1: policy_loss=-0.0024, value_loss=754.2473, entropy=0.1102, kl_div=-0.0324
  Round 1/5: Mean predicted reward = -2.033
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.116 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.111 xgb-l:0.111 mlp-adaptive-xl:0.067 mlp-l:0.080 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9694, entropy=0.1073, kl_div=0.0000
    Epoch 1: policy_loss=-0.0295, value_loss=0.9694, entropy=0.1055, kl_div=-0.0054
  Round 2/5: Mean predicted reward = 9.470
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.116 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.111 xgb-l:0.111 mlp-adaptive-xl:0.067 mlp-l:0.080 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.1142, kl_div=0.0000
    Epoch 1: policy_loss=-0.0271, value_loss=0.9692, entropy=0.1121, kl_div=0.0119
  Round 3/5: Mean predicted reward = 9.547
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.116 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.111 xgb-l:0.111 mlp-adaptive-xl:0.067 mlp-l:0.080 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.1150, kl_div=0.0000
    Epoch 1: policy_loss=-0.0171, value_loss=0.9688, entropy=0.1132, kl_div=0.0135
  Round 4/5: Mean predicted reward = 9.585
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.116 gb-tuned-l:0.046 gb-tuned-xl:0.046 xgb-xl:0.111 xgb-l:0.111 mlp-adaptive-xl:0.067 mlp-l:0.080 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.1053, kl_div=0.0000
    Epoch 1: policy_loss=-0.0161, value_loss=0.9689, entropy=0.1041, kl_div=0.0281
  Round 5/5: Mean predicted reward = 9.658

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 83 Results ---
  Mean Oracle Reward: 9.752
  Min Oracle Reward: 7.864
  Max Oracle Reward: 11.016
  Std Oracle Reward: 0.801
  Sequence Diversity: 0.875
  Models Used: 10
  Model R2 - Mean: 0.266, Max: 0.360, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 2706
    Oracle Count: 2656 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 84/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2706
  Performance plateaued, reducing LR to 0.000019

--- Round 84 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.844) ---
  GTGCCGACAG
  TGCAGCCAGT
  GCTATGCCGA
  AGGCCGTCTA
  GAGGTCCCTA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.656
  Max reward: 11.680
  With intrinsic bonuses: 9.579

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0957, kl_div=0.0000
    Epoch 1: policy_loss=-0.0114, value_loss=0.9689, entropy=0.0952, kl_div=0.0167

=== Surrogate Model Training ===
Total samples: 2738

Training on 2605 samples (removed 133 outliers)
Reward range: [6.68, 11.90], mean: 9.33
  Created 13 candidate models for data size 2605
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.333 (std: 0.074)
  rf-tuned-xl: R2 = 0.336 (std: 0.072)
  gb-tuned-l: R2 = 0.254 (std: 0.070)
  gb-tuned-xl: R2 = 0.254 (std: 0.070)
  xgb-xl: R2 = 0.330 (std: 0.079)
  xgb-l: R2 = 0.330 (std: 0.079)
  mlp-adaptive-xl: R2 = 0.291 (std: 0.090)
  mlp-l: R2 = 0.318 (std: 0.105)
  svr-rbf-xl: R2 = 0.365 (std: 0.085)
  svr-poly-l: R2 = 0.365 (std: 0.085)
  knn-tuned-sqrt: R2 = 0.170 (std: 0.089)
  knn-tuned-l: R2 = 0.170 (std: 0.089)
  ridge: R2 = 0.039 (std: 0.050)

Model-based training with 10 models
Best R2: 0.365, Mean R2: 0.273
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.072 mlp-l:0.094 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=533.4581, entropy=0.0913, kl_div=0.0000
    Epoch 1: policy_loss=0.0021, value_loss=533.4580, entropy=0.0908, kl_div=-0.0006
  Round 1/5: Mean predicted reward = -1.963
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.072 mlp-l:0.094 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.0963, kl_div=0.0000
    Epoch 1: policy_loss=-0.0058, value_loss=0.9692, entropy=0.0959, kl_div=0.0022
  Round 2/5: Mean predicted reward = 9.517
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.072 mlp-l:0.094 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0931, kl_div=0.0000
    Epoch 1: policy_loss=-0.0034, value_loss=0.9690, entropy=0.0930, kl_div=0.0111
  Round 3/5: Mean predicted reward = 9.482
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.072 mlp-l:0.094 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9697, entropy=0.0895, kl_div=0.0000
    Epoch 1: policy_loss=-0.0125, value_loss=0.9697, entropy=0.0893, kl_div=0.0177
  Round 4/5: Mean predicted reward = 9.511
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.109 rf-tuned-xl:0.113 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.072 mlp-l:0.094 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.0878, kl_div=0.0000
    Epoch 1: policy_loss=-0.0036, value_loss=0.9687, entropy=0.0878, kl_div=0.0082
  Round 5/5: Mean predicted reward = 9.478

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 84 Results ---
  Mean Oracle Reward: 9.609
  Min Oracle Reward: 6.045
  Max Oracle Reward: 11.395
  Std Oracle Reward: 1.098
  Sequence Diversity: 0.844
  Models Used: 10
  Model R2 - Mean: 0.273, Max: 0.365, Count: 13
  Total Sequences Evaluated: 2738
    Oracle Count: 2688 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 85/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2738
  Performance plateaued, reducing LR to 0.000150

--- Round 85 Configuration ---
Learning rate: 0.000150
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.781) ---
  ACATCGGCGG
  CGCTATAGGC
  GAACGGCTGC
  AGCATTCGGC
  CGTCTCGAAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.581
  Max reward: 11.909
  With intrinsic bonuses: 9.557

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.1106, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2029

=== Surrogate Model Training ===
Total samples: 2770

Training on 2636 samples (removed 134 outliers)
Reward range: [6.68, 11.90], mean: 9.33
  Created 13 candidate models for data size 2636
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.350 (std: 0.068)
  rf-tuned-xl: R2 = 0.347 (std: 0.072)
  gb-tuned-l: R2 = 0.262 (std: 0.069)
  gb-tuned-xl: R2 = 0.262 (std: 0.069)
  xgb-xl: R2 = 0.353 (std: 0.073)
  xgb-l: R2 = 0.353 (std: 0.073)
  mlp-adaptive-xl: R2 = 0.315 (std: 0.079)
  mlp-l: R2 = 0.328 (std: 0.074)
  svr-rbf-xl: R2 = 0.374 (std: 0.081)
  svr-poly-l: R2 = 0.374 (std: 0.081)
  knn-tuned-sqrt: R2 = 0.177 (std: 0.094)
  knn-tuned-l: R2 = 0.177 (std: 0.094)
  ridge: R2 = 0.045 (std: 0.061)

Model-based training with 10 models
Best R2: 0.374, Mean R2: 0.286
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.109 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.079 mlp-l:0.090 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1366.2546, entropy=0.0928, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0606
  Round 1/5: Mean predicted reward = -6.289
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.109 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.079 mlp-l:0.090 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.0928, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1166
  Round 2/5: Mean predicted reward = 9.483
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.109 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.079 mlp-l:0.090 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9683, entropy=0.1034, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1315
  Round 3/5: Mean predicted reward = 9.514
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.109 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.079 mlp-l:0.090 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0974, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1038
  Round 4/5: Mean predicted reward = 9.581
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.109 gb-tuned-l:0.047 gb-tuned-xl:0.047 xgb-xl:0.116 xgb-l:0.116 mlp-adaptive-xl:0.079 mlp-l:0.090 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.1276, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0930
  Round 5/5: Mean predicted reward = 9.508

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 85 Results ---
  Mean Oracle Reward: 9.578
  Min Oracle Reward: 6.729
  Max Oracle Reward: 11.840
  Std Oracle Reward: 1.145
  Sequence Diversity: 0.781
  Models Used: 10
  Model R2 - Mean: 0.286, Max: 0.374, Count: 13
  Total Sequences Evaluated: 2770
    Oracle Count: 2720 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 86/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2770
  Performance plateaued, reducing LR to 0.000136

--- Round 86 Configuration ---
Learning rate: 0.000136
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  ATTCTGGAAC
  CCGGTAGACT
  GCGACAGTTC
  TCACATGCGG
  CGCGTTCGAA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.140
  Max reward: 10.936
  With intrinsic bonuses: 9.100

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.0972, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0632

=== Surrogate Model Training ===
Total samples: 2802

Training on 2668 samples (removed 134 outliers)
Reward range: [6.68, 11.95], mean: 9.33
  Created 13 candidate models for data size 2668
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.352 (std: 0.080)
  rf-tuned-xl: R2 = 0.351 (std: 0.069)
  gb-tuned-l: R2 = 0.265 (std: 0.074)
  gb-tuned-xl: R2 = 0.265 (std: 0.074)
  xgb-xl: R2 = 0.342 (std: 0.074)
  xgb-l: R2 = 0.342 (std: 0.074)
  mlp-adaptive-xl: R2 = 0.310 (std: 0.087)
  mlp-l: R2 = 0.305 (std: 0.085)
  svr-rbf-xl: R2 = 0.379 (std: 0.086)
  svr-poly-l: R2 = 0.379 (std: 0.086)
  knn-tuned-sqrt: R2 = 0.187 (std: 0.101)
  knn-tuned-l: R2 = 0.187 (std: 0.101)
  ridge: R2 = 0.049 (std: 0.065)

Model-based training with 10 models
Best R2: 0.379, Mean R2: 0.285
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.117 rf-tuned-xl:0.116 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.077 mlp-l:0.073 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=5093.3550, entropy=0.0883, kl_div=0.0000
    Epoch 1: policy_loss=0.0076, value_loss=5093.3550, entropy=0.0884, kl_div=-0.0328
  Round 1/5: Mean predicted reward = -7.284
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.117 rf-tuned-xl:0.116 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.077 mlp-l:0.073 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9686, entropy=0.0956, kl_div=0.0000
    Epoch 1: policy_loss=-0.0093, value_loss=0.9686, entropy=0.0945, kl_div=-0.0809
  Round 2/5: Mean predicted reward = 9.585
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.117 rf-tuned-xl:0.116 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.077 mlp-l:0.073 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0868, kl_div=0.0000
    Epoch 1: policy_loss=-0.0282, value_loss=0.9692, entropy=0.0858, kl_div=0.0235
  Round 3/5: Mean predicted reward = 9.456
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.117 rf-tuned-xl:0.116 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.077 mlp-l:0.073 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.0790, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0975
  Round 4/5: Mean predicted reward = 9.593
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.117 rf-tuned-xl:0.116 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.077 mlp-l:0.073 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9696, entropy=0.0957, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0512
  Round 5/5: Mean predicted reward = 9.575

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 86 Results ---
  Mean Oracle Reward: 9.092
  Min Oracle Reward: 4.485
  Max Oracle Reward: 11.154
  Std Oracle Reward: 1.547
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.285, Max: 0.379, Count: 13
  Total Sequences Evaluated: 2802
    Oracle Count: 2752 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 87/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2802

--- Round 87 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.781) ---
  GACTCATCGG
  ACGTCGGAGC
  CTACTCGGAG
  GCGCCAGTAG
  GGTATCACCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.313
  Max reward: 10.639
  With intrinsic bonuses: 9.342

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.0828, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0826

=== Surrogate Model Training ===
Total samples: 2834

Training on 2699 samples (removed 135 outliers)
Reward range: [6.68, 11.95], mean: 9.33
  Created 13 candidate models for data size 2699
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.355 (std: 0.083)
  rf-tuned-xl: R2 = 0.356 (std: 0.082)
  gb-tuned-l: R2 = 0.275 (std: 0.079)
  gb-tuned-xl: R2 = 0.275 (std: 0.079)
  xgb-xl: R2 = 0.340 (std: 0.085)
  xgb-l: R2 = 0.340 (std: 0.085)
  mlp-adaptive-xl: R2 = 0.329 (std: 0.083)
  mlp-l: R2 = 0.306 (std: 0.099)
  svr-rbf-xl: R2 = 0.384 (std: 0.090)
  svr-poly-l: R2 = 0.384 (std: 0.090)
  knn-tuned-sqrt: R2 = 0.188 (std: 0.099)
  knn-tuned-l: R2 = 0.188 (std: 0.099)
  ridge: R2 = 0.053 (std: 0.073)

Model-based training with 10 models
Best R2: 0.384, Mean R2: 0.290
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.116 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.089 mlp-l:0.071 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=6017.1890, entropy=0.0937, kl_div=0.0000
    Epoch 1: policy_loss=0.0125, value_loss=6017.1885, entropy=0.0907, kl_div=0.0450
  Round 1/5: Mean predicted reward = -8.128
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.116 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.089 mlp-l:0.071 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0874, kl_div=0.0000
    Epoch 1: policy_loss=0.0066, value_loss=0.9691, entropy=0.0857, kl_div=0.0360
  Round 2/5: Mean predicted reward = 9.483
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.116 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.089 mlp-l:0.071 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0663, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1303
  Round 3/5: Mean predicted reward = 9.555
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.116 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.089 mlp-l:0.071 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.0892, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0553
  Round 4/5: Mean predicted reward = 9.490
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.115 rf-tuned-xl:0.116 gb-tuned-l:0.052 gb-tuned-xl:0.052 xgb-xl:0.099 xgb-l:0.099 mlp-adaptive-xl:0.089 mlp-l:0.071 svr-rbf-xl:0.154 svr-poly-l:0.154 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9681, entropy=0.0796, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1617
  Round 5/5: Mean predicted reward = 9.626

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 87 Results ---
  Mean Oracle Reward: 9.333
  Min Oracle Reward: 6.793
  Max Oracle Reward: 10.562
  Std Oracle Reward: 1.117
  Sequence Diversity: 0.781
  Models Used: 10
  Model R2 - Mean: 0.290, Max: 0.384, Count: 13
  Total Sequences Evaluated: 2834
    Oracle Count: 2784 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 88/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2834

--- Round 88 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  GTGCGCAAGC
  GAGCGCTGAC
  GGGCGCCTAA
  GGCGGCATAC
  AGCTAGTCGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.114
  Max reward: 11.492
  With intrinsic bonuses: 9.155

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9680, entropy=0.0779, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0558

=== Surrogate Model Training ===
Total samples: 2866

Training on 2730 samples (removed 136 outliers)
Reward range: [6.65, 11.95], mean: 9.33
  Created 13 candidate models for data size 2730
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.352 (std: 0.083)
  rf-tuned-xl: R2 = 0.352 (std: 0.083)
  gb-tuned-l: R2 = 0.274 (std: 0.079)
  gb-tuned-xl: R2 = 0.274 (std: 0.079)
  xgb-xl: R2 = 0.353 (std: 0.088)
  xgb-l: R2 = 0.353 (std: 0.088)
  mlp-adaptive-xl: R2 = 0.335 (std: 0.106)
  mlp-l: R2 = 0.326 (std: 0.095)
  svr-rbf-xl: R2 = 0.381 (std: 0.089)
  svr-poly-l: R2 = 0.381 (std: 0.089)
  knn-tuned-sqrt: R2 = 0.178 (std: 0.090)
  knn-tuned-l: R2 = 0.178 (std: 0.090)
  ridge: R2 = 0.051 (std: 0.075)

Model-based training with 10 models
Best R2: 0.381, Mean R2: 0.291
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.108 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.109 xgb-l:0.109 mlp-adaptive-xl:0.092 mlp-l:0.084 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=930.5871, entropy=0.0896, kl_div=0.0000
    Epoch 1: policy_loss=0.0065, value_loss=930.5870, entropy=0.0911, kl_div=-0.1294
  Round 1/5: Mean predicted reward = -7.462
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.108 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.109 xgb-l:0.109 mlp-adaptive-xl:0.092 mlp-l:0.084 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.0771, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0526
  Round 2/5: Mean predicted reward = 9.664
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.108 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.109 xgb-l:0.109 mlp-adaptive-xl:0.092 mlp-l:0.084 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0777, kl_div=0.0000
    Epoch 1: policy_loss=-0.0085, value_loss=0.9691, entropy=0.0766, kl_div=0.0185
  Round 3/5: Mean predicted reward = 9.394
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.108 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.109 xgb-l:0.109 mlp-adaptive-xl:0.092 mlp-l:0.084 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0908, kl_div=0.0000
    Epoch 1: policy_loss=-0.0304, value_loss=0.9691, entropy=0.0900, kl_div=0.0394
  Round 4/5: Mean predicted reward = 9.491
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.108 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.109 xgb-l:0.109 mlp-adaptive-xl:0.092 mlp-l:0.084 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=0.0826, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0526
  Round 5/5: Mean predicted reward = 9.641

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 88 Results ---
  Mean Oracle Reward: 9.164
  Min Oracle Reward: 0.722
  Max Oracle Reward: 11.502
  Std Oracle Reward: 1.953
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.291, Max: 0.381, Count: 13
  Total Sequences Evaluated: 2866
    Oracle Count: 2816 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 89/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 2866
  Performance plateaued, reducing LR to 0.000019

--- Round 89 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  TGCAGAATGC
  TCCGACGTGA
  GCAGCCAGTG
  GCTCAAGCTG
  CGAGGCGACT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.485
  Max reward: 10.656
  With intrinsic bonuses: 9.529

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.0825, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=0.9686, entropy=0.0824, kl_div=0.0124

=== Surrogate Model Training ===
Total samples: 2898

Training on 2762 samples (removed 136 outliers)
Reward range: [6.65, 11.95], mean: 9.34
  Created 13 candidate models for data size 2762
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.358 (std: 0.089)
  rf-tuned-xl: R2 = 0.357 (std: 0.089)
  gb-tuned-l: R2 = 0.274 (std: 0.095)
  gb-tuned-xl: R2 = 0.274 (std: 0.095)
  xgb-xl: R2 = 0.366 (std: 0.087)
  xgb-l: R2 = 0.366 (std: 0.087)
  mlp-adaptive-xl: R2 = 0.314 (std: 0.096)
  mlp-l: R2 = 0.319 (std: 0.079)
  svr-rbf-xl: R2 = 0.382 (std: 0.094)
  svr-poly-l: R2 = 0.382 (std: 0.094)
  knn-tuned-sqrt: R2 = 0.187 (std: 0.095)
  knn-tuned-l: R2 = 0.187 (std: 0.095)
  ridge: R2 = 0.050 (std: 0.076)

Model-based training with 10 models
Best R2: 0.382, Mean R2: 0.294
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.112 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.122 xgb-l:0.122 mlp-adaptive-xl:0.073 mlp-l:0.077 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1107.0974, entropy=0.0649, kl_div=0.0000
    Epoch 1: policy_loss=-0.0128, value_loss=1107.0974, entropy=0.0649, kl_div=-0.0233
  Round 1/5: Mean predicted reward = -8.154
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.112 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.122 xgb-l:0.122 mlp-adaptive-xl:0.073 mlp-l:0.077 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0895, kl_div=0.0000
    Epoch 1: policy_loss=-0.0118, value_loss=0.9689, entropy=0.0898, kl_div=-0.0168
  Round 2/5: Mean predicted reward = 9.515
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.112 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.122 xgb-l:0.122 mlp-adaptive-xl:0.073 mlp-l:0.077 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.0727, kl_div=0.0000
    Epoch 1: policy_loss=-0.0249, value_loss=0.9689, entropy=0.0730, kl_div=0.0241
  Round 3/5: Mean predicted reward = 9.416
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.112 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.122 xgb-l:0.122 mlp-adaptive-xl:0.073 mlp-l:0.077 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0705, kl_div=0.0000
    Epoch 1: policy_loss=0.0041, value_loss=0.9692, entropy=0.0709, kl_div=0.0068
  Round 4/5: Mean predicted reward = 9.467
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.112 rf-tuned-xl:0.112 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.122 xgb-l:0.122 mlp-adaptive-xl:0.073 mlp-l:0.077 svr-rbf-xl:0.143 svr-poly-l:0.143 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9692, entropy=0.0673, kl_div=0.0000
    Epoch 1: policy_loss=-0.0076, value_loss=0.9692, entropy=0.0671, kl_div=0.0056
  Round 5/5: Mean predicted reward = 9.562

  === Progress Analysis ===
  Status: NORMAL

--- Round 89 Results ---
  Mean Oracle Reward: 9.511
  Min Oracle Reward: 7.062
  Max Oracle Reward: 10.562
  Std Oracle Reward: 0.828
  Sequence Diversity: 0.812
  Models Used: 10
  Model R2 - Mean: 0.294, Max: 0.382, Count: 13
  Total Sequences Evaluated: 2898
    Oracle Count: 2848 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 90/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 2898

--- Round 90 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  CTAGGCTGCA
  ATTCACGGCG
  TCGGACATCG
  GCGACGTCTA
  CAATGCCTGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.618
  Max reward: 10.745
  With intrinsic bonuses: 9.568

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.0865, kl_div=0.0000
    Epoch 1: policy_loss=-0.0228, value_loss=0.9688, entropy=0.0806, kl_div=0.0101

=== Surrogate Model Training ===
Total samples: 2930

Training on 2792 samples (removed 138 outliers)
Reward range: [6.68, 11.95], mean: 9.34
  Created 13 candidate models for data size 2792
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.357 (std: 0.094)
  rf-tuned-xl: R2 = 0.359 (std: 0.095)
  gb-tuned-l: R2 = 0.277 (std: 0.091)
  gb-tuned-xl: R2 = 0.277 (std: 0.091)
  xgb-xl: R2 = 0.355 (std: 0.103)
  xgb-l: R2 = 0.355 (std: 0.103)
  mlp-adaptive-xl: R2 = 0.334 (std: 0.102)
  mlp-l: R2 = 0.345 (std: 0.093)
  svr-rbf-xl: R2 = 0.380 (std: 0.097)
  svr-poly-l: R2 = 0.380 (std: 0.097)
  knn-tuned-sqrt: R2 = 0.194 (std: 0.100)
  knn-tuned-l: R2 = 0.194 (std: 0.100)
  ridge: R2 = 0.049 (std: 0.080)

Model-based training with 10 models
Best R2: 0.380, Mean R2: 0.296
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.112 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.108 xgb-l:0.108 mlp-adaptive-xl:0.088 mlp-l:0.098 svr-rbf-xl:0.138 svr-poly-l:0.138 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1552.6993, entropy=0.0603, kl_div=0.0000
    Epoch 1: policy_loss=-0.0075, value_loss=1552.6993, entropy=0.0559, kl_div=-0.1956
  Round 1/5: Mean predicted reward = -9.825
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.112 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.108 xgb-l:0.108 mlp-adaptive-xl:0.088 mlp-l:0.098 svr-rbf-xl:0.138 svr-poly-l:0.138 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.0508, kl_div=0.0000
    Epoch 1: policy_loss=-0.0237, value_loss=0.9688, entropy=0.0525, kl_div=-0.0814
  Round 2/5: Mean predicted reward = 9.553
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.112 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.108 xgb-l:0.108 mlp-adaptive-xl:0.088 mlp-l:0.098 svr-rbf-xl:0.138 svr-poly-l:0.138 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.0561, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0981
  Round 3/5: Mean predicted reward = 9.568
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.112 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.108 xgb-l:0.108 mlp-adaptive-xl:0.088 mlp-l:0.098 svr-rbf-xl:0.138 svr-poly-l:0.138 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.0570, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1237
  Round 4/5: Mean predicted reward = 9.638
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.112 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.108 xgb-l:0.108 mlp-adaptive-xl:0.088 mlp-l:0.098 svr-rbf-xl:0.138 svr-poly-l:0.138 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0629, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1077
  Round 5/5: Mean predicted reward = 9.628

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 90 Results ---
  Mean Oracle Reward: 9.641
  Min Oracle Reward: 5.751
  Max Oracle Reward: 10.836
  Std Oracle Reward: 1.018
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.296, Max: 0.380, Count: 13
  Total Sequences Evaluated: 2930
    Oracle Count: 2880 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 91/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 2930
  Consistent improvement, increasing LR to 0.000327

--- Round 91 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GGACAGTCCG
  GTGACCGTAA
  GCTTGCCGAA
  CGTAGACCGT
  TACCTAAGGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.519
  Max reward: 10.604
  With intrinsic bonuses: 9.453

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0518, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1531

=== Surrogate Model Training ===
Total samples: 2962

Training on 2824 samples (removed 138 outliers)
Reward range: [6.65, 11.95], mean: 9.34
  Created 13 candidate models for data size 2824
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.356 (std: 0.102)
  rf-tuned-xl: R2 = 0.356 (std: 0.103)
  gb-tuned-l: R2 = 0.268 (std: 0.109)
  gb-tuned-xl: R2 = 0.268 (std: 0.109)
  xgb-xl: R2 = 0.362 (std: 0.107)
  xgb-l: R2 = 0.362 (std: 0.107)
  mlp-adaptive-xl: R2 = 0.326 (std: 0.113)
  mlp-l: R2 = 0.336 (std: 0.097)
  svr-rbf-xl: R2 = 0.381 (std: 0.106)
  svr-poly-l: R2 = 0.381 (std: 0.106)
  knn-tuned-sqrt: R2 = 0.193 (std: 0.104)
  knn-tuned-l: R2 = 0.193 (std: 0.104)
  ridge: R2 = 0.050 (std: 0.087)

Model-based training with 10 models
Best R2: 0.381, Mean R2: 0.295
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.111 gb-tuned-l:0.045 gb-tuned-xl:0.045 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.082 mlp-l:0.090 svr-rbf-xl:0.142 svr-poly-l:0.142 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=1214.5735, entropy=0.0729, kl_div=0.0000
    Epoch 1: policy_loss=0.0265, value_loss=1214.5740, entropy=0.0710, kl_div=-0.1623
  Round 1/5: Mean predicted reward = -11.221
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.111 gb-tuned-l:0.045 gb-tuned-xl:0.045 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.082 mlp-l:0.090 svr-rbf-xl:0.142 svr-poly-l:0.142 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9687, entropy=0.0592, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1923
  Round 2/5: Mean predicted reward = 9.598
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.111 gb-tuned-l:0.045 gb-tuned-xl:0.045 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.082 mlp-l:0.090 svr-rbf-xl:0.142 svr-poly-l:0.142 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.0521, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3630
  Round 3/5: Mean predicted reward = 9.529
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.111 gb-tuned-l:0.045 gb-tuned-xl:0.045 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.082 mlp-l:0.090 svr-rbf-xl:0.142 svr-poly-l:0.142 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9696, entropy=0.0618, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.3391
  Round 4/5: Mean predicted reward = 9.598
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.110 rf-tuned-xl:0.111 gb-tuned-l:0.045 gb-tuned-xl:0.045 xgb-xl:0.117 xgb-l:0.117 mlp-adaptive-xl:0.082 mlp-l:0.090 svr-rbf-xl:0.142 svr-poly-l:0.142 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.0738, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2272
  Round 5/5: Mean predicted reward = 9.481

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 91 Results ---
  Mean Oracle Reward: 9.468
  Min Oracle Reward: 5.123
  Max Oracle Reward: 10.702
  Std Oracle Reward: 1.096
  Sequence Diversity: 0.719
  Models Used: 10
  Model R2 - Mean: 0.295, Max: 0.381, Count: 13
  Total Sequences Evaluated: 2962
    Oracle Count: 2912 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 92/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 2962
  Performance plateaued, reducing LR to 0.000100

--- Round 92 Configuration ---
Learning rate: 0.000100
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  GTAGGCCACT
  AACAGTGCTG
  GATCAGCGCG
  TGTGCAACGC
  CGGAAGTCGC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.395
  Max reward: 11.147
  With intrinsic bonuses: 9.341

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0676, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0565

=== Surrogate Model Training ===
Total samples: 2994

Training on 2854 samples (removed 140 outliers)
Reward range: [6.65, 11.95], mean: 9.35
  Created 13 candidate models for data size 2854
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.360 (std: 0.111)
  rf-tuned-xl: R2 = 0.369 (std: 0.101)
  gb-tuned-l: R2 = 0.271 (std: 0.114)
  gb-tuned-xl: R2 = 0.271 (std: 0.114)
  xgb-xl: R2 = 0.373 (std: 0.118)
  xgb-l: R2 = 0.373 (std: 0.118)
  mlp-adaptive-xl: R2 = 0.328 (std: 0.127)
  mlp-l: R2 = 0.339 (std: 0.097)
  svr-rbf-xl: R2 = 0.383 (std: 0.114)
  svr-poly-l: R2 = 0.383 (std: 0.114)
  knn-tuned-sqrt: R2 = 0.197 (std: 0.112)
  knn-tuned-l: R2 = 0.197 (std: 0.112)
  ridge: R2 = 0.049 (std: 0.084)

Model-based training with 10 models
Best R2: 0.383, Mean R2: 0.300
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.118 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.079 mlp-l:0.087 svr-rbf-xl:0.137 svr-poly-l:0.137 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=5845.7822, entropy=0.0678, kl_div=0.0000
    Epoch 1: policy_loss=0.0258, value_loss=5845.7827, entropy=0.0669, kl_div=0.0350
  Round 1/5: Mean predicted reward = -15.135
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.118 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.079 mlp-l:0.087 svr-rbf-xl:0.137 svr-poly-l:0.137 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0696, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0574
  Round 2/5: Mean predicted reward = 9.536
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.118 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.079 mlp-l:0.087 svr-rbf-xl:0.137 svr-poly-l:0.137 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0621, kl_div=0.0000
    Epoch 1: policy_loss=-0.0273, value_loss=0.9691, entropy=0.0606, kl_div=0.0328
  Round 3/5: Mean predicted reward = 9.414
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.118 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.079 mlp-l:0.087 svr-rbf-xl:0.137 svr-poly-l:0.137 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0591, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1135
  Round 4/5: Mean predicted reward = 9.456
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.118 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.123 xgb-l:0.123 mlp-adaptive-xl:0.079 mlp-l:0.087 svr-rbf-xl:0.137 svr-poly-l:0.137 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0550, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0967
  Round 5/5: Mean predicted reward = 9.502

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 92 Results ---
  Mean Oracle Reward: 9.451
  Min Oracle Reward: 4.249
  Max Oracle Reward: 11.446
  Std Oracle Reward: 1.429
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.300, Max: 0.383, Count: 13
  Total Sequences Evaluated: 2994
    Oracle Count: 2944 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 93/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 2994
  Performance plateaued, reducing LR to 0.000055

--- Round 93 Configuration ---
Learning rate: 0.000055
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.812) ---
  TAAGCGGTCC
  GTACGGGACC
  ACGTGCTCGA
  GCCGTACGTA
  CACGGGATAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.297
  Max reward: 10.838
  With intrinsic bonuses: 9.271

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0672, kl_div=0.0000
    Epoch 1: policy_loss=-0.0075, value_loss=0.9692, entropy=0.0660, kl_div=0.0091

=== Surrogate Model Training ===
Total samples: 3026

Training on 2886 samples (removed 140 outliers)
Reward range: [6.64, 11.95], mean: 9.35
  Created 13 candidate models for data size 2886
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.351 (std: 0.125)
  rf-tuned-xl: R2 = 0.352 (std: 0.126)
  gb-tuned-l: R2 = 0.263 (std: 0.118)
  gb-tuned-xl: R2 = 0.263 (std: 0.118)
  xgb-xl: R2 = 0.347 (std: 0.140)
  xgb-l: R2 = 0.347 (std: 0.140)
  mlp-adaptive-xl: R2 = 0.330 (std: 0.126)
  mlp-l: R2 = 0.336 (std: 0.110)
  svr-rbf-xl: R2 = 0.377 (std: 0.125)
  svr-poly-l: R2 = 0.377 (std: 0.125)
  knn-tuned-sqrt: R2 = 0.205 (std: 0.115)
  knn-tuned-l: R2 = 0.205 (std: 0.115)
  ridge: R2 = 0.049 (std: 0.092)

Model-based training with 12 models
Best R2: 0.377, Mean R2: 0.292
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.085 mlp-l:0.091 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=3418.0320, entropy=0.0621, kl_div=0.0000
    Epoch 1: policy_loss=-0.0261, value_loss=3418.0320, entropy=0.0615, kl_div=-0.0747
  Round 1/5: Mean predicted reward = -17.128
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.085 mlp-l:0.091 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.0464, kl_div=0.0000
    Epoch 1: policy_loss=-0.0265, value_loss=0.9688, entropy=0.0458, kl_div=-0.0137
  Round 2/5: Mean predicted reward = 9.538
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.085 mlp-l:0.091 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0576, kl_div=0.0000
    Epoch 1: policy_loss=-0.0014, value_loss=0.9689, entropy=0.0574, kl_div=-0.0072
  Round 3/5: Mean predicted reward = 9.475
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.085 mlp-l:0.091 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.0624, kl_div=0.0000
    Epoch 1: policy_loss=-0.0110, value_loss=0.9688, entropy=0.0621, kl_div=-0.0003
  Round 4/5: Mean predicted reward = 9.552
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.105 rf-tuned-xl:0.106 gb-tuned-l:0.044 gb-tuned-xl:0.044 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.085 mlp-l:0.091 svr-rbf-xl:0.137 svr-poly-l:0.137 knn-tuned-sqrt:0.024 knn-tuned-l:0.024 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0702, kl_div=0.0000
    Epoch 1: policy_loss=-0.0228, value_loss=0.9691, entropy=0.0703, kl_div=0.0484
  Round 5/5: Mean predicted reward = 9.524

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 93 Results ---
  Mean Oracle Reward: 9.279
  Min Oracle Reward: 5.363
  Max Oracle Reward: 11.010
  Std Oracle Reward: 1.272
  Sequence Diversity: 0.812
  Models Used: 12
  Model R2 - Mean: 0.292, Max: 0.377, Count: 13
  Total Sequences Evaluated: 3026
    Oracle Count: 2976 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 94/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 3026
  Performance plateaued, reducing LR to 0.000019

--- Round 94 Configuration ---
Learning rate: 0.000019
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GTGCGCACAT
  GGCAGGATCC
  AGCGTGAGCC
  GGACCTGCTA
  TACGTGAGAC
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.881
  Max reward: 12.462
  With intrinsic bonuses: 9.855

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9684, entropy=0.0586, kl_div=0.0000
    Epoch 1: policy_loss=-0.0091, value_loss=0.9684, entropy=0.0585, kl_div=0.0023

=== Surrogate Model Training ===
Total samples: 3058

Training on 2916 samples (removed 142 outliers)
Reward range: [6.65, 11.95], mean: 9.35
  Created 13 candidate models for data size 2916
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.353 (std: 0.126)
  rf-tuned-xl: R2 = 0.351 (std: 0.122)
  gb-tuned-l: R2 = 0.276 (std: 0.113)
  gb-tuned-xl: R2 = 0.276 (std: 0.113)
  xgb-xl: R2 = 0.355 (std: 0.132)
  xgb-l: R2 = 0.355 (std: 0.132)
  mlp-adaptive-xl: R2 = 0.336 (std: 0.116)
  mlp-l: R2 = 0.332 (std: 0.127)
  svr-rbf-xl: R2 = 0.380 (std: 0.125)
  svr-poly-l: R2 = 0.380 (std: 0.125)
  knn-tuned-sqrt: R2 = 0.200 (std: 0.114)
  knn-tuned-l: R2 = 0.200 (std: 0.114)
  ridge: R2 = 0.051 (std: 0.089)

Model-based training with 12 models
Best R2: 0.380, Mean R2: 0.296
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.102 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.087 mlp-l:0.084 svr-rbf-xl:0.136 svr-poly-l:0.136 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=8025.3325, entropy=0.0608, kl_div=0.0000
    Epoch 1: policy_loss=-0.0034, value_loss=8025.3325, entropy=0.0608, kl_div=-0.0035
  Round 1/5: Mean predicted reward = -19.701
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.102 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.087 mlp-l:0.084 svr-rbf-xl:0.136 svr-poly-l:0.136 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0572, kl_div=0.0000
    Epoch 1: policy_loss=-0.0046, value_loss=0.9691, entropy=0.0572, kl_div=-0.0030
  Round 2/5: Mean predicted reward = 9.496
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.102 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.087 mlp-l:0.084 svr-rbf-xl:0.136 svr-poly-l:0.136 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0519, kl_div=0.0000
    Epoch 1: policy_loss=-0.0037, value_loss=0.9690, entropy=0.0520, kl_div=-0.0052
  Round 3/5: Mean predicted reward = 9.402
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.102 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.087 mlp-l:0.084 svr-rbf-xl:0.136 svr-poly-l:0.136 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0540, kl_div=0.0000
    Epoch 1: policy_loss=-0.0039, value_loss=0.9691, entropy=0.0541, kl_div=-0.0022
  Round 4/5: Mean predicted reward = 9.632
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.103 rf-tuned-xl:0.102 gb-tuned-l:0.048 gb-tuned-xl:0.048 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.087 mlp-l:0.084 svr-rbf-xl:0.136 svr-poly-l:0.136 knn-tuned-sqrt:0.022 knn-tuned-l:0.022 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0544, kl_div=0.0000
    Epoch 1: policy_loss=-0.0040, value_loss=0.9691, entropy=0.0546, kl_div=0.0081
  Round 5/5: Mean predicted reward = 9.472

  === Progress Analysis ===
  Status: NORMAL

--- Round 94 Results ---
  Mean Oracle Reward: 9.874
  Min Oracle Reward: 7.668
  Max Oracle Reward: 12.520
  Std Oracle Reward: 0.980
  Sequence Diversity: 0.719
  Models Used: 12
  Model R2 - Mean: 0.296, Max: 0.380, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 3058
    Oracle Count: 3008 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 95/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 3058

--- Round 95 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  CGAACTGGCT
  ACGGCATGTC
  CGCGTCTGAA
  GTCAGCATGC
  GCACGCGGAT
  ... (32 total)

Oracle Evaluation:
  Mean reward: 10.027
  Max reward: 11.343
  With intrinsic bonuses: 10.012

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0713, kl_div=0.0000
    Epoch 1: policy_loss=0.2239, value_loss=0.9689, entropy=0.0684, kl_div=-0.4573

=== Surrogate Model Training ===
Total samples: 3090

Training on 2947 samples (removed 143 outliers)
Reward range: [6.68, 11.95], mean: 9.36
  Created 13 candidate models for data size 2947
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.341 (std: 0.129)
  rf-tuned-xl: R2 = 0.343 (std: 0.127)
  gb-tuned-l: R2 = 0.274 (std: 0.117)
  gb-tuned-xl: R2 = 0.274 (std: 0.117)
  xgb-xl: R2 = 0.340 (std: 0.138)
  xgb-l: R2 = 0.340 (std: 0.138)
  mlp-adaptive-xl: R2 = 0.322 (std: 0.126)
  mlp-l: R2 = 0.297 (std: 0.112)
  svr-rbf-xl: R2 = 0.377 (std: 0.130)
  svr-poly-l: R2 = 0.377 (std: 0.130)
  knn-tuned-sqrt: R2 = 0.186 (std: 0.118)
  knn-tuned-l: R2 = 0.186 (std: 0.118)
  ridge: R2 = 0.050 (std: 0.087)

Model-based training with 10 models
Best R2: 0.377, Mean R2: 0.285
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.109 gb-tuned-l:0.055 gb-tuned-xl:0.055 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.088 mlp-l:0.069 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=11574.9053, entropy=0.0746, kl_div=0.0000
    Epoch 1: policy_loss=0.0267, value_loss=11574.9072, entropy=0.0762, kl_div=-0.1268
  Round 1/5: Mean predicted reward = -18.647
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.109 gb-tuned-l:0.055 gb-tuned-xl:0.055 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.088 mlp-l:0.069 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.0423, kl_div=0.0000
    Epoch 1: policy_loss=-0.0189, value_loss=0.9689, entropy=0.0406, kl_div=0.0161
  Round 2/5: Mean predicted reward = 9.481
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.109 gb-tuned-l:0.055 gb-tuned-xl:0.055 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.088 mlp-l:0.069 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0476, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2256
  Round 3/5: Mean predicted reward = 9.554
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.109 gb-tuned-l:0.055 gb-tuned-xl:0.055 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.088 mlp-l:0.069 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9695, entropy=0.0652, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1035
  Round 4/5: Mean predicted reward = 9.572
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.107 rf-tuned-xl:0.109 gb-tuned-l:0.055 gb-tuned-xl:0.055 xgb-xl:0.106 xgb-l:0.106 mlp-adaptive-xl:0.088 mlp-l:0.069 svr-rbf-xl:0.153 svr-poly-l:0.153 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0657, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1540
  Round 5/5: Mean predicted reward = 9.632

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 95 Results ---
  Mean Oracle Reward: 9.966
  Min Oracle Reward: 7.411
  Max Oracle Reward: 11.238
  Std Oracle Reward: 0.734
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.285, Max: 0.377, Count: 13
  New best mean reward!
  Total Sequences Evaluated: 3090
    Oracle Count: 3040 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 96/100
======================================================================
Learning Rate: 0.000272
Exploration Rate: 0.050
Total data collected: 3090
  Consistent improvement, increasing LR to 0.000327

--- Round 96 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  ATCGTGAACG
  AGCGCCAGTT
  GTCCGGAGAC
  CACCGGTAGT
  CGATCCTGAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.461
  Max reward: 10.795
  With intrinsic bonuses: 9.450

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0523, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1991

=== Surrogate Model Training ===
Total samples: 3122

Training on 2979 samples (removed 143 outliers)
Reward range: [6.68, 11.95], mean: 9.36
  Created 13 candidate models for data size 2979
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.351 (std: 0.126)
  rf-tuned-xl: R2 = 0.352 (std: 0.125)
  gb-tuned-l: R2 = 0.280 (std: 0.114)
  gb-tuned-xl: R2 = 0.280 (std: 0.114)
  xgb-xl: R2 = 0.347 (std: 0.145)
  xgb-l: R2 = 0.347 (std: 0.145)
  mlp-adaptive-xl: R2 = 0.332 (std: 0.130)
  mlp-l: R2 = 0.318 (std: 0.118)
  svr-rbf-xl: R2 = 0.384 (std: 0.131)
  svr-poly-l: R2 = 0.384 (std: 0.131)
  knn-tuned-sqrt: R2 = 0.193 (std: 0.116)
  knn-tuned-l: R2 = 0.193 (std: 0.116)
  ridge: R2 = 0.054 (std: 0.086)

Model-based training with 10 models
Best R2: 0.384, Mean R2: 0.293
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.109 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.090 mlp-l:0.078 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6698.7769, entropy=0.0582, kl_div=0.0000
    Epoch 1: policy_loss=0.0195, value_loss=6698.7798, entropy=0.0623, kl_div=-0.1697
  Round 1/5: Mean predicted reward = -23.319
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.109 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.090 mlp-l:0.078 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.0626, kl_div=0.0000
    Epoch 1: policy_loss=-0.0086, value_loss=0.9693, entropy=0.0658, kl_div=-0.0259
  Round 2/5: Mean predicted reward = 9.483
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.109 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.090 mlp-l:0.078 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0532, kl_div=0.0000
    Epoch 1: policy_loss=-0.0242, value_loss=0.9692, entropy=0.0504, kl_div=0.0318
  Round 3/5: Mean predicted reward = 9.486
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.109 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.090 mlp-l:0.078 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9691, entropy=0.0545, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0549
  Round 4/5: Mean predicted reward = 9.516
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.108 rf-tuned-xl:0.109 gb-tuned-l:0.053 gb-tuned-xl:0.053 xgb-xl:0.104 xgb-l:0.104 mlp-adaptive-xl:0.090 mlp-l:0.078 svr-rbf-xl:0.150 svr-poly-l:0.150 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0590, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1634
  Round 5/5: Mean predicted reward = 9.533

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 96 Results ---
  Mean Oracle Reward: 9.478
  Min Oracle Reward: 6.692
  Max Oracle Reward: 10.695
  Std Oracle Reward: 1.068
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.293, Max: 0.384, Count: 13
  Total Sequences Evaluated: 3122
    Oracle Count: 3072 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 96, 'cumulative_calls': 3072, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 97/100
======================================================================
Learning Rate: 0.000200
Exploration Rate: 0.050
Total data collected: 3122

--- Round 97 Configuration ---
Learning rate: 0.000200
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  CCTATCGGGA
  GCGATGCACT
  ACGTTACGGC
  GGGGATACCC
  GCGATTCCAG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.620
  Max reward: 10.857
  With intrinsic bonuses: 9.577

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.0592, kl_div=0.0000
    Epoch 1: policy_loss=-0.0300, value_loss=0.9686, entropy=0.0593, kl_div=-0.0187

=== Surrogate Model Training ===
Total samples: 3154

Training on 3010 samples (removed 144 outliers)
Reward range: [6.68, 11.95], mean: 9.37
  Created 13 candidate models for data size 3010
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.350 (std: 0.124)
  rf-tuned-xl: R2 = 0.353 (std: 0.123)
  gb-tuned-l: R2 = 0.281 (std: 0.111)
  gb-tuned-xl: R2 = 0.281 (std: 0.111)
  xgb-xl: R2 = 0.352 (std: 0.136)
  xgb-l: R2 = 0.352 (std: 0.136)
  mlp-adaptive-xl: R2 = 0.333 (std: 0.129)
  mlp-l: R2 = 0.306 (std: 0.133)
  svr-rbf-xl: R2 = 0.390 (std: 0.130)
  svr-poly-l: R2 = 0.390 (std: 0.130)
  knn-tuned-sqrt: R2 = 0.203 (std: 0.113)
  knn-tuned-l: R2 = 0.203 (std: 0.113)
  ridge: R2 = 0.053 (std: 0.087)

Model-based training with 12 models
Best R2: 0.390, Mean R2: 0.296
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.100 rf-tuned-xl:0.103 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.084 mlp-l:0.065 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=3569.6406, entropy=0.0616, kl_div=0.0000
    Epoch 1: policy_loss=-0.0149, value_loss=3569.6416, entropy=0.0629, kl_div=-0.1750
  Round 1/5: Mean predicted reward = -25.414
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.100 rf-tuned-xl:0.103 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.084 mlp-l:0.065 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.0661, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0812
  Round 2/5: Mean predicted reward = 9.677
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.100 rf-tuned-xl:0.103 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.084 mlp-l:0.065 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9689, entropy=0.0575, kl_div=0.0000
    Epoch 1: policy_loss=-0.0134, value_loss=0.9689, entropy=0.0572, kl_div=-0.0062
  Round 3/5: Mean predicted reward = 9.606
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.100 rf-tuned-xl:0.103 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.084 mlp-l:0.065 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0681, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2171
  Round 4/5: Mean predicted reward = 9.649
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.100 rf-tuned-xl:0.103 gb-tuned-l:0.050 gb-tuned-xl:0.050 xgb-xl:0.102 xgb-l:0.102 mlp-adaptive-xl:0.084 mlp-l:0.065 svr-rbf-xl:0.149 svr-poly-l:0.149 knn-tuned-sqrt:0.023 knn-tuned-l:0.023 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0548, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0970
  Round 5/5: Mean predicted reward = 9.636

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 97 Results ---
  Mean Oracle Reward: 9.571
  Min Oracle Reward: 6.794
  Max Oracle Reward: 11.171
  Std Oracle Reward: 1.117
  Sequence Diversity: 0.719
  Models Used: 12
  Model R2 - Mean: 0.296, Max: 0.390, Count: 13
  Total Sequences Evaluated: 3154
    Oracle Count: 3104 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 96, 'cumulative_calls': 3072, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 97, 'cumulative_calls': 3104, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 98/100
======================================================================
Learning Rate: 0.000110
Exploration Rate: 0.050
Total data collected: 3154

--- Round 98 Configuration ---
Learning rate: 0.000110
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.750) ---
  ACTCGTCGGA
  CGGTCCGGAA
  ACGGCTGTCA
  CGGAGGTCCA
  TTGAGCGCCA
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.241
  Max reward: 10.791
  With intrinsic bonuses: 9.224

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=0.0000, value_loss=0.9686, entropy=0.0565, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0553

=== Surrogate Model Training ===
Total samples: 3186

Training on 3039 samples (removed 147 outliers)
Reward range: [6.68, 11.95], mean: 9.37
  Created 13 candidate models for data size 3039
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.344 (std: 0.124)
  rf-tuned-xl: R2 = 0.349 (std: 0.123)
  gb-tuned-l: R2 = 0.279 (std: 0.112)
  gb-tuned-xl: R2 = 0.279 (std: 0.112)
  xgb-xl: R2 = 0.348 (std: 0.146)
  xgb-l: R2 = 0.348 (std: 0.146)
  mlp-adaptive-xl: R2 = 0.335 (std: 0.111)
  mlp-l: R2 = 0.356 (std: 0.117)
  svr-rbf-xl: R2 = 0.386 (std: 0.131)
  svr-poly-l: R2 = 0.386 (std: 0.131)
  knn-tuned-sqrt: R2 = 0.187 (std: 0.127)
  knn-tuned-l: R2 = 0.187 (std: 0.127)
  ridge: R2 = 0.048 (std: 0.083)

Model-based training with 10 models
Best R2: 0.386, Mean R2: 0.295
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.102 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.089 mlp-l:0.110 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=5087.6245, entropy=0.0484, kl_div=0.0000
    Epoch 1: policy_loss=-0.0076, value_loss=5087.6250, entropy=0.0480, kl_div=-0.0398
  Round 1/5: Mean predicted reward = -26.732
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.102 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.089 mlp-l:0.110 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9688, entropy=0.0583, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0518
  Round 2/5: Mean predicted reward = 9.544
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.102 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.089 mlp-l:0.110 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0628, kl_div=0.0000
    Epoch 1: policy_loss=-0.0136, value_loss=0.9692, entropy=0.0625, kl_div=0.0164
  Round 3/5: Mean predicted reward = 9.578
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.102 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.089 mlp-l:0.110 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0481, kl_div=0.0000
    Epoch 1: policy_loss=-0.0153, value_loss=0.9690, entropy=0.0481, kl_div=0.0406
  Round 4/5: Mean predicted reward = 9.561
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.097 rf-tuned-xl:0.102 gb-tuned-l:0.051 gb-tuned-xl:0.051 xgb-xl:0.101 xgb-l:0.101 mlp-adaptive-xl:0.089 mlp-l:0.110 svr-rbf-xl:0.148 svr-poly-l:0.148 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9688, entropy=0.0658, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.0967
  Round 5/5: Mean predicted reward = 9.773

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 98 Results ---
  Mean Oracle Reward: 9.243
  Min Oracle Reward: 2.800
  Max Oracle Reward: 10.968
  Std Oracle Reward: 1.750
  Sequence Diversity: 0.750
  Models Used: 10
  Model R2 - Mean: 0.295, Max: 0.386, Count: 13
  Total Sequences Evaluated: 3186
    Oracle Count: 3136 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 96, 'cumulative_calls': 3072, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 97, 'cumulative_calls': 3104, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 98, 'cumulative_calls': 3136, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 99/100
======================================================================
Learning Rate: 0.000038
Exploration Rate: 0.050
Total data collected: 3186

--- Round 99 Configuration ---
Learning rate: 0.000038
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  CCTACGGGAG
  CGGACATTGA
  GGCCGATCAG
  TGACTCGGAC
  AATGCGCGCG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.685
  Max reward: 11.282
  With intrinsic bonuses: 9.697

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0566, kl_div=0.0000
    Epoch 1: policy_loss=-0.0118, value_loss=0.9692, entropy=0.0564, kl_div=0.0099

=== Surrogate Model Training ===
Total samples: 3218

Training on 3072 samples (removed 146 outliers)
Reward range: [6.68, 12.05], mean: 9.37
  Created 13 candidate models for data size 3072
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.351 (std: 0.125)
  rf-tuned-xl: R2 = 0.349 (std: 0.119)
  gb-tuned-l: R2 = 0.278 (std: 0.116)
  gb-tuned-xl: R2 = 0.278 (std: 0.116)
  xgb-xl: R2 = 0.350 (std: 0.146)
  xgb-l: R2 = 0.350 (std: 0.146)
  mlp-adaptive-xl: R2 = 0.364 (std: 0.144)
  mlp-l: R2 = 0.336 (std: 0.139)
  svr-rbf-xl: R2 = 0.391 (std: 0.131)
  svr-poly-l: R2 = 0.391 (std: 0.131)
  knn-tuned-sqrt: R2 = 0.190 (std: 0.118)
  knn-tuned-l: R2 = 0.190 (std: 0.118)
  ridge: R2 = 0.046 (std: 0.083)

Model-based training with 10 models
Best R2: 0.391, Mean R2: 0.297
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.099 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.100 xgb-l:0.100 mlp-adaptive-xl:0.115 mlp-l:0.086 svr-rbf-xl:0.151 svr-poly-l:0.151 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=6668.6426, entropy=0.0450, kl_div=0.0000
    Epoch 1: policy_loss=-0.0175, value_loss=6668.6440, entropy=0.0449, kl_div=-0.0307
  Round 1/5: Mean predicted reward = -24.880
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.099 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.100 xgb-l:0.100 mlp-adaptive-xl:0.115 mlp-l:0.086 svr-rbf-xl:0.151 svr-poly-l:0.151 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0558, kl_div=0.0000
    Epoch 1: policy_loss=-0.0160, value_loss=0.9690, entropy=0.0552, kl_div=0.0135
  Round 2/5: Mean predicted reward = 9.612
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.099 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.100 xgb-l:0.100 mlp-adaptive-xl:0.115 mlp-l:0.086 svr-rbf-xl:0.151 svr-poly-l:0.151 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9693, entropy=0.0571, kl_div=0.0000
    Epoch 1: policy_loss=0.0012, value_loss=0.9693, entropy=0.0569, kl_div=-0.0083
  Round 3/5: Mean predicted reward = 9.539
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.099 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.100 xgb-l:0.100 mlp-adaptive-xl:0.115 mlp-l:0.086 svr-rbf-xl:0.151 svr-poly-l:0.151 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9692, entropy=0.0500, kl_div=0.0000
    Epoch 1: policy_loss=-0.0148, value_loss=0.9692, entropy=0.0499, kl_div=-0.0136
  Round 4/5: Mean predicted reward = 9.547
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.101 rf-tuned-xl:0.099 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.100 xgb-l:0.100 mlp-adaptive-xl:0.115 mlp-l:0.086 svr-rbf-xl:0.151 svr-poly-l:0.151 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9690, entropy=0.0615, kl_div=0.0000
    Epoch 1: policy_loss=-0.0112, value_loss=0.9690, entropy=0.0616, kl_div=0.0134
  Round 5/5: Mean predicted reward = 9.423

  === Progress Analysis ===
  Status: NORMAL

--- Round 99 Results ---
  Mean Oracle Reward: 9.695
  Min Oracle Reward: 7.145
  Max Oracle Reward: 11.330
  Std Oracle Reward: 1.095
  Sequence Diversity: 0.719
  Models Used: 10
  Model R2 - Mean: 0.297, Max: 0.391, Count: 13
  Total Sequences Evaluated: 3218
    Oracle Count: 3168 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 96, 'cumulative_calls': 3072, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 97, 'cumulative_calls': 3104, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 98, 'cumulative_calls': 3136, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 99, 'cumulative_calls': 3168, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
EXPERIMENT ROUND 100/100
======================================================================
Learning Rate: 0.000300
Exploration Rate: 0.050
Total data collected: 3218

--- Round 100 Configuration ---
Learning rate: 0.000300
Entropy coefficient: 0.0050
Exploration rate: 0.050

--- Generated Sequences (Diversity: 0.719) ---
  GTACTACCGG
  AGTCGGCTAC
  GCGTGGACAC
  TGAGGCCCTA
  ATCCCAGTGG
  ... (32 total)

Oracle Evaluation:
  Mean reward: 9.476
  Max reward: 11.277
  With intrinsic bonuses: 9.418

Policy Update:
  Adaptive update: clip_ratio=0.20, entropy_coef=0.005
    Epoch 0: policy_loss=-0.0000, value_loss=0.9689, entropy=0.0590, kl_div=0.0000
    Epoch 1: policy_loss=0.2871, value_loss=0.9689, entropy=0.0552, kl_div=0.0344

=== Surrogate Model Training ===
Total samples: 3250

Training on 3105 samples (removed 145 outliers)
Reward range: [6.65, 12.05], mean: 9.37
  Created 13 candidate models for data size 3105
Current R2 threshold: 0.2
  rf-tuned-l: R2 = 0.351 (std: 0.132)
  rf-tuned-xl: R2 = 0.356 (std: 0.127)
  gb-tuned-l: R2 = 0.282 (std: 0.109)
  gb-tuned-xl: R2 = 0.282 (std: 0.109)
  xgb-xl: R2 = 0.366 (std: 0.138)
  xgb-l: R2 = 0.366 (std: 0.138)
  mlp-adaptive-xl: R2 = 0.340 (std: 0.119)
  mlp-l: R2 = 0.346 (std: 0.128)
  svr-rbf-xl: R2 = 0.390 (std: 0.130)
  svr-poly-l: R2 = 0.390 (std: 0.130)
  knn-tuned-sqrt: R2 = 0.198 (std: 0.113)
  knn-tuned-l: R2 = 0.198 (std: 0.113)
  ridge: R2 = 0.050 (std: 0.081)

Model-based training with 10 models
Best R2: 0.390, Mean R2: 0.301
Running 5 virtual training rounds
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.103 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.114 xgb-l:0.114 mlp-adaptive-xl:0.088 mlp-l:0.094 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.10, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=6777.4814, entropy=0.0441, kl_div=0.0000
    Epoch 1: policy_loss=0.0144, value_loss=6777.4868, entropy=0.0423, kl_div=-0.1469
  Round 1/5: Mean predicted reward = -25.856
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.103 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.114 xgb-l:0.114 mlp-adaptive-xl:0.088 mlp-l:0.094 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0458, kl_div=0.0000
    Epoch 1: policy_loss=-0.0105, value_loss=0.9690, entropy=0.0465, kl_div=0.0013
  Round 2/5: Mean predicted reward = 9.619
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.103 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.114 xgb-l:0.114 mlp-adaptive-xl:0.088 mlp-l:0.094 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=0.0000, value_loss=0.9693, entropy=0.0546, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1227
  Round 3/5: Mean predicted reward = 9.569
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.103 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.114 xgb-l:0.114 mlp-adaptive-xl:0.088 mlp-l:0.094 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9690, entropy=0.0399, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.2370
  Round 4/5: Mean predicted reward = 9.470
Current Method: weighted
    Using performance-based weights
    Model weights: rf-tuned-l:0.098 rf-tuned-xl:0.103 gb-tuned-l:0.049 gb-tuned-xl:0.049 xgb-xl:0.114 xgb-l:0.114 mlp-adaptive-xl:0.088 mlp-l:0.094 svr-rbf-xl:0.145 svr-poly-l:0.145 
  Adaptive update: clip_ratio=0.20, entropy_coef=0.003
    Epoch 0: policy_loss=-0.0000, value_loss=0.9691, entropy=0.0448, kl_div=0.0000
    Early stopping at epoch 1: KL divergence = 0.1501
  Round 5/5: Mean predicted reward = 9.525

  === Progress Analysis ===
  Status: NORMAL
  • Reward trend declining. Consider resetting exploration parameters.

--- Round 100 Results ---
  Mean Oracle Reward: 9.410
  Min Oracle Reward: 6.844
  Max Oracle Reward: 11.492
  Std Oracle Reward: 1.097
  Sequence Diversity: 0.719
  Models Used: 10
  Model R2 - Mean: 0.301, Max: 0.390, Count: 13
  Total Sequences Evaluated: 3250
    Oracle Count: 3200 [{'round': 1, 'cumulative_calls': 32, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 2, 'cumulative_calls': 64, 'new_calls': 32, 'best_reward_so_far': 10.86023888225496}, {'round': 3, 'cumulative_calls': 96, 'new_calls': 32, 'best_reward_so_far': 11.21924579831493}, {'round': 4, 'cumulative_calls': 128, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 5, 'cumulative_calls': 160, 'new_calls': 32, 'best_reward_so_far': 12.092586299836972}, {'round': 6, 'cumulative_calls': 192, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 7, 'cumulative_calls': 224, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 8, 'cumulative_calls': 256, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 9, 'cumulative_calls': 288, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 10, 'cumulative_calls': 320, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 11, 'cumulative_calls': 352, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 12, 'cumulative_calls': 384, 'new_calls': 32, 'best_reward_so_far': 12.175835684844499}, {'round': 13, 'cumulative_calls': 416, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 14, 'cumulative_calls': 448, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 15, 'cumulative_calls': 480, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 16, 'cumulative_calls': 512, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 17, 'cumulative_calls': 544, 'new_calls': 32, 'best_reward_so_far': 13.242595685696616}, {'round': 18, 'cumulative_calls': 576, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 19, 'cumulative_calls': 608, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 20, 'cumulative_calls': 640, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 21, 'cumulative_calls': 672, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 22, 'cumulative_calls': 704, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 23, 'cumulative_calls': 736, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 24, 'cumulative_calls': 768, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 25, 'cumulative_calls': 800, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 26, 'cumulative_calls': 832, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 27, 'cumulative_calls': 864, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 28, 'cumulative_calls': 896, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 29, 'cumulative_calls': 928, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 30, 'cumulative_calls': 960, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 31, 'cumulative_calls': 992, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 32, 'cumulative_calls': 1024, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 33, 'cumulative_calls': 1056, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 34, 'cumulative_calls': 1088, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 35, 'cumulative_calls': 1120, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 36, 'cumulative_calls': 1152, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 37, 'cumulative_calls': 1184, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 38, 'cumulative_calls': 1216, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 39, 'cumulative_calls': 1248, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 40, 'cumulative_calls': 1280, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 41, 'cumulative_calls': 1312, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 42, 'cumulative_calls': 1344, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 43, 'cumulative_calls': 1376, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 44, 'cumulative_calls': 1408, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 45, 'cumulative_calls': 1440, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 46, 'cumulative_calls': 1472, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 47, 'cumulative_calls': 1504, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 48, 'cumulative_calls': 1536, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 49, 'cumulative_calls': 1568, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 50, 'cumulative_calls': 1600, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 51, 'cumulative_calls': 1632, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 52, 'cumulative_calls': 1664, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 53, 'cumulative_calls': 1696, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 54, 'cumulative_calls': 1728, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 55, 'cumulative_calls': 1760, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 56, 'cumulative_calls': 1792, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 57, 'cumulative_calls': 1824, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 58, 'cumulative_calls': 1856, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 59, 'cumulative_calls': 1888, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 60, 'cumulative_calls': 1920, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 61, 'cumulative_calls': 1952, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 62, 'cumulative_calls': 1984, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 63, 'cumulative_calls': 2016, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 64, 'cumulative_calls': 2048, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 65, 'cumulative_calls': 2080, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 66, 'cumulative_calls': 2112, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 67, 'cumulative_calls': 2144, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 68, 'cumulative_calls': 2176, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 69, 'cumulative_calls': 2208, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 70, 'cumulative_calls': 2240, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 71, 'cumulative_calls': 2272, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 72, 'cumulative_calls': 2304, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 73, 'cumulative_calls': 2336, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 74, 'cumulative_calls': 2368, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 75, 'cumulative_calls': 2400, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 76, 'cumulative_calls': 2432, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 77, 'cumulative_calls': 2464, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 78, 'cumulative_calls': 2496, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 79, 'cumulative_calls': 2528, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 80, 'cumulative_calls': 2560, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 81, 'cumulative_calls': 2592, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 82, 'cumulative_calls': 2624, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 83, 'cumulative_calls': 2656, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 84, 'cumulative_calls': 2688, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 85, 'cumulative_calls': 2720, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 86, 'cumulative_calls': 2752, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 87, 'cumulative_calls': 2784, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 88, 'cumulative_calls': 2816, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 89, 'cumulative_calls': 2848, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 90, 'cumulative_calls': 2880, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 91, 'cumulative_calls': 2912, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 92, 'cumulative_calls': 2944, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 93, 'cumulative_calls': 2976, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 94, 'cumulative_calls': 3008, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 95, 'cumulative_calls': 3040, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 96, 'cumulative_calls': 3072, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 97, 'cumulative_calls': 3104, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 98, 'cumulative_calls': 3136, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 99, 'cumulative_calls': 3168, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}, {'round': 100, 'cumulative_calls': 3200, 'new_calls': 32, 'best_reward_so_far': 13.480150027888824}]

======================================================================
DyNA PPO ALGORITHM COMPLETE! Time used 1373.23 seconds
======================================================================
Total rounds executed: 100
Total sequences evaluated: 3250
Best mean reward: 9.966 (achieved at round 95)

==================================================
TRAINING SUMMARY
==================================================
Total Rounds: 100
Final Mean Reward: 9.4099
Best Mean Reward: 9.9661
Best Max Reward: 13.4443
Initial Lr: 0.0003
Final Lr: 0.0003
Total Updates: 701
Final Diversity: 0.7188
Convergence Round: 9
==================================================

Generating learning curves...
Learning curves saved to experiments/20251007171423_r100_b32_l10_weighted_dynamic/metrices_plot_weighted_r100_b32_l10.png
Saving training metrics...
Metrics saved to experiments/20251007171423_r100_b32_l10_weighted_dynamic/metrices_data_weighted_r100_b32_l10.json

======================================================================
FINAL OPTIMIZED SEQUENCES
======================================================================

Deterministic (Exploitation):
  CCGGCCTGCT: 10.377
  CCGGCCTGCT: 10.337
  CCGGCCTGCT: 10.402
  CCGGCCTGCT: 10.480
  CCGGCCTGCT: 10.278

Stochastic (Exploration):
  CCGGCCTGCT: 10.036
  CCGGCCTGCA: 10.317
  CCGGCCTGCT: 10.165
  CCGGCCTGCA: 10.421
  CCGGCCTGCT: 10.580

Final Performance:
  Mean reward: 10.339
  Max reward: 10.580
  Std reward: 0.147

Best sequence found: CCGGCCTGCT
   Reward: 10.580

======================================================================
Training complete! Check 'learning_curves.png' and 'training_metrics.json'
======================================================================

=== Model Weight Evolution Analysis ===
Model evolution plot saved to experiments/20251007171423_r100_b32_l10_weighted_dynamic/model_evolution_r100_b32_l10.png

=== Model Performance Summary ===

Final weight distribution (Round 100):
  svr: 1.452
  xgb: 1.139
  rf: 1.008
  mlp: 0.908
  gb: 0.493

Overall model importance (average weight across all rounds):
  svr: 1.216
  rf: 0.854
  xgb: 0.754
  gb: 0.405
  mlp: 0.382
  knn: 0.140
  ridge: 0.029
Detailed performance data saved to model_performance_details.csv
